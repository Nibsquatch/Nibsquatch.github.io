[
  {
    "objectID": "posts/Blog 8/index.html",
    "href": "posts/Blog 8/index.html",
    "title": "Blog 8 - Exploring Advanced Optimization Using Newton‚Äôs Method and Adam",
    "section": "",
    "text": "Abstract\n\n\nLoading the Model and Packages\nSource Code:\n\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport kagglehub\nimport os\nfrom kagglehub import KaggleDatasetAdapter\nfrom AdvancedOptimization import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\nplt.style.use('seaborn-v0_8-whitegrid')\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nTesting Basic Function on Simple Data\nBefore we move on to advanced experiments with our Newton optimizer, we will first check that our new Logistic Regression class can converge on data that we know standard Logistic Regression can converge on. Looking back at Blog 5, we will use a small dataset to test the basic function of our Logistic Regression model using the exact function to generate our data as we did in Blog 5.\n\n# define a function to generate random data points\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# generate our test data and convert to float\nX, y = classification_data()\nX, y = torch.tensor(X, dtype = torch.float), torch.tensor(y, dtype = torch.float)\n\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_29368\\2062740850.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X, y = torch.tensor(X, dtype = torch.float), torch.tensor(y, dtype = torch.float)\n\n\n\n# instantiate our new Logisitic Regression model with a Newton Optimizer and compare it to the Gradient Descent Optimizer\nLR1 = LogisticRegression()\nLR2 = LogisticRegression()\n\nopt1 = GradientDescentOptimizer(LR1)\nopt2 = NewtonOptimizer(LR2)\n\n# for keeping track of loss values\nloss1_vec = []\nloss2_vec = []\n\n# keeps track of mean loss between the two algorithms\nmean_loss = 1\n\n# training loop to iterate through optimization steps\nwhile (mean_loss &gt; 0):\n    \n    # Track loss values (convert tensors to scalars)\n    loss1_vec.append(LR1.loss(X, y))\n    loss2_vec.append(LR2.loss(X, y))\n\n    # Perform optimization step\n    opt1.step(X, y, alpha = 0.1, Beta = 0) # Beta = 0 to skip the momentum term\n    opt2.step(X, y, alpha = 0.1)\n\n    mean_loss = (LR1.loss(X, y) + LR2.loss(X, y)) / 2\n\n# Plot loss over iterations\nplt.plot(range(1, len(loss1_vec) + 1), loss1_vec, color = \"blue\", label = \"Standard\")\nplt.plot(range(1, len(loss2_vec) + 1), loss2_vec, color = \"red\", label = \"Newton\")\nplt.semilogx()\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Loss (binary cross entropy)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis is good as our test has demonstrated that our Newton Optimizer functions, and is able to converge on simple data in approximately the same number of iterations as our standard gradient descent (without the momentum term). Lets move on to more advanced tests of the Newtwon Optimizer.\n\n\nExperiments: Using Newton‚Äôs Method to Classify Breast Cancer\nOur fist experiment we will be to determine whether or not we can find the appropiate learning rate alpha in order for our Logistic Regression model to properly classify breast cancer in patients as either malignant or benign. We will be loading a dataset from kaggle.\n\n# Load the dataset\npath = \"Pumpkin_Seeds_Dataset.xlsx\"\ndf = pd.read_excel(path)\n\ndf[\"Class\"] = df[\"Class\"].replace({\"√áer√ßevelik\": 1, \"√úrg√ºp Sivrisi\": 0})\n\n# split into X and y\nX, y = df.drop(\"Class\", axis = 1), df[\"Class\"]\n\n# Convert to torch tensors (as float for X and y)\nX, y = torch.tensor(X.values, dtype = torch.float32), torch.tensor(y.values, dtype = torch.float32)\n\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_29368\\2679479813.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"Class\"] = df[\"Class\"].replace({\"√áer√ßevelik\": 1, \"√úrg√ºp Sivrisi\": 0})\n\n\nNow that we have have our dataset, lets try to fit a Logistic Model with a Newton Optimzer and see how well the data converves. We‚Äôll start off with a standard learning rate of alpha = .1\n\n# instantiate model and optimizer\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# iterable\niterations = 1000\n\n# keep track of loss across iterations for plotting\nloss_vec = []\n\n# training loop\nfor _ in range(iterations):\n\n    # keep track of loss\n    loss_vec.append(LR.loss(X, y))\n\n    # perfomr the optimizations step\n    opt.step(X, y, alpha = .1, Beta = .9)\n\n\n# Plot loss over iterations\nplt.plot(range(1, len(loss_vec) + 1), loss_vec, color = \"red\")\nplt.semilogx()\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Loss (binary cross entropy)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDiscussion"
  },
  {
    "objectID": "posts/Blog 6/index.html",
    "href": "posts/Blog 6/index.html",
    "title": "Blog 6 - Implementing Sparse Kernel Logistic Regression",
    "section": "",
    "text": "Abstract\nIn this post, we explore Sparse Kernel Logistic Regression (SKLR) as a method for binary classification on nonlinear datasets. We implement SKLR using an RBF kernel and investigate the effects of kernel hyperparameters on model performance. By generating training and testing data with the make_moons function, we show how improper kernel choices ‚Äî particularly a very high gamma value ‚Äî can lead to overfitting. Through ROC curve analysis and Area Under the Curve (AUC) scores, we demonstrate that while the model fits the training data closely, its generalization to unseen data deteriorates significantly. These results highlight the importance of balancing kernel flexibility and regularization strength to build models that generalize well beyond the training set.\n\n\nLoading The Model and Packages\nKernelLogisticRegression Source Code: https://raw.githubusercontent.com/Nibsquatch/Nibsquatch.github.io/refs/heads/main/KernelLogistic.py\n\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport matplotlib.pyplot as plt\nfrom KernelLogistic import KernelLogisticRegression\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n\nImplementation\n\n# Define a function to generate classification data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n# Define a function to plot the classification data\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# Generate and plot roughly linearly separable data\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nFrom the plot, it is clear that it this data is on the edge between linearly separable and linearly inseparable. Lets explore how Sparse Kernel Logistic Regression is able to classify these data points.\n\n# first lets define a standard kernel\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n# instantiate a model with the given kernel and random parameters\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X, y, m_epochs = 100000, alph = 0.001)\n\nLet‚Äôs inspect the entries of the model weight vector to see if they have remained close to zero even after training.\n\n# look at the mean of the weight vecctors\nprint(\"The mean of the weight vector a is: \" + str(KR.a.mean().item()))\nprint(\"The number of entries in a is: \" + str(len(KR.a)))\n\nThe mean of the weight vector a is: 1.2202486686874181e-05\nThe number of entries in a is: 100\n\n\nThus, across 100 entries in a the mean weight vector value is remarkably close to 0. Thus, our Sparse Kernel Logisitic Regression has maintained the property that many entries are equal to or close to zero. Lets plot the scores along with the training data.\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nThis plot highlights that there are sparse points in the traiing data that have a weights distinguishable from zero, highlighted in black.\n\n\nExperiments\nLets explore how the size of lambda effects the outcome of the model, especially with respect to weight sizes.\n\n# instantiate a model with the given kernel and an unreasonably large lambda\nKR = KernelLogisticRegression(rbf_kernel, lam = 1000, gamma = 1)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X, y, m_epochs = 100000, alph = 0.001)\n\nLet‚Äôs inspect the entries of the model weight vector to see if they have remained close to zero even after training.\n\n# look at the mean of the weight vecctors\nprint(\"The mean of the weight vector a is: \" + str(KR.a.mean().item()))\nprint(\"The number of entries in a is: \" + str(len(KR.a)))\n\nThe mean of the weight vector a is: 0.009166927076876163\nThe number of entries in a is: 100\n\n\nWhile the weight vectors have remained very close to 0, they are noticabely larger for a large lambda than they were for a smaller lambda. Lets visualize the new decision region to determine how the large lambda affects the distinguishability of weight vectors from zero.\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nA larger lambda seems to have increased the number of weight vectors which are distinguisable from 0 by a large amount, in fact, it seems that every point now has a weight distinguishable from zero. Lets now try an unnusually small lambda.\n\n# instantiate a model with the given kernel and an unreasonably small lambda\nKR = KernelLogisticRegression(rbf_kernel, lam = .00001, gamma = 1)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X, y, m_epochs = 100000, alph = 0.001)\n\n\n# look at the mean of the weight vecctors\nprint(\"The mean of the weight vector a is: \" + str(KR.a.mean().item()))\nprint(\"The number of entries in a is: \" + str(len(KR.a)))\n\nThe mean of the weight vector a is: 0.00044106869609095156\nThe number of entries in a is: 100\n\n\nWhile the weight vectors have remained very close to 0, they are noticabely larger for an extremely small lambda than they were for a normal lambda. Lets visualize the new decision region to determine how the extremely small lambda affects the distinguishability of weight vectors from zero.\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nSimilar toa larger lambda, an extremely small lambda seems to have increased the number of weight vectors which are distinguisable from 0 by a large amount, in fact, it seems that every point now has a weight distinguishable from zero. Now, lets examine how gamma can affect the shape of the decision boundaries.\n\n# instantiate a model with the given kernel and a larger gamma\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X, y, m_epochs = 100000, alph = 0.001)\n\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nAs visualized by the plot, the decision regions have been altered drastically simply by increasing gamma by a factor of 10. The decision boundaries are much different in size and shape, and have less smooth edges. Lets now see what happends if we decrease gamma by a factor of 10.\n\n# instantiate a model with the given kernel and a larger gamma\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = .1)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X, y, m_epochs = 100000, alph = 0.001)\n\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nDecision boundaries for a smaller gamma have been compressed and smoothed out although they still retained their approximate size and location for each classification.\n\n\nKernel Methods on Nonlinear Patterns\nLets load a function to generate nonlinear data and test the ideal parameters for kernel regression.\n\nfrom sklearn.datasets import make_moons\n\n# Generate nonlinear data using the make_moons function from sklearn\nX, y = make_moons(n_samples = 100, noise = .2)\n\n# Convert X, y to torch tensors\nX = torch.from_numpy(X).float()\ny = torch.from_numpy(y).float()\n\n# Plot our nonlinear data\n# Generate and plot roughly linearly separable data\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 7\n      4 X, y = make_moons(n_samples = 100, noise = .2)\n      6 # Convert X, y to torch tensors\n----&gt; 7 X = torch.from_numpy(X).float()\n      8 y = torch.from_numpy(y).float()\n     10 # Plot our nonlinear data\n     11 # Generate and plot roughly linearly separable data\n\nNameError: name 'torch' is not defined\n\n\n\nAs we can see, our make_moons function has generated two distinct half crescent moons correlated to our two different classes. This is quite starkly nonlinear data. Lets see how kernelized logistic regression performs.\n\n# instantiate a model with the given kernel and reasonable parameters (higher gamma to allow wiggle)\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 5)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X, y, m_epochs = 100000, alph = 0.001)\n\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nUsing a reasonably large gamma allows the decision boundaries to shape the crescent moons fairly well. Lets now try this with training and testing data.\n\nfrom sklearn.model_selection import train_test_split\n\n# Generate training and testing data\nX, y = make_moons(n_samples = 200, noise = 0.2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n\n# Convert X, y to torch tensors\nX_train = torch.from_numpy(X_train).float()\ny_train = torch.from_numpy(y_train).float()\nX_test = torch.from_numpy(X_test).float()\ny_test = torch.from_numpy(y_test).float()\n\n\n# instantiate a model with the given kernel and reasonable parameters (higher gamma to allow wiggle)\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 5)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X_train, y_train, m_epochs = 100000, alph = 0.001)\n\n\n# plot the decision region and highlight points which have weights distinquishable from zero\nix = torch.abs(KR.a).view(-1) &gt; 1e-4\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X_train[ix, 0],X_train[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nIt seems like the model has found a reasonable decision boundary for the data. Lets see how the testing accuracy stacks up.\n\n# make predictions on the training data\npreds = KR.predict(X_train)\nacct = (1.0*(preds == y_train)).mean().item()\n\n# make predictions on the testing data\npreds = KR.predict(X_test)\naccv = (1.0*(preds == y_test)).mean().item()\n\nprint(\"The model has training accuracy: \" + str(acct))\nprint(\"The model has testing accuracy: \" + str(accv))\n\nThe model has training accuracy: 0.49459999799728394\nThe model has testing accuracy: 0.5095999836921692\n\n\nWhile it seems like the model is finding a reasonable decision boundary, the training and testing accuracy are hardly better than 50% which means that our model is not learning anything important about the data.\n\n\nOverfitting\nLets now explore how change the parameter gamma can lead to overfitting by taking advantage of the make_moons function and by plotting the ROC curves across the iteratons of the loop.\n\n# Generate nonlinear training and testing data\nX_train, y_train = make_moons(n_samples=100, noise=0.2)\nX_test, y_test = make_moons(n_samples=100, noise=0.2)\n\n# Convert to torch tensors\nX_train = torch.from_numpy(X_train).float()\ny_train = torch.from_numpy(y_train).float()\nX_test = torch.from_numpy(X_test).float()\ny_test = torch.from_numpy(y_test).float()\n\n# We will use a RBF kernel (too large gamma -&gt; overfit)\ndef rbf_kernel(X1, X2, gamma):\n    return torch.exp(-gamma * torch.cdist(X1, X2)**2)\n\n# instantiate and train a model on the training data with an obscenely large gamme\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 1000)\n\n# Fit the data with 100000 training iterations and a reasonably small alpha\nKR.fit(X_train, y_train, m_epochs = 100000, alph = 0.001)\n\n\n# import some functions from sklearn to help\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# get the sigmoid of the scores to represent probabilities to be used for ROC calculation\nscores = KR.sig(KR.score(X_test))\n\nfpr, tpr, thresholds = roc_curve(y_test.numpy(), scores)\n\n# Plot the ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label = f'AUC = {auc(fpr, tpr):.2f}')\nplt.plot([0, 1], [0, 1], 'k--')  # dashed diagonal line\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Kernel Logistic Regression')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC curve for Kernel Logistic Regression with gamma = 1000 shows poor model performance, achieving an AUC of only 0.59. This result indicates that the model performs only slightly better than random guessing. The high gamma value causes the RBF kernel to become overly sensitive to individual data points, leading to overfitting and poor generalization to new data. To improve performance, it is necessary to reduce gamma significantly and consider tuning regularization or exploring alternative kernels.\n\n\nDiscussion\nIn this project, we implemented Sparse Kernel Logistic Regression to classify nonlinear data, focusing on how different kernel parameters affect model performance. Changing lambda also resulted in more of the weight vectors erring from zero, which we had hoped to keep close to zero. Larger gamma tended to cause the decision bounds to fit the training data more accurately, often leading to overfitting while smaller lambda‚Äôs tended to cause the model to lose the patterns in the data. Through visualization and ROC curve analysis, we observed that high gamma values, such as 1000, led the model to overfit, fitting the noise in the training data while performing poorly on new data. This process highlighted the importance of carefully tuning hyperparameters and evaluating models with metrics like AUC that capture performance across all decision thresholds. Overall, this project helped us deepen our understanding of kernel methods, regularization, and the balance between model complexity and generalization."
  },
  {
    "objectID": "posts/Blog 4/index.html",
    "href": "posts/Blog 4/index.html",
    "title": "Blog 4 - Perceptron",
    "section": "",
    "text": "Abstract\nThe standard Perceptron algorithm iteratively updates a weight vector for binary classification, with each iteration running in O(p) time, where p is the number of features. While the number of data points (n) does not affect individual updates. Experiments show that the Perceptron always converges on linearly separable data, though iteration count varies. Notably, higher-dimensional data converges faster, suggesting that additional features aid optimization. The algorithm will never converge on linearly separable data and demonstrates high volatility in the loss after each update. When Perceptron is implemented with minibatch, this both allows for smoother convergence as well as the potential to converge on linearly inseparable data but increasing the worst case runtime to O(kp), where k is the size of the batch.\n\n\nMethods Overview\nPerceptron source code used in this project: https://raw.githubusercontent.com/Nibsquatch/Nibsquatch.github.io/refs/heads/main/perceptron.py\nPerceptron.grad() was implemented by taking as input a randomely selected batch of entries in the feature matrix and the associated target vector. The target vector comes in the form {0,1} and is converted to {-1, 1} respectively. The score for the entry is computed, and if the score is negative then a perceptron update is passed, otherwise an update containing only a zero shift is passed. When there are more than one entries in the batch, the updates is passed as average of the updates for each misclassified point. A learning rate is also passed, with 1 representing a standard Perceptron update when the batch size also equals 1.\n\n\nLoading Perceptron\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\n\nTesting That Our Perceptron Algorithim Works\nLets run a training loop to check that our Peceptron algorithim is running as expected and will eventually achieve a loss equal to 0 on linearly separable data. First we will need to generate linearly separable data.\n\nfrom matplotlib import pyplot as plt\nimport torch\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\n# first we need to generate data\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\n# assign the example data to variables\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nNow that we have generated linearly separable data, lets visualize before we begin implementing linear decision boundaries.\n\n# Visualize our training data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTime to initiate our training model to test whether or not our Perceptron Algorithim can achieve a loss of 0.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point batch\n    k = 1 # k = 1 one to serve as a standard perceptron update\n    ix = torch.randperm(X.size(0))[:k]\n    x_ix = X[ix,:]\n    y_ix = y[ix]\n    \n    # perform a perceptron update using the random data point (with alpha = 1 to perform standard perceptron update)\n    opt.step(x_ix, y_ix, 1)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAlthough it took a large number of iterations, our perceptron algorithim was eventually able to reach a loss of 0 and is able to identify the proper weight vector to linearly separate the data points. Lets look at our data with the final weight vector plotted.\n\n# define a function to plot a line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot the existing data and add a line to the graph\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\n\nplot_perceptron_data(X, y, ax) # plot data\ndraw_line(p.w, -.5, 1.5, ax, color = \"black\") # draw weight vector\n\n\n\n\n\n\n\n\nFrom the visualization, we can tell that the minimization of loss 0 has correctly identified a linear line which perfectly separates the data into their correct classes. Now, let‚Äôs explore the algorithim performs on data which is mathematically impossible to seprate linearly. A max iterations of 1000 will be applied to the training loop, as the algorithim will run forever without a cap.\n\n# Generate Linearly Inseparable Data\ndef perceptron_data_insep(n_points = 270, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # add 30 points that are the opposite class in a similar location as a previous point, thus making linearly inseparable data\n    for _ in range(30):  \n        rand_i = torch.randint(0, n_points, (1,))  # Pick a random point index\n        x_new = X[rand_i] + torch.normal(0.0, noise, size=(1, p_dims + 1))  # Slightly modify the chosen point\n        \n        # flip class\n        y_new = torch.tensor([1 - y[rand_i].item()])  # Convert to tensor\n\n        X = torch.cat((X, x_new), dim=0)  # Add new point\n        y = torch.cat((y, y_new), dim=0)  # Add new label\n\n    return X, y\n\nNow that we have generated linearly inseparable data, lets visualize before we attempt to implement a linear decision boundaries.\n\n# Visualize our training data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data_insep()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTime to initiate a training model on linearly inseparable data, with a max iterations of 1000.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\niterations = 0\n\nwhile loss &gt; 0 and iterations &lt; 1000: # terminates after 5,000 iterations since the data is linearly inseparable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point batch\n    k = 1 # k = 1 one to serve as a standard perceptron update\n    ix = torch.randperm(X.size(0))[:k]\n    x_ix = X[ix,:]\n    y_ix = y[ix]\n    \n    # perform a perceptron update using the random data point (with alpha = 1 to perform standard perceptron update)\n    opt.step(x_ix, y_ix, 1)\n\n    # iterate\n    iterations += 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nCompared to the linearly separable data, the algorithim has significant trouble minimizing the loss. Instead of a generally smooth curve, the loss function jumps from low to high as there is never a solution to the problem the algorithim is trying to solve. Lets look at our data with the final weight vector plotted, despite the weight vector being imperfect.\n\n# plot the existing data and add a line to the graph\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\n\nplot_perceptron_data(X, y, ax) # plot the data\ndraw_line(p.w, -.5, 1.5, ax, color = \"black\") # plot the final weight vector\n\n\n\n\n\n\n\n\nAs expected, our model is not able to perfectly differentiate between classes, as there is no linear classifier that can accomplish the task. Now lets explore how the loss function evolves on data of more than two dimenstions.\n\n# Generate 5 dimensional data\ndef perceptron_data5(n_points = 300, noise = 0.2, p_dims = 5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    return X, y\n\nX, y = perceptron_data5()\n\n# perform the training loop\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point batch\n    k = 1 # k = 1 one to serve as a standard perceptron update\n    ix = torch.randperm(X.size(0))[:k]\n    x_ix = X[ix,:]\n    y_ix = y[ix]\n    \n    # perform a perceptron update using the random data point (with alpha = 1 to perform standard perceptron update)\n    opt.step(x_ix, y_ix, 1)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThe Perceptron Algorithim is able to work extremely quickly to reduce the loss to 0, even on data with dimensionality greater than 2; in fact, the Perceptron algorithm seems to run even faster for data of higher dimensionality. Lets explore if 10 dimensional data will be optimized even faster.\n\n# Generate 10 dimensional data\ndef perceptron_data10(n_points = 300, noise = 0.2, p_dims = 10):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    return X, y\n\nX, y = perceptron_data10()\n\n# perform the training loop\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point batch\n    k = 1 # k = 1 one to serve as a standard perceptron update\n    ix = torch.randperm(X.size(0))[:k]\n    x_ix = X[ix,:]\n    y_ix = y[ix]\n    \n    # perform a perceptron update using the random data point (with alpha = 1 to perform standard perceptron update)\n    opt.step(x_ix, y_ix, 1)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAlthough the Perceptron algorithim optimized the weight vector for the 10 dimensional data much faster than it did for 2 dimensional data, the rate of optimization does not seem to have a purely linear relationship. In general, multidimensional data does seem to be able to be optimized faster than simple 2 dimensional data.\n\n\nMiniBatch Experiments\nLets now explore how altering the size of the data batch as well as the canging the learing rate affects how quickly the model converges to a loss of 0 on simple, linearly separable data with 2 dimensions. Lets start by comparing k = 1, 10, and n (where n is the size of the dataset) with a constant learing rate of 1.0.\n\n# Generate Fresh Linearly Separable Data\nX, y = perceptron_data()\n\n# instantiate the models and optimizers\np1 = Perceptron()\np10 = Perceptron() \npn = Perceptron() \n\nopt1 = PerceptronOptimizer(p1)\nopt10 = PerceptronOptimizer(p10)\noptn = PerceptronOptimizer(pn)\n\n# for keeping track of loss values\nloss1_vec = []\nloss10_vec = []\nlossn_vec = []\n\nloss = 1.0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss1_vec.append(p1.loss(X,y))\n    loss10_vec.append(p10.loss(X,y))\n    lossn_vec.append(pn.loss(X,y))\n\n    # calculates the average loss of the 3 models\n    loss = (p1.loss(X,y) + p10.loss(X,y) + pn.loss(X,y)) / 3\n    \n    # pick a single random data point batch\n    ix = torch.randperm(X.size(0))[:1]\n    x_1 = X[ix,:]\n    y_1 = y[ix]\n\n    # pick a 10 random data point batch\n    ix = torch.randperm(X.size(0))[:10]\n    x_10 = X[ix,:]\n    y_10 = y[ix]\n\n    # take the whole dataset\n    \n    # perform a perceptron update using the random data point (with alpha = 1) for each model and batch size\n    opt1.step(x_1, y_1, 1)\n    opt10.step(x_10, y_10, 1)\n    optn.step(X, y, 1)\n\nplt.plot(loss1_vec, color = \"slategrey\", label = \"k = 1\")\nplt.plot(loss10_vec, color = \"blue\", label = \"k = 10\")\nplt.plot(lossn_vec, color = \"red\", label = \"k = n\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWith values of k spanning 1, 10, n each form of Perceptron was able to converge to a loss 0 in a similar number of iterations. However, it appears that as k grows larger the rate of convergence beceoms more stable. This is demonstrated as the standard, k = 1, Perceptron algorithim has large changes in loss followed by regions of no improvment. As k grows larger cruves appear to smooth out, and descend slower but more consistently. Now, lets see how a Perceptron algorithm with k = n performs on linearly inseparable data with a sufficiently small alpha.\n\n# Generate fresh linearly inseparable data\nX, y = perceptron_data_insep()\n\n# instantiate the model and optimizer\np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss\nloss_vec = []\n\nloss = 1.0\niterations = 0\n\nwhile loss &gt; 0 and iterations &lt; 25000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using k = n and a small alpha\n    opt.step(X, y, .005)\n\n    iterations += 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nTo spare wasting time and computational powers, the number of iterations was limited to 25,000. However, the overall trend seems to be a gradual convergence to 0 loss at a decreasing rate. It can be assumed that if unlimited iteratins could be allowed then the model would eventually converge to a loss of 0 for minibatch perceptron with a small alpha (such as .005) and k = n.¬†\n\n\nDiscussion\nReflecting on the time complexity of the standard Perceptron algorithm, the algorithim selects a random point which takes O(1) time; although the number of entries (n) may increase this does not affect the runtime of selecting one at random. However, for each Perceptron update the score for this entry must be calculated which is dependent on the number of features (p), as each score is calculated as a dot product. Thus, each iteration of the Perceptron algorithim takes O(p) time. Reflecting on the time complexity of the minibatch Perceptron algorithm, each iteration processes a batch of ùëò k examples, rather than a single one. For each example in the batch, the algorithm computes a dot product between the weight vector and the input, which takes O(p) time per example. Since there are k examples, the total time for computing predictions is O(kp). If all examples are misclassified, the algorithm may need to update the weights using each one, again requiring O(kp) time in the worst case. Thus, each iteration of the minibatch Perceptron algorithm takes O(kp) time.\nFindings: When given linearly separable data the standard Perceptron algorithm will always find a weight vector that achieves a loss of 0, even if it takes in some cases thousands of iterations; the time it takes for the Perceptron algorithm to optimize the weight vector also seems to have some relationshop to the number of dimensions in the data, with multidimensional data being optimized in much fewer iterations than simple 2 dimensional data. The algorithm will also never converge on non linearly separable data, and the loss fucntion across each Perceptron update demonsrates high volatility, as each update is attempting to reach a solution which does not exist. With regards to minibatch Perceptron, the larger value of k the more stable the increase in accuracy but often the longer it takes to converge. The learning rate alpha can also effect this rate."
  },
  {
    "objectID": "posts/Blog 2/index.html",
    "href": "posts/Blog 2/index.html",
    "title": "Blog 2 - Maximizing Loan Profit",
    "section": "",
    "text": "Introduction\nAccess to bank loans plays a critical role in enabling individuals and businesses to finance important expenses, from home purchases to entrepreneurial ventures. This blog post explores the landscape of bank lending, focusing on the factors influencing loan approval, interest rates, and default risks. Variables such as loan intent were explored across age groups, as well as the variables affecting load grade. High grade loans (A) are associated with individuals at low risk of defaulting. Through exploratory data analysis income, loan amount, loan as a percent of income, loan interest rate, and past default hisotry were identified as variables most stronly associated with loan quality. Weigthed variables that took into account past default history as well as percent income, loan amount, and interst rate were also generated. Linear Regression was then used to determine the combination of these variables that could most accurately predict whether or not a loaner would default. The weight vector for these variables was taken to build a linear score function, and the score function threshold was optimized to maximize bank profit per buyer in the final model. In the final model, loan as a percent of income, loan interest weight, weighted interest rate, and past default history were selected as predictors with a threshold of t = 2.2, resulting in an expected profit per buyer of 1294.33.\n\n\nData Exploration\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# load the raw bank data and clean it\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n# drop rows with NA values\ndf_train = df_train.dropna()\n\n# convert the interest rate to a percent\ndf_train[\"loan_int_rate\"] = df_train[\"loan_int_rate\"] / 100\n\n# make a copy of the data to use and view\ndf_ = df_train.copy()\ndf_.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n0.1491\n1\n0.25\nN\n2\n\n\n\n\n\n\n\nLets explore how loan intent varies across age groups\n\n# first lets add a column for age groups by creating age groups with 10 year ranges (ex. 20 - 29 is group 2)\ndf_[\"age_group\"] = df_[\"person_age\"] // 10\ndf_[\"age_group\"][df_[\"age_group\"] &gt;= 10 ] = 10 # group all loaners older than 100 into a single group\n\n# Get the counts of each loan intent within each age group\nloan_intent_counts = df_.groupby(\"age_group\")[\"loan_intent\"].value_counts().unstack()\nloan_intent_counts = loan_intent_counts.fillna(0) # fill NA values with 0 instead\n\n# Convert counts to proportions relative to each age group\nloan_intent_prop = loan_intent_counts.div(loan_intent_counts.sum(axis=1), axis=0)\n\n# conert the table into long format so that each loan intent can be plotted for each age group\ntemp = pd.melt(loan_intent_prop)\n\ntemp[\"age_group\"] = 0 # rebuild the age group category exactly as calculated previosly\nfor i in range(0, len(temp) + 1):\n  temp[\"age_group\"][i] = (i % 9) + 2\n\n# plot loan intent as a proportion of each age group\nax = sns.barplot(data = temp, x = \"age_group\", y = \"value\", hue = \"loan_intent\")\nax.set_xlabel(\"Age Group\")\nax.set_ylabel(\"Proportion\")\nax.set_title(\"Loan Intent by Age\")\n\nText(0.5, 1.0, 'Loan Intent by Age')\n\n\n\n\n\n\n\n\n\nThe chart indicates that personal loans are most common among individuals aged 20-29, 40-49, and 90-99, suggesting a consistent need for general-purpose borrowing across different life stages. Medical loans peak in the 40-49 and 50-59 age groups, likely due to increasing healthcare expenses. Education loans spike in the 70-79 age group, which may indicate late-stage career shifts or funding for dependents. Debt consolidation loans are relatively steady, with moderate representation in the 30-39, 50-59, and 60-69 age groups. Home improvement loans have lower proportions overall but see slight increases among borrowers aged 20-29 and 80-89. Venture loans are sporadic, with small peaks in the 50-59 and 60-69 age groups, possibly reflecting mid-life entrepreneurial activity. These trends highlight how borrowing needs evolve across different age brackets; lets explore income relates to loan size.\n\n# make a copy of the data frame to work with and create income categories similar to our age group categories (ex. $0 - $99,999 is group 0)\ndf_2 = df_train.copy()\ndf_2[\"income_cat\"] = df_2[\"person_income\"] // 50000\ndf_2[\"income_cat\"][df_2[\"income_cat\"] &gt;= 10] = 10 # group all income creater than $1,000,000 into a single group\n\n# find the mean loan amount in each income category\nprint(df_2.groupby(\"income_cat\")[\"loan_amnt\"].mean().round())\n\nincome_cat\n0      7061.0\n1     10485.0\n2     13356.0\n3     15176.0\n4     16728.0\n5     19252.0\n6     18127.0\n7     20516.0\n8     19615.0\n9     17306.0\n10    14535.0\nName: loan_amnt, dtype: float64\n\n\nWhile salary (income category) does seem to affect the loan amount, with people having higher salaries taking out larger loans, the higher income categories actually often have lower loan amounts than the lower income categories. The spread of loan amount vs income fits a in some ways a bell curve. This may be a product of people making larger salaries not needing to take out loans as frequently as the middle salaries, which can afford to take out larger loans and pay them back. Let‚Äôs also determine what factors seem to be related to higher interest rates.\n\n# group the data by loan grade and view the mean interest rates for each loan grade\nprint(df_train.groupby([\"loan_grade\"])[\"loan_int_rate\"].mean())\n\nloan_grade\nA    0.073384\nB    0.110033\nC    0.134562\nD    0.153583\nE    0.170473\nF    0.185190\nG    0.202300\nName: loan_int_rate, dtype: float64\n\n\nHigh grade loans (loans which are expected to be paid back) are associated with the lowest interest rates. There seems to be a direct correlation between loan grade and interest rate. Lets explore factors related to loan grades by grouping.\n\n# group the data by loan grade and view the mean income, mean loan amount, and mean percent income for each load grade\ntemp = df_train.groupby(\"loan_grade\")[[\"person_income\", \"loan_amnt\", \"loan_percent_income\"]].mean()\n\nprint(temp)\n\n            person_income     loan_amnt  loan_percent_income\nloan_grade                                                  \nA            66773.007816   8555.884885             0.152629\nB            66662.091096  10031.025007             0.173846\nC            66416.633130   9322.102794             0.168928\nD            64555.473908  10821.646695             0.188833\nE            70868.349432  12929.083807             0.204190\nF            80756.546012  15395.705521             0.220982\nG            77342.477273  17384.659091             0.243409\n\n\nAs we can see from this datatable, high load grade is directly correlated with lower percent of total income. Meaning that higher quality loans, with lower interest rates, are associated with loaners whose loan takes up the smallest percent of their total income. This is likely related to the loan taking up a lower percent of their income making it less likely for them to default on the loan and be more capable of paying it back. It also seems to follow that people with higher income tend to take out larger loans than would correlate with their increase income.\n\n\nFeature Selection & Weight Assessment\nSelect the predictor and target variables related to loan grade.\n\nfrom sklearn.model_selection import train_test_split\n\n# select the predictor variables identified with loan grade. Also the include the loaners default history\npredictor_cols = [\"person_income\", \"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\", \"cb_person_default_on_file\"]\nX_model = df_train[predictor_cols].copy()  # Use .copy() to avoid modifying the original DataFrame\n\n# target variable selection\ny_model = np.array(df_train[\"loan_status\"])\n\n# Create binary indicators for past defaults\nX_model[\"past_default_yes\"] = X_model[\"cb_person_default_on_file\"] == \"Y\"\nX_model[\"past_default_no\"] = X_model[\"cb_person_default_on_file\"] == \"N\"\n\n# Generate weighted default predictor\nX_model[\"weighted_default\"] = X_model[\"loan_percent_income\"] * (1 +  X_model[\"past_default_yes\"])\n\n# Generate weigthed interest rate predictor\nX_model[\"weighted_interest\"] = (1 + X_model[\"loan_int_rate\"]) * X_model[\"loan_amnt\"] * (1 + X_model[\"past_default_yes\"])\n\n# Drop the original categorical column\nX_model.drop(\"cb_person_default_on_file\", axis=1, inplace=True)\n\n# Breaking into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.2)\n\nFit a Linear Regression model to the data and assess its accuracy of prediction. Try all combinations of predictors. Once a good model is fit, pull out the weight coefficient to use for a linear score based classifier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom itertools import combinations\nfrom sklearn.metrics import accuracy_score\n\n# create arrays to hold all the predictors we will consider\nnumerical_predictors = [\"person_income\", \"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\", \"weighted_default\", \"weighted_interest\"]\nqual_predictors = [\"past_default_yes\", \"past_default_no\"]\npredictor_cols = numerical_predictors + qual_predictors\n\n# create an empty array to store the accuracy results for predictor combinations\nresults = []\n\n# For each combination of predictor variables train a Linear Model and assess its accuracy\nfor r in range(1, len(predictor_cols) + 1): # From 1 feature to all features\n    for subset in combinations(predictor_cols, r): # For each combination of those features\n        \n        # assign the test columns\n        test_cols = list(subset)\n        \n        # Select subset of features for the training and testing data\n        X_train_subset = X_train[test_cols]\n        X_test_subset = X_test[test_cols]\n        \n        # Train a simple logistic regression model to the subset\n        model = LogisticRegression(max_iter=1000)\n        model.fit(X_train_subset, y_train)\n\n        # Predict and evaluate\n        y_pred = model.predict(X_test_subset)\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        # Store results\n        results.append({\"features\": test_cols, \"accuracy\": accuracy})\n\nFrom the list of feature combinations, pull out the top 5 and retest them against the premade training and testing data. Once the best from this list has been identified, verify its accuracy with cross validation to check for overfitting. If the features pass, pull out the weights coefficient to use for a linear score based classifier.\n\n# select the top 5 feature combinations by accuracy\nbest_features = pd.DataFrame(results).sort_values(by = \"accuracy\", ascending = False).head(5)[\"features\"].tolist()\n\n# create an empty array to store the features that will ultimately be selected for the model as well as variable to hold the features testing accuracy\ntrue_predictors = []\nbest_acc = 0\n\n# For each of the top 5 features train a Linear Model and asses its ability to predict on the testing data. Select the features with the highest predictive accuracy as the features to use for bank profit maximization\nfor features in best_features:\n    # fit a model to the feature list\n    LR_test = LogisticRegression()\n    m_test = LR_test.fit(X_train[features], y_train)\n\n    # use the model to generate predictions on the testing data\n    y_pred = LR_test.predict(X_test[features])\n\n    # assess the accuracy of the models predictions against the testing data\n    acc = accuracy_score(y_test, y_pred)\n\n    # if the current features are better than the previous best, reassign the new best\n    if acc &gt; best_acc:\n        best_acc = acc\n        true_predictors = features\n\nprint(\"The predictors with the highest testing accuracy are: \" + str(true_predictors) + \"\\nAccuracy = \" + str(best_acc))\n\n# Check with cross validation\nLR_real = LogisticRegression()\nm_real = LR_real.fit(X_train[true_predictors], y_train)\n\ncv_scores_LR = cross_val_score(LR_real, X_test[true_predictors], y_test, cv = 5)\n\nprint(\"Cross validation for the selected predictors: \" + str(cv_scores_LR))\n\n# store the weight coefficients for this model to build our linear score function\nw = m_real.coef_\n\n# store the dimenstions the of weight vector as it must be reshaped to a n x 1 instead of a 1 x n\ns = w.shape\n\n# reshape the weight vector\nw = w.reshape(s[1], s[0])\n\nThe predictors with the highest testing accuracy are: ['loan_percent_income', 'loan_int_rate', 'weighted_interest']\nAccuracy = 0.8347883020515059\nCross validation for the selected predictors: [0.82878953 0.81788441 0.8220524  0.81659389 0.82969432]\n\n\nUpon inspecting accuracy versus the overall test set income the predictors above were selected as the best variables based purely on accuracy. These predictors also had reasonable cross validation scores that were similar to the overall testing accuracy; this implies that the model is consisent and not overfit. The weight constants from this model were then pulled out and stored for later used in a linear score function.\n\n\nOptimizing a Linear Score Classification\nStart by defining our linear score function using our weight vector and our classification function with an arbitrary starting threshold. We will then optimize this threshold such that the bank makes the largest possible profit per loan.\n\nfrom matplotlib import pyplot as plt\n\n# Performs the dot product between the data vector X and the weight vector w\ndef score(X, w):\n    return np.dot(X,w)\n\n# Returns the predictions of the model. 1 if the loaner is predicted to default, and 0 if they are predicted to repay the loan\ndef predict(score, threshold, df):\n  scores = score(df, w)\n  return 1*(scores &gt; threshold)\n\n# pull out model data\nX_model = X_train.copy()\n\n# initialize the benefit column\nX_model[\"benefit\"] = 0\n\n# initialize variables to store the best benefit and threshold through iteration\nbest_benefit = 0\nbest_threshold = 0\n\n# assign scores to each individual based on the features selected previously and the weight vector associated with these features\nscores = score(X_model[true_predictors], w)\n\n# generate a plot to visualize how profit changes as threshold changes\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n# for thresholds between 0 and 10\nfor t in np.linspace(0, 10, 101):\n    \n    # assign predictions based on the threshold\n    X_model[\"y_pred\"] = scores &gt;= t\n\n    # calculate whether each indivdual is a true negative or a false negative\n    X_model[\"tn\"] = (X_model[\"y_pred\"] == 0) & (y_train == 0)\n    X_model[\"fn\"] = (X_model[\"y_pred\"] == 0) & (y_train == 1)\n    \n    # calculate the gain/loss for each individual based on whether they are considered a true negative or a false negative\n    X_model[\"benefit\"][X_model[\"tn\"]] = X_model[\"loan_amnt\"] * (1 + 0.25 * X_model[\"loan_int_rate\"]) ** 10 - X_model[\"loan_amnt\"]\n    X_model[\"benefit\"][X_model[\"fn\"]] = X_model[\"loan_amnt\"] * (1 + 0.25 * X_model[\"loan_int_rate\"]) ** 3 - 1.7 * X_model[\"loan_amnt\"]\n\n    # sum the total benefit across all invididuals and divide by the total number of individuals to find the average profit per loaner\n    average_benefit = np.sum(X_model[\"benefit\"]) / len(X_model)\n\n    # add this point to the cumulative graph\n    ax.scatter(t, average_benefit, color = \"steelblue\", s = 10)\n\n    # if the current benefit is bette than the previous best, reassign the benefit and store the current threshold\n    if average_benefit &gt; best_benefit:\n        best_benefit = average_benefit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net benefit\", title = f\"Best benefit ${best_benefit:.2f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nMaximization of the bank‚Äôs profit results in a threshold of 4.7, with a profit per buyer of 1308.88. The value of the threshold has been stored for later use. Lets now test the model with our optimal weight vector and threshold against new testing data.\n\n# load the testing data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n# drop NA columns and convert the interest rate to percent\ndf_test = df_test.dropna()\ndf_test[\"loan_int_rate\"] = df_test[\"loan_int_rate\"]  /100\n\n# Create binary indicators for past defaults\ndf_test[\"past_default_yes\"] = df_test[\"cb_person_default_on_file\"] == \"Y\"\ndf_test[\"past_default_no\"] = df_test[\"cb_person_default_on_file\"] == \"N\"\n\n# Generate weighted default predictor\ndf_test[\"weighted_default\"] = df_test[\"loan_percent_income\"] * (1 +  df_test[\"past_default_yes\"])\n\n# Generate weigthed interest rate predictor\ndf_test[\"weighted_interest\"] = (1 + df_test[\"loan_int_rate\"]) * df_test[\"loan_amnt\"] * (1 + df_test[\"past_default_yes\"])\n\n# Drop the original categorical column\ndf_test.drop(\"cb_person_default_on_file\", axis=1, inplace=True)\n\n# initialize the benefit column\ndf_test[\"benefit\"] = 0\n\n# calculate the score array\nscores = score(df_test[true_predictors],w)\n\n# calculate predictions based on the optimal threshold\ndf_test[\"y_pred\"] = scores &gt;= best_threshold\n\n# determine if each individual can be considered either a true negative or a false negative\ndf_test[\"tn\"] = (df_test[\"y_pred\"] == 0) & (df_test[\"loan_status\"] == 0)    \ndf_test[\"fn\"] = (df_test[\"y_pred\"] == 0) & (df_test[\"loan_status\"] == 1)\n\n# calculate the gain / loss for each individual based on whether they are true negative or a true positive\ndf_test[\"benefit\"][df_test[\"tn\"]] = df_test[\"loan_amnt\"] * (1 + 0.25 * df_test[\"loan_int_rate\"]) ** 10 - df_test[\"loan_amnt\"]\ndf_test[\"benefit\"][df_test[\"fn\"]] = df_test[\"loan_amnt\"] * (1 + 0.25 * df_test[\"loan_int_rate\"]) ** 3 - 1.7 * df_test[\"loan_amnt\"]\n\n\n# calculate the average profit per loaner based on the total benefit and the number of loaners\naverage_benefit = np.sum(df_test[\"benefit\"]) / len(df_test)\n\nprint(\"The expected profit per buyer is: $\" + str(round(average_benefit, 2)))\n\nThe expected profit per buyer is: $1239.44\n\n\nThe expected profit per buyer on the test set (1239.44) is only slightly lower than the expected profit per buyer on the training set (1308.88). Thus our model was able to generalize from the training set.\n\n\nEvaluating Model Fairness\nLets explore if some loaners are more or less likely to receive loans based on their age\n\n# remake age groups in 10 year gaps and calculate the difference in default prediction vs reality\n# age group 2 represents loaners aged between 20 and 29 years old\ndf_test[\"age_group\"] = df_test[\"person_age\"] // 10\n\n# calculates the difference between prediction and actual outcome of loan status (1 means predicted to default and did not, 0 represents accurate prediction, -1 represent failure to predict default)\ndf_test[\"diff\"] = df_test[\"y_pred\"] - df_test[\"loan_status\"]\n\n# Group by age and view mean prediction, actual default status, and the mean difference\ntemp = df_test.groupby(\"age_group\")[[\"loan_status\", \"y_pred\", \"diff\"]].mean().reset_index()\ntemp\n\n\n\n\n\n\n\n\nage_group\nloan_status\ny_pred\ndiff\n\n\n\n\n0\n2\n0.228689\n0.189085\n-0.039604\n\n\n1\n3\n0.204082\n0.153846\n-0.050235\n\n\n2\n4\n0.200000\n0.119231\n-0.080769\n\n\n3\n5\n0.261905\n0.095238\n-0.166667\n\n\n4\n6\n0.500000\n0.500000\n0.000000\n\n\n5\n7\n0.500000\n0.000000\n-0.500000\n\n\n\n\n\n\n\nIn almost all age groups, other than age group 6, the algorithm underpredicts the loan default rate. However, two age groups which are less likely to get a loan than other groups (excluding group 7 as it only has two individuals), are age group 20-29 and 30-39. Despite, the algorithim underpredicting the default rate in this age group these groups are still predicted to be much more likely to defualt than the other age groups. The algorithm also seems to underpredict default rates more as loaners grow older. Lets now look at the difficulty of getting loans based on loan intent.\n\n# create a copy dataset to work with\ntemp2 = df_test.copy()\n\n# convert the data to long format so that both prediction and actual default outcome can be viewed for each loan intent\ntemp2_melted = pd.melt(temp2, id_vars = \"loan_intent\", value_vars = [\"y_pred\", \"loan_status\"], var_name = \"Type\", value_name = \"Proportion of Loan Status\")\n\n# Generate a bar plot of defualt prediction rate  vs actual default rate for each loan type\nsns.barplot(data = temp2_melted, y = \"loan_intent\", x = \"Proportion of Loan Status\", hue = \"Type\", palette = \"BuPu\", saturation = 0.5, orient = \"h\", errorbar = None)\n\n\n\n\n\n\n\n\nMedical, Debt Consolidation, and Personal loans have the highest predicted default rate making them the hardes loans to get; however both Medical and Debt Consolidation loans do have the highest actual default rate. Education and Venture are the next hardest loans to get. Interestingly, both Venture and Education loans are preidcted to default more than they actually do and are the only loan type to display this behavior. Home Improvement loans are predicted to default the least, despite having the third highest actual default rate. Now, lets examine how gross income affects the ease with which credit can be obtained.\n\n# create a copy dataset to work with\ntemp3 = df_test.copy()\n\n# Group income into categories separated by $25,000 in income (ex. group 1: loaners making up to $24,999)\ntemp3[\"income_group\"] = temp3[\"person_income\"] // 25000\ntemp3[\"income_group\"][temp3[\"income_group\"] &gt;= 10 ] = 10 # assign all loaners making $250,000 or more into one group\n\n# convert the data to long format so that both prediction and actual default outcome can be viewed for income group\ntemp3_melted = pd.melt(temp3, id_vars = \"income_group\", value_vars = [\"y_pred\", \"loan_status\"], var_name = \"Type\", value_name = \"Proportion of Loan Status\")\n\n# Generate a bar plot of defualt prediction rate vs actual default rate for each income group\nsns.barplot(data = temp3_melted, x = \"income_group\", y = \"Proportion of Loan Status\", hue = \"Type\", palette = \"BuPu\", saturation = 0.5, errorbar = None)\n\n\n\n\n\n\n\n\nIn this model, lower income is strongly associated with greater likelihood to default, making it much harder get loans at low income. While lower income is generally associated with higher default rates, it is unrealistic that the model predicts loaners making more than 75,000 to pay back their loans 100% of the time. In fact, the second highest default rate is actually in the 200,000 - 224,999 income category.\n\n\nConclusion\nOur analysis revealed that income stability, credit history, and collateral availability are the most significant determinants in securing a loan. Borrowers with lower credit scores or irregular income streams often struggle to obtain loans or face significantly higher interest rates. While banks aim to mitigate risks through stringent approval processes, these measures inadvertently create barriers for individuals in need, particularly those seeking medical loans or small business financing.\nIndividuals seeking loans for medical expenses often face greater difficulties in obtaining credit due to the high default rates associated with medical debt. From a financial standpoint, banks perceive these loans as high-risk due to the uncertainty surrounding medical outcomes and the borrower‚Äôs ability to repay. However, this raises ethical concerns regarding fairness in credit access. Fairness, in this context, can be defined as the equitable opportunity for all individuals to obtain financial assistance without undue discrimination or excessive barriers.\nGiven that medical expenses are often urgent and unavoidable, it is arguably unjust that those in dire need of healthcare financing encounter significant hurdles. A fairer approach could involve government-backed loan programs, flexible repayment structures, or the integration of medical hardship considerations into lending criteria. By reassessing how medical loans are evaluated, financial institutions can contribute to a system that balances risk management with social responsibility."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 8 - Exploring Advanced Optimization Using Newton‚Äôs Method and Adam\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 7 - Exploring Overfitting as a Result of Overparameterization and Double Descent\n\n\n\n\n\nIn this post we explore how Overparameterization can actually improve accuracy instead of leading to overfitting, especially in the context corruption detection.\n\n\n\n\n\nApr 29, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 6 - Implementing Sparse Kernel Logistic Regression\n\n\n\n\n\nImplementing Logistic Regression using a Sparse Kernel Model\n\n\n\n\n\nApr 28, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 5 - Implementing Logistic Regression\n\n\n\n\n\nImplementing gradient descent to solve emperical risk minimization for Logistic Regression\n\n\n\n\n\nApr 7, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4 - Perceptron\n\n\n\n\n\nImplementing and Testing the Perceptron Algorithim\n\n\n\n\n\nMar 24, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3 - Auditing Bias\n\n\n\n\n\nPredicting employment status in Missouri on the basis of demographics and auditing the fairness of our model.\n\n\n\n\n\nMar 12, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 2 - Maximizing Loan Profit\n\n\n\n\n\nThis blog post identifies variables in individual loaners personal information and history to identify the best variables to fit a linear score based classifier in order to maximize bank profits per buyer.\n\n\n\n\n\nMar 5, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1 - Classifying Palmer Penquins\n\n\n\n\n\nBuilding a machine learning model to classify Palmer Penquins based on physical properties\n\n\n\n\n\nFeb 26, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing whether or not I can edit this"
  },
  {
    "objectID": "posts/Blog 1/index.html",
    "href": "posts/Blog 1/index.html",
    "title": "Blog 1 - Classifying Palmer Penquins",
    "section": "",
    "text": "Abstract\nThis blog utilizes the Palmer Penguins dataset to develop predictive models for determining the species of penguins based on their morphological measurements. The dataset comprises various features, including culmen length and depth, flipper length, and body mass, across three species: Adelie, Chinstrap, and Gentoo. Qualitative features such as Island, Clutch Completion, and Sex are also included. Through visual analysis, features which differed between species were identified and selected for model training. Both Logistic Regression and Decision Trees (not shown) were implemented and evaluated. Model performance was assessed using training accuracy both absolute and through cross validation as well as assessment on separate testing data.\n\n\nData Preparation and Feature Selection\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution.\nLets look at the three quantitative predictor variables and plot the combinations we can make.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Depth vs Culmen Length\")\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth vs Flipper Length\")\n\nsns.stripplot(x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Culmen Length vs Flipper Length\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile Culmen Depth vs Flipper Length provides the clearest specification for Gentoo Penguins, Adelie and Chinstrap become too muddled to properly distinguish. Culmen Length vs Flipper Length does a good job in making three clusters, with Chinstrap isolating the most, howeever there is still overlap in the regions of Gentoo and Adelie.\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoders for categorical variables\nle = LabelEncoder()\nle.fit(df[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare data\ndf_train, y_train = prepare_data(df)\n\n# Visualize our new training data\ndf_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nWith our new training data lets prepare a new data frame only including the predictors we want\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\ncols = []\nfor pair in combinations(all_quant_cols, 2):\n    \n    # Combinations to test training accuracy of\n    cols.append(list(pair) + all_qual_cols)\n\nfor combo in cols:\n    print(combo)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n\nTesting the Model\nNow that we have our combination of predictors, lets test the training accuracy of Linear Regression models on each predictor to determine which is the best.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\ni = 0\nbest_acc = 0\nbest_combo = 1\n\nfor combo in cols:\n    \n    print(\"Testing accuracy for predictors: \" + str(combo) + \"\\n\")\n\n    LR = LogisticRegression(max_iter = 10000)\n    m = LR.fit(df_train[combo], y_train)\n    \n    acc = LR.score(df_train[combo], y_train)\n    \n    if acc &gt; best_acc:\n        best_acc = acc\n        best_combo = i\n    \n    print(\"Iteration \" + str(i + 1) + \" has a training accuracy of: \" + str(acc))\n    cv_scores_LR = cross_val_score(LR, df_train, y_train, cv = 5)\n    print(\"Iteration \" + str(i + 1) + \" also cross validation: \" + str(cv_scores_LR) + \"\\n\")\n    i += 1\n\nprint(\"The best combination of predictors is: \" + str(cols[best_combo]))\n\n# pull out the columns that we want based on the best training accuracy\npredictor_cols = cols[best_combo]\n\nX_train = df_train[predictor_cols]\n\nLR_real = LogisticRegression(max_iter = 10000)\nm_real = LR_real.fit(X_train, y_train)\n\nTesting accuracy for predictors: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 1 has a training accuracy of: 0.99609375\nIteration 1 also cross validation: [1. 1. 1. 1. 1.]\n\nTesting accuracy for predictors: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 2 has a training accuracy of: 0.9765625\nIteration 2 also cross validation: [1. 1. 1. 1. 1.]\n\nTesting accuracy for predictors: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 3 has a training accuracy of: 0.8828125\nIteration 3 also cross validation: [1. 1. 1. 1. 1.]\n\nThe best combination of predictors is: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nAfter analyzing the training accuracy for each combination of predictors, Culmen Length and Culmen Depth have the highest training accuracy, thus we will test this model against the test data. Next we will plot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\n# load the testing data to check the accuracy of our model\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# prep the testing data for later use\nX_test, y_test = prepare_data(test)\nX_test = X_test[predictor_cols]\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR_real, X_train, y_train)\n\n\n\n\n\n\n\n\nThe model seems to be making reasonable decisions for classiying Penquin species on each island. The only island in which the model does not make perfect decisions, is Dream island which is home to all species of Penquins. Next, assess the model against the testing data.\n\n# Check the model accuracy against the testing data\nprint(\"Linear Regression for the model has testing accuracy of: \" + str(LR_real.score(X_test, y_test)))\n\nLinear Regression for the model has testing accuracy of: 1.0\n\n\n100% testing accuracy has been achieved with a Linear Regression model for classifying Penquin Species. Lets view the confusion matrix for our succesfull model as well as look at the decision regions for the model evaluated on the test set.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR_real.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix For the Model:\\n\" + str(C))\n\nplot_regions(LR_real, X_test, y_test)\n\nConfusion Matrix For the Model:\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\n\n\n\n\n\n\n\n100% accuracy has been achieved! We have now developed a model with 100% testing accuracy for identifying Palmer Penquins based on physiologcal characteristics. By plotting the decision regions versus the test data and by looking at the confusion matrix, it becomes clear that the model was able to distinguish every penquin soley based on the provided information. If you ran this code yourself and did not achieve 100% accuracy on one of these models, re-run the code with new training and testing data (this will produce a model with 100% testing accuracy after 2 - 3 tries if not on the first). Since it was mentioned above, lets run the DecisionTreeClassifiers for fun.\n\n\nDiscussion\nThrough the process of analyzing the Palmer Penguins dataset, several key insights were uncovered regarding the classification of penguin species based on their physical characteristics. First, identifying the quantitative features which created the clearest three groupings was key. In fact, more time should have been spent in the beginning graphing out possible combinations of quantitative features; this process may have more quickly identified Culmen Length and Depth as the best features for clustering penquin species together. Plotting tables for the qualitative features also proved key, as it quickly identified that Island was a good indicator of species. In addition, when dealing with models such as the DecisionTreeClassifier (not shown in this post) utilizing cross validation as a check for overfitting helped identify the correct depth range for our models, as lower values procuded worse testing accuracy, but the extremely high complexity values produced overfitted results. Finally, as a general note, I would spend more time in the beginning exploring potential data combinations via graphical methods in order to more quickly identify the predicators which might work best rather than plug and chug; I did, however, enjoy the process of tinkering with the different models and datasets until I found the perfect one."
  },
  {
    "objectID": "posts/Blog 3/index.html",
    "href": "posts/Blog 3/index.html",
    "title": "Blog 3 - Auditing Bias",
    "section": "",
    "text": "Abstract\nThis blog post examines the fairness and accuracy of a machine learning model designed to predict employment status in Missouri using demographic data. By auditing for bias, the analysis assesses disparities in prediction accuracy, false positive rates, and false negative rates across racial groups. The results indicate that the model meets calibtration and statistical parity, but fails to meet error rate balance. The study also explores potential ethical and practical concerns, such as privacy risks, model reliability, and unintended consequences in commercial and governmental applications. Recommendations for mitigating these risks include implementing strict data privacy measures, incorporating human oversight, regularly auditing the model, and ensuring transparency in decision-making processes.\n\n\nData Preparation and Package Installation\nLoading the ACS data from Missouri and selecting the features we want to train on.\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MO\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n# filte the data to only use recommended features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n27\n17.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n42\n19.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n2\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n3\n26\n17.0\n5\n16\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n4\n37\n16.0\n5\n16\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n1\n1.0\n1\n2\n6.0\n\n\n\n\n\n\n\nTransforming the features into a basic problem to address the task of predicting employment status while excluding race as a demographic.\n\n# subset the features we want to use\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n# construct a basic problem to predict employment status without considering race\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\n# separate into a feature matrix, label vector, and a group label vector\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nSplit into a training and test split.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nAssessing Initial Variance\nLets now look at some basic descriptives.\n\nimport pandas as pd\nimport seaborn as sns\n\n# convert the data back into a dataframe to make it easier to work with\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n# how many individuals are there in the data\nprint(\"There are \" + str(len(df)) + \" individuals in the dataset \\n\")\n\n# how many individuals are currently employed\nprint(\"There are \" + str(len(df[df[\"label\"] == 1])) + \" individuals in the dataset are currently employed \\n\")\n\n# how many individuals in each group\nprint(\"The number of individuals in each group: \")\nprint(df.groupby(\"group\").size())\n\n# proportion of individuals in each group that are currently employed\nprint(\"\\nThe proportion of individuals in each group that are currently employed: \")\nprint(df.groupby(\"group\")[\"label\"].mean())\n\n# lets now add sex to the consideration of race and proportion predicted to be employed\ndf[\"group\"][df[\"group\"] &gt; 2] = 3\nax = sns.barplot(data = df, x = \"group\", y = \"label\", hue = \"SEX\", errorbar = None)\nax.set_xlabel(\"Race (White, Black, Other)\")\nax.set_ylabel(\"Proportion Employed\")\n\nThere are 49932 individuals in the dataset \n\nThere are 22306 individuals in the dataset are currently employed \n\nThe number of individuals in each group: \ngroup\n1    43713\n2     3838\n3      150\n4        5\n5       27\n6      762\n7       51\n8      316\n9     1070\ndtype: int64\n\nThe proportion of individuals in each group that are currently employed: \ngroup\n1    0.454396\n2    0.395779\n3    0.460000\n4    0.600000\n5    0.407407\n6    0.493438\n7    0.529412\n8    0.373418\n9    0.299065\nName: label, dtype: float64\n\n\nText(0, 0.5, 'Proportion Employed')\n\n\n\n\n\n\n\n\n\nWhile there is some variance across sex and race in the employment rate, for the most part the employment rate is approximately equal across all groups. Although both white females and males are more employed more than any other subgroup.\n\n\nTraining a Model\nLets train a model using a decision tree classifier with an optimal model complexity. Select model complexity based on the complexity that gets the higest accuracy and the lowest standard deviation.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n# assess 50 model complexities\n\ncomplexity = []\n\nfor i in range(1, 50, 1):\n    # fit a model with the given complexity\n    m = DecisionTreeClassifier(max_depth = i)\n    m.fit(X_train, y_train)\n\n    # assess accuracy\n    test_hat = m.predict(X_test)\n    test_acc = (test_hat == y_test).mean()\n\n    # store this result\n    complexity.append((test_acc, i))\n\ncomplexity.sort(reverse = True)\n\nprint(\"Purely by accuracy, the best five model complexities are: \" + str(complexity[:5]))\n\nPurely by accuracy, the best five model complexities are: [(0.8406760653636655, 7), (0.8403556552387056, 9), (0.8398750400512656, 8), (0.8394745273950657, 10), (0.8393143223325857, 6)]\n\n\nAssess these models to find which one is most consistent by checking with cross validation.\n\nfrom sklearn.model_selection import cross_val_score\n\n# pull out the top 5 complexities\nbest_complexity = []\nfor comp in complexity:\n    best_complexity.append(comp[1])\n\nbest_complexity = best_complexity[:5]\nbest_comp = 0\nbest_std = float(\"inf\")\n\n# assess which of the best complexities has the lowest standard deviation in cross validation\nfor comp in best_complexity:\n    # fit the model\n    m = DecisionTreeClassifier(max_depth = comp)\n    m.fit(X_train, y_train)\n\n    # assess cross validation\n    cv_scores = cross_val_score(m, X_train, y_train, cv = 5)\n    std = np.std(cv_scores)\n\n    if std &lt; best_std:\n        best_std = std\n        best_comp = comp\n\nprint(\"The model complexity that minimizes standard devition is: \" + str(best_comp) + \" with a standard deviation of: \" + str(best_std))\n\nThe model complexity that minimizes standard devition is: 6 with a standard deviation of: 0.003064476956674178\n\n\nLets now fit the model using our optimal depth into our pipeline.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n# fit the model into our pipeline\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = best_comp))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=6))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=6))]) StandardScaler?Documentation for StandardScalerStandardScaler() DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=6) \n\n\n\n# pull predictions from the model based on the test data\ny_hat = model.predict(X_test)\n\n# assess the model's overall predicitve accuracy\nprint(\"The overall testing accuracy is: \" + str((y_hat == y_test).mean()))\n\n# asses the model's predictive accuracy for white individuals\nprint(\"The testing accuracy for white individuals is: \" + str((y_hat == y_test)[group_test == 1].mean()))\n\n# assess the model's predicitve accuracy for black individuals\nprint(\"The testing accuracy for black individuals is: \" + str((y_hat == y_test)[group_test == 2].mean()))\n\n# assess the model's predicitve accuracy for others\nprint(\"The testing accuracy for other racial groups is: \" + str((y_hat == y_test)[group_test == 3].mean()))\n\nThe overall testing accuracy is: 0.8393143223325857\nThe testing accuracy for white individuals is: 0.8399193178692582\nThe testing accuracy for black individuals is: 0.8356605800214822\nThe testing accuracy for other racial groups is: 0.8421052631578947\n\n\nThe model has an approximately equal prediction rate for employment status across racial groups. In this model white individuals are the least likely to be predicted as employed, black individuals are slightly more likely, and members of all other racial groups are more likely as a whole. Lets now look at the positive predictive value as well as the false negative and false positive rates of the model.\n\n# calculate the overall accuracy of the model\nprint(\"The overall accuracy of the model at predicting employment status is: \" + str((y_hat == y_test).mean()))\n\n# calculate the overall PPV\nTP = np.sum((y_hat == y_test) & (y_hat == 1))\nFP = np.sum((y_hat != y_test) & (y_hat == 1))\nPPV = TP / (TP + FP)\nprint(\"The overall positive predictive value of our model is: \" + str(PPV))\n\n# calculate the overall FNR\nFN = np.sum((y_hat != y_test) & (y_hat == 0))\nFNR = FN / (FN + TP)\nprint(\"The overall false negative rate of our model is: \" + str(FNR))\n\n# calculate the overall FPR\nTN = np.sum((y_hat == y_test) & (y_hat == 0))\nFPR = FP / (FP + TN)\nprint(\"The overall false positive rate of our model is: \" + str(FPR))\n\nThe overall accuracy of the model at predicting employment status is: 0.8393143223325857\nThe overall positive predictive value of our model is: 0.802518223989397\nThe overall false negative rate of our model is: 0.1438670908448215\nThe overall false positive rate of our model is: 0.17462642836214473\n\n\nNow lets look at these statistics broken down by group.\n\n# calculate the model's accuracy of predicting employment status for white, black and othe racial groups\nprint(\"The model's accuracy of predicting employment status for white individuals is: \" + str((y_hat == y_test)[group_test == 1].mean()))\nprint(\"The model's accuracy of predicting employment status for black individuals is: \" + str((y_hat == y_test)[group_test == 2].mean()))\nprint(\"The model's accuracy of predicting employment status for other individuals is: \" + str((y_hat == y_test)[group_test &gt; 2].mean()) + \"\\n\")\n\n# calculate the PPV for white, black, and other racial groups\n# white individuals\nTP_W = np.sum((y_hat == y_test)[group_test == 1] & (y_hat == 1)[group_test == 1])\nFP_W = np.sum((y_hat != y_test)[group_test == 1] & (y_hat == 1)[group_test == 1])\nPPV_W = TP_W / (TP_W + FP_W)\nprint(\"The positive predictive value for white individuals in our model is: \" + str(PPV_W))\n\n# black individuals\nTP_B = np.sum((y_hat == y_test)[group_test == 2] & (y_hat == 1)[group_test == 2])\nFP_B = np.sum((y_hat != y_test)[group_test == 2] & (y_hat == 1)[group_test == 2])\nPPV_B = TP_B / (TP_B + FP_B)\nprint(\"The positive predictive value for black individuals in our model is: \" + str(PPV_B))\n\n# other individuals\nTP_O = np.sum((y_hat == y_test)[group_test &gt; 2] & (y_hat == 1)[group_test &gt; 2])\nFP_O = np.sum((y_hat != y_test)[group_test &gt; 2] & (y_hat == 1)[group_test &gt; 2])\nPPV_O = TP_O / (TP_O + FP_O)\nprint(\"The positive predictive value for other individuals in our model is: \" + str(PPV_O) + \"\\n\")\n\n# calculate the FNR for white, black, and other racial groups\n# white individuals\nFN_W = np.sum((y_hat != y_test)[group_test == 1] & (y_hat == 0)[group_test == 1])\nFNR_W = FN_W / (FN_W + TP_W)\nprint(\"The false negative rate for white individuals in our model is: \" + str(FNR_W))\n\n# black individuals\nFN_B = np.sum((y_hat != y_test)[group_test == 2] & (y_hat == 0)[group_test == 2])\nFNR_B = FN_B / (FN_B + TP_B)\nprint(\"The false negative rate for black individuals in our model is: \" + str(FNR_B))\n\n# other individuals\nFN_O = np.sum((y_hat != y_test)[group_test &gt; 2] & (y_hat == 0)[group_test &gt; 2])\nFNR_O = FN_O / (FN_O + TP_O)\nprint(\"The false negative rate for other individuals in our model is: \" + str(FNR_O) + \"\\n\")\n\n# calculate the FPR for white, black, and other racial groups\n# white individuals\nTN_W = np.sum((y_hat == y_test)[group_test == 1] & (y_hat == 0)[group_test == 1])\nFPR_W = FP_W / (FP_W + TN_W)\nprint(\"The false positive rate for white individuals in our model is: \" + str(FPR_W))\n\n# black individuals\nTN_B = np.sum((y_hat == y_test)[group_test == 2] & (y_hat == 0)[group_test == 2])\nFPR_B = FP_B / (FP_B + TN_B)\nprint(\"The false positive rate for black individuals in our model is: \" + str(FPR_B))\n\n# other individuals\nTN_O = np.sum((y_hat == y_test)[group_test &gt; 2] & (y_hat == 0)[group_test &gt; 2])\nFPR_O = FP_O / (FP_O + TN_O)\nprint(\"The false positive rate for other individuals in our model is: \" + str(FPR_O))\n\nThe model's accuracy of predicting employment status for white individuals is: 0.8399193178692582\nThe model's accuracy of predicting employment status for black individuals is: 0.8356605800214822\nThe model's accuracy of predicting employment status for other individuals is: 0.8343653250773994\n\nThe positive predictive value for white individuals in our model is: 0.8082397003745319\nThe positive predictive value for black individuals in our model is: 0.7468671679197995\nThe positive predictive value for other individuals in our model is: 0.7744107744107744\n\nThe false negative rate for white individuals in our model is: 0.14331083763398175\nThe false negative rate for black individuals in our model is: 0.14857142857142858\nThe false negative rate for other individuals in our model is: 0.14814814814814814\n\nThe false positive rate for white individuals in our model is: 0.17447606065769297\nThe false positive rate for black individuals in our model is: 0.1738382099827883\nThe false positive rate for other individuals in our model is: 0.17819148936170212\n\n\nInterestingly, across all groups the accuracy, positive predictive value, FNR, and FPR are all extremely similar and displays very little discrepancy from the overall rates. The only thing that could be said is that white individuals have the highest PPV, other indivudals a slightly lower PPV, and black individuals having the clear lowest PPV. Based on the information above, this implies that our model is well - calibrated as the algorithim is aproximately equally likely to predict white, black, and other individuals to be employed. The false negative rate is approximately equal across racial groups and the false positive rate is also approximately equal across racial groups; however, the false positive rates are not equal to the false negative rates and our model fails to meet error rate balance. Because our data is binary, and the proportion of individuals in each group predicted to be employed is approximately equal, statistical parity is met for our model. Thus our model can be considered reasonably fair.\n\nimport matplotlib.pyplot as plt\n\n# pull out prevalence from the dataset\np_W, p_B, p_O = df.groupby(\"group\")[\"label\"].mean()\n\n# fix the PPV to the lowest of the PPV across groups\nPPV_fixed = min(PPV_W, PPV_B, PPV_O)\n\n# function to compute FPR given FNR based on eqn 2.6\ndef compute_FPR(fnr, p, ppv_fixed):\n    return (p / (1 - p)) * ((1 - ppv_fixed) / ppv_fixed) * (1 - fnr)\n\n# Generate FNR-FPR tradeoff curves for each group\nfnr_range = np.linspace(0, 1, 100)\nfpr_W_curve = compute_FPR(fnr_range, p_W, PPV_fixed)\nfpr_B_curve = compute_FPR(fnr_range, p_B, PPV_fixed)\nfpr_O_curve = compute_FPR(fnr_range, p_O, PPV_fixed)\n\n# generate an empty plot\nplt.figure(figsize = (8,6))\n\n# plot the feasible trade off curves\nplt.plot(fnr_range, fpr_W_curve, color = \"blue\", label = \"White Individuals\")\nplt.plot(fnr_range, fpr_B_curve, color = \"red\", label = \"Black Individuals\", linestyle = \"dashed\")\nplt.plot(fnr_range, fpr_O_curve, color = \"green\", label = \"Other Individuals\", linestyle = \"dashdot\")\n\n# mark the observed (FNR, FPR) values\nplt.scatter(FNR_W, FPR_W, color = \"blue\", label = \"White Individuals\", marker = \"x\")\nplt.scatter(FNR_B, FPR_B, color = \"red\", label = \"Black Individuals\", marker = \"x\")\nplt.scatter(FNR_O, FPR_O, color = \"green\", label = \"Other Individuals\", marker = \"x\")\n\n# Labels and legend\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n\n\n\nTo equalize FPR across groups, the FNR for white individuals would need to increase, while the FNR for black and other individuals would need to decrease. The exact amount of change is dependent on the slopes of the curves. Lets calculate the neccesary changes.\n\n# Compute the target FPR (mean across groups)\ntarget_FPR = (FPR_W + FPR_B + FPR_O) / 3\n\n# Compute adjustments for each group\nadj_W = FNR_W * (target_FPR / FPR_W)\nadj_B = FNR_B * (target_FPR / FPR_B)\nadj_O = FNR_O * (target_FPR / FPR_O)\n\nprint(\"The required FNR adjustment for white individuals is: \" + str(adj_W))\nprint(\"The required FNR adjustment for black individuals is: \" + str(adj_B))\nprint(\"The required FNR adjustment for other individuals is: \" + str(adj_O))\n\nThe required FNR adjustment for white individuals is: 0.14415345616394387\nThe required FNR adjustment for black individuals is: 0.14999332410359234\nThe required FNR adjustment for other individuals is: 0.14591204404703959\n\n\n\n\nDiscussion\nBecause our model is relatively fair across all groups, and has reasonable overall predictive accuracy many companies which may want to consider employment status could be imterested. A primary example would be banks seeking to hand out loans; employment status is a great marker of financial stability. Banks would not want to given loans that would be unlikely to receive back, thus they would want to be able to do predict the employment status of the individuals seeking loans.\nDeploying this model for large-scale predictions in commercial or governmental settings could influence critical decisions, such as loan approvals, hiring practices, or government assistance distribution. Since the model demonstrates fairness across racial groups and satisfies error rate balance and statistical parity, it reduces the risk of systemic bias in predictions. However, small discrepancies, such as lower positive predictive value for Black individuals, could still lead to disparities in real-world outcomes; this could especially be true in systemically oppressed neighborhoods which have disproportionately higher unemployment rates. This could result in continuing to deny these individuals the resources they need.\nAlthough our model displays error rate balance, the difference is only ~ 3%. This is a small difference and is likely not incredibly harmful. Across all other measures the models is reasonably fair. The only other troubling statistic is the fairly large decrease in PPV for black individuals as opposed to white individuals.\nBeyond bias, several potential issues with deploying this model could raise concerns. First, the model‚Äôs reliance on demographic data may lead to privacy risks and ethical dilemmas, particularly if sensitive attributes are misused or inferred. Additionally, employment status is dynamic and influenced by economic conditions, meaning the model could become outdated over time. To address these concerns, I would propose several solutions. First, strict data privacy protocols should be in place to ensure that sensitive information is handled securely and used responsibly. Second, human oversight should complement automated decisions, especially in high-stakes scenarios, to prevent over-reliance on imperfect predictions. Third, regular model audits and recalibrations should be conducted to maintain accuracy and fairness as employment patterns and economic conditions shift. Finally, transparency in model deployment is essential‚Äîindividuals affected by the predictions should have access to explanations and recourse options if they believe they were unfairly assessed."
  },
  {
    "objectID": "posts/Blog 5/index.html",
    "href": "posts/Blog 5/index.html",
    "title": "Blog 5 - Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nThis project focuses on implementing logistic regression with an emphasis on understanding the underlying optimization techniques. Logistic regression is a fundamental method for binary classification, relying on a linear decision boundary and the sigmoid activation function to model probabilities. The implementation includes the computation of the logistic loss, its gradient, and parameter updates using gradient descent, both with and without the inclusion of momentum. Through a series of experiments, we explore how different learning rates and momentum values affect the convergence behavior of the model. We also explore how the model handles overfitting as well as how applicable the model is to real world data.\n\n\nData Preparation\nSource Code: https://raw.githubusercontent.com/Nibsquatch/Nibsquatch.github.io/refs/heads/main/logistic.py\n\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport matplotlib.pyplot as plt\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n# generate random data points\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\nGradient Descent Experimentation\nFirst lets performa a vanilla gradient descent with pdims = 2, a small Œ±, and ùõΩ = 0. We would expect the loss to decrease monotonically.\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\n# iterate through 1000 gradient descent updates\nfor _ in range(1000):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    opt.step(X, y, alpha = 0.1, Beta = 0)\n\n# plot the loss as a function of the number of gradient descents\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nThe implementation appears correct because the loss function decreases monotonically over iterations, which is expected in properly functioning gradient descent for logistic regression. The smooth decline in binary cross-entropy loss indicates that the model is learning effectively without divergence or oscillations. Additionally, the logarithmic x-axis confirms that the loss reduction is well-behaved over multiple orders of magnitude in iterations. These characteristics suggest that the weight updates are correctly applied and that the optimization process is progressing as intended. Now on the same data, implement gradient descent with a ùõΩ = .9 and compare the difference in plots.\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\n# iterate through 1000 gradient descent updates\nfor _ in range(1000):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y)\n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    opt.step(X, y, alpha = 0.1, Beta = .9)\n\n# plot the loss as a function of the # of gradient descent updateds\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nComparing the gradient descent with momentum (ùõΩ&gt;0) to the previous vanilla gradient descent (ùõΩ=0), we observe that the addition of momentum leads to a smoother and faster convergence. The loss decreases more rapidly in the early iterations, demonstrating how momentum helps accelerate learning by reducing oscillations and maintaining directional consistency. In contrast, the previous descent without momentum had a more gradual decline in loss. This confirms that momentum improves optimization efficiency, reaching a lower loss in fewer iterations while maintaining stability. Now, let‚Äôs explore how the algorithim handles overfitting where pdims &gt; n_points.\n\n# generate random data points with pdims &gt; n points\ndef classification_data2(n_points = 150, noise = 0.2, p_dims = 300):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# generate a training and test set with pdims &gt; n points\nX_train, y_train = classification_data2(noise = 0.5)\nX_test, y_test = classification_data2(noise = .5)\n\n# instantiate a model and an optimizer\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# training loop without keeping track of loss since we care more about the predictive accuracy of the model\nfor _ in range(1000):\n\n    # only this line actually changes the parameter value\n    opt.step(X_train, y_train, alpha = 0.1, Beta = .9)\n\n# calculate training accuracy\ny_pred = LR.predict(X_train)\nacc = (1.0*(y_pred == y_train)).mean().item()\n\nprint(\"The model has a training accuracy of: \" + str(acc))\n\n# calculate the testing accuracy\ny_pred = LR.predict(X_test)\nacc = (1.0*(y_pred == y_test)).mean().item()\n\nprint(\"The model has a testing accuracy of: \" + str(acc))\n\nThe model has a training accuracy of: 1.0\nThe model has a testing accuracy of: 0.9399999976158142\n\n\nThis demonstrates that although the testing accuracy is still fairly high, working with data where the number of dimensions is greater than the number entries tends to cause overfitting of the training data as the model fit 100% to the training data\n\n\nLogistic Regression on Real World Data\nNow, lets perform logistic regression on real world data. The Wine Quality dataset from the UCI Machine Learning Repository consists of two datasets related to red and white Vinho Verde wines from Portugal. These datasets were originally compiled by researchers Paulo Cortez and colleagues at the University of Minho in collaboration with the Viticulture Commission of the Vinho Verde Region (CVRVV). Each dataset contains physicochemical measurements of wine samples‚Äî1,599 red wines and 4,898 white wines‚Äîalongside sensory quality scores rated by professional tasters. The goal is to model wine quality based on chemical properties, making the data suitable for both regression and classification tasks. The original study describing the dataset is ‚ÄúModeling wine preferences by data mining from physicochemical properties‚Äù by Cortez et al., published in Decision Support Systems (2009). We will converting the wine quality score to high or low, with high quality (1) being scores greater than 5 and low quality (0) being scores less than or equal to 5. Our logistic regression will attempt to classify the wine as high or low quality based on its physiochemical measurements.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom ucimlrepo import fetch_ucirepo \nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n  \n# fetch dataset \nwine_quality = fetch_ucirepo(id=186) \n  \n# data (as pandas dataframes) \nX = wine_quality.data.features.to_numpy() \ny = wine_quality.data.targets.to_numpy()\n\n# Convert quality scores to binary labels (1 for high, 0 for low)\ny = (y &gt; 5).astype(int)  # Vectorized operation\n\n# Split into train (60%), val (20%), test (20%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n\n# Normalize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train, dtype = torch.float32)\ny_train = torch.tensor(y_train, dtype = torch.float32).squeeze()\nX_test = torch.tensor(X_test, dtype = torch.float32)\ny_test = torch.tensor(y_test, dtype = torch. float32).squeeze()\n\n# add a 1 to the end of each row\nX_train = torch.cat((X_train, torch.ones((X_train.shape[0], 1))), 1)\nX_test = torch.cat((X_test, torch.ones((X_test.shape[0], 1))), 1)\n\n\n# perform a logistic regression on the data\n# instantiate a model and an optimizer\nLR1 = LogisticRegression()\nLR2 = LogisticRegression() \nopt1 = GradientDescentOptimizer(LR1)\nopt2 = GradientDescentOptimizer(LR2)\n\n# for keeping track of loss values\nloss1_vec = []\nloss2_vec = []\n\n# perform 1000 iterations of gradient descent\nfor _ in range(1000):\n    # Track loss values (convert tensors to scalars)\n    loss1_vec.append(LR1.loss(X_train, y_train))\n    loss2_vec.append(LR2.loss(X_train, y_train))\n\n    # Perform optimization step\n    opt1.step(X_train, y_train, alpha = 0.1, Beta = 0)\n    opt2.step(X_train, y_train, alpha = 0.1, Beta = 0.9)\n\n# Plot loss over iterations\nplt.plot(range(1, len(loss1_vec) + 1), loss1_vec, color = \"blue\", label = \"Beta = 0\")\nplt.plot(range(1, len(loss2_vec) + 1), loss2_vec, color = \"red\", label = \"Beta = 0.9\")\nplt.semilogx()\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Loss (binary cross entropy)\")\nplt.legend()\nplt.show()\n\n# Compute the Loss and Accuracy of the Model Against the Test Set\nprint(\"The loss of the model with Beta = 0 is: \" + str(LR1.loss(X_test, y_test).item()))\nprint(\"The loss of the model with Beta = .9 is: \" + str(LR2.loss(X_test, y_test).item()))\n\n# calculate the models accuracies\ny_pred1 = LR1.predict(X_test)\ny_pred2 = LR2.predict(X_test)\n\nacc1 = (1.0*(y_pred1 == y_test)).mean()\nacc2 = (1.0*(y_pred2 == y_test)).mean()\n\nprint(\"The accuracy of the model with Beta = 0 is: \" + str(acc1.item()))\nprint(\"The accuracy of the model with Beta = .9 is: \" + str(acc2.item()))\n\n\n\n\n\n\n\n\nThe loss of the model with Beta = 0 is: 0.5396895408630371\nThe loss of the model with Beta = .9 is: 0.5408527851104736\nThe accuracy of the model with Beta = 0 is: 0.7169230580329895\nThe accuracy of the model with Beta = .9 is: 0.7192307710647583\n\n\nThe model with Beta = .9 as opposed to Beta = 0 (standard logistic regression) demonstrates almost identical loss to the Beta = 0 model but converges to its final loss much quicker; however, the loss, even after convergence remains extremely high which is not ideal for prediction. The accuracy of the two models are almost identical but both seem to be better than simply guessing at predicting the wine quality; however, the predictive accuracy could be better if the loss could be reduced.\n\n\nDicussion\nThis project demonstrated the implementation of logistic regression from scratch, focusing on how different optimization techniques impact model performance. By manually coding the sigmoid function, logistic loss, and gradient descent (with and without momentum), the underlying process of empirical risk minimization was explored in detail. Experiments with synthetic datasets showed that the algorithm behaves as expected, with momentum significantly improving convergence speed, particularly in the early stages of training. However, when working with high-dimensional data, it was observed that logistic regression can overfit, achieving perfect training accuracy but failing to generalize well to new data. In applying the model to real-world wine quality data, it became evident that, despite faster convergence with momentum, the models still struggled with high loss values. This highlighted the importance of further tuning and potentially exploring more complex models to improve predictive accuracy. Overall, this project emphasized the crucial balance between optimization and generalization in machine learning."
  },
  {
    "objectID": "posts/Blog 7/index.html",
    "href": "posts/Blog 7/index.html",
    "title": "Blog 7 - Exploring Overfitting as a Result of Overparameterization and Double Descent",
    "section": "",
    "text": "Abstract\nIn this post, we investigate the phenomenon of double descent within the context of a corrupt image detection task. Our goal is to analyze how the performance of a supervised learning model changes as we vary the number of input features, particularly focusing on the relationship between model complexity and generalization error. Using a dataset where the objective is to classify whether images are corrupted or not, we track both training and testing mean squared error (MSE) across a range of feature counts. Our analysis reveals a clear double descent curve, with the test error initially decreasing, peaking near the interpolation threshold, and then decreasing again as the model becomes highly overparameterized. This behavior highlights the limitations of classical bias-variance intuition and showcases a modern perspective on model complexity.\n\n\nConsidering p &gt; n For Linear Regression\nWhen the number of features \\(p\\) exceeds the number of data observations \\(n\\), the standard closed-form solution for linear regression becomes invalid due to an issue involving matrix invertibility. The formula \\(\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) relies on the inversion of the matrix \\(\\mathbf{X}^T\\mathbf{X}\\). However, when \\(p &gt; n\\), the matrix \\(\\mathbf{X}\\) has more columns than rows, making \\(\\mathbf{X}^T\\mathbf{X}\\) a \\(p \\times p\\) matrix that is not full rank. This means it is singular, or rank-deficient, and therefore not invertible. As a result, the operation \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) is undefined, and the entire expression for \\(\\hat{\\mathbf{w}}\\) in Equation 1 breaks down. This is why the closed-form solution is only valid when \\(n &gt; p\\), ensuring that \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible.\n\n\nLoading the Model and Packages\nSource Code: https://raw.githubusercontent.com/Nibsquatch/Nibsquatch.github.io/refs/heads/main/OverparametrizedRegression.py\n\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\nfrom sklearn.model_selection import train_test_split\nfrom OverparametrizedRegression import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n\nTesting Model on Simple Data\n\n# First we will define a class to generate random feature maps\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\nTo test our model we will fit nonlinear data using our random features class and feed these features to our LinearRegression model. We will then plot our predictions overtop the data.\n\n# Generate Nonlinear Data\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\n# transorm our data\nphi = RandomFeatures(n_features = 10)\nphi.fit(X)\nphi_X = phi.transform(X)\n\n# instantiate our model and optimizer\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\n\n# fit the model to our transformed data\nopt.fit(phi_X, y)\n\n# pull predictions from the data\ny_pred = LR.predict(phi_X)\n\n# Plot predictions vs actual data\nplt.figure(figsize = (8, 5))\nplt.scatter(X, y, label = 'Nonlinear Data', color = 'blue', alpha = 0.6)\nplt.plot(X, y_pred, label = 'Model Prediction', color = 'red', linewidth = 2)\nplt.title(\"Nonlinear Data vs Model Prediction\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nJust based off preliminary predictions on training data is appears as if our overparametrized model is performing well. Lets continue on to explore how this model performs more in depth.\n\n\nDouble Descent in Image Corruption Detection\nTo perform image corruption detections we will load a greyscale image of a flower and then replace some of the pixels with completely grey chunks to represent corruption of the image. We will then attempt to use our model to predict the number of corruptions in the image based on the image itself. Lets start by loading the image and corrupting it.\n\n# load our flower image\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\n# define a function to corrupt the image\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n# use our function to load and corrupt the flower image and view the new image side by side with the natural image\nX, y = corrupted_image(flower, mean_patches = 50)\n\n# Show original and corrupted side by side\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].imshow(flower)  \nax[0].set_title(\"Original\")\nax[0].axis(\"off\")\n\nax[1].imshow(X, vmin = 0, vmax=1)  # Keep value range, don't force colormap\nax[1].set_title(f\"Corrupted: {y} patches\")\nax[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nNow that we have a method to generate corrupted flowers with varying levels of corruption, we will generate a dataset of 200 samples of corrupted images and split into testing and training data.\n\n# set 200 samples as our benchmark dataset size\nn_samples = 200\n\n# generate 200 corrupted images\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n# Reshape the images to lay them out into long rows\nX = X.reshape(n_samples, -1)\n\n# split into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 42)\n\nTime to train on our model on our corrupt image dataset. As a baseline, lets see how a standard linear model would perform before overparameterization. We will then iterate through varying levels of overparametrization and keep track of the MSE for both training and testing. For our feature extension we will also used a squared activation instead of a sigmoid activation.\n\n# Initialize arrays to keep track of MSE and number of features\nMSE_training, MSE_testing = [], []\nnum_features = []\n\n# max features to use for our feature matrix and iteration\nmax_features = 1000\n\nfor i in range(1, max_features + 1):\n\n    # generate our phi function\n    phi = RandomFeatures(n_features = i, activation = square)\n    \n    # fit to both the training and testing data\n    phi.fit(X_train)\n    phi_X_train = phi.transform(X_train)\n\n    phi.fit(X_test)\n    phi_X_test = phi.transform(X_test)\n\n    # instantiate and fit a linear regression model with an optimizer\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n\n    opt.fit(phi_X_train, y_train)\n\n    # get the training MSE, testing MSE, and the number of features\n    MSE_training.append(LR.loss(phi_X_train, y_train))\n    MSE_testing.append(LR.loss(phi_X_test, y_test))\n    num_features.append(len(LR.w))\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[8], line 17\n     14 phi.fit(X_train)\n     15 phi_X_train = phi.transform(X_train)\n---&gt; 17 phi.fit(X_test)\n     18 phi_X_test = phi.transform(X_test)\n     20 # instantiate and fit a linear regression model with an optimizer\n\nCell In[4], line 30, in RandomFeatures.fit(self, X)\n     29 def fit(self, X):\n---&gt; 30     self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n     31     self.b = torch.rand((self.n_features), dtype = torch.float64)\n\nKeyboardInterrupt: \n\n\n\nLets visualize how our training and testing MSE are changing as a result of increasing the number of features.\n\n# Create plots\nfig, axes = plt.subplots(1, 2, figsize = (12, 5))\n\n# --- Training Plot ---\naxes[0].scatter(num_features, MSE_training, color = 'gray', s = 10)\naxes[0].axvline(100, color = \"black\", lw = 2)\naxes[0].set_yscale('log')\naxes[0].set_xlabel(\"Number of features\")\naxes[0].set_ylabel(\"Mean squared error (training)\")\n\n# --- Testing Plot ---\naxes[1].scatter(num_features, MSE_testing, color = 'darkred', s = 10)\naxes[1].axvline(100, color = \"black\", lw = 2)\naxes[1].set_yscale('log')\naxes[1].set_xlabel(\"Number of features\")\naxes[1].set_ylabel(\"Mean squared error (testing)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe vertical black lines represent the iterpolation point, or the point where the number of features is equivalent to the number of samples. This is a typical marker for double descent, where as the number of features becomes greater than the number of samples the loss will begin to go back down. Our data demonstrates that this is clearly the case for image classification as the MSE increase dramatically until the interpolation threshold and then begins to gradually decrease; the MSE for the training data has seeminly high variance but because of the log scale it actually demonstrates effectively no loss. If we had more computing power available we could attempt to extend this model to 10,000 features as it seems as if the MSE for the testing data is still following a negative trend even around 1000 features.\n\n# print the best number of features to produce the minimum MSE in the testing data\nmin_MSE = min(MSE_testing)\nmin_features = num_features[MSE_testing.index(min_MSE)]\n\nprint(\"The smallest testing MSE achieved was \" + str(min_MSE.item()) + \" which occured with \" + str(min_features) + \" features which occurs far beyond the interpolation point.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 2\n      1 # print the best number of features to produce the minimum MSE in the testing data\n----&gt; 2 min_MSE = min(MSE_testing)\n      3 min_features = num_features[MSE_testing.index(min_MSE)]\n      5 print(\"The smallest testing MSE achieved was \" + str(min_MSE.item()) + \" which occured with \" + str(min_features) + \" features which occurs far beyond the interpolation point.\")\n\nNameError: name 'MSE_testing' is not defined\n\n\n\n\n\nDiscussion\nThrough our corrupt image detection experiment, we observed a distinct double descent pattern in model performance as the number of features increased. The training error decreased until the number of features reached the interpolation threshold, where the training error dropped to vitrually none, while the test error showed a notable spike at the interpolation threshold before steadily dropping again in the overparameterized regime. If we had access to more computing, future experiments should include increasing the number of features past the limits tested in this plog. These findings still support recent theoretical developments in machine learning that suggest overparameterized models can generalize well despite perfectly fitting the training data."
  }
]