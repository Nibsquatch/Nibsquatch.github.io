[
  {
    "objectID": "posts/Blog 5/index.html",
    "href": "posts/Blog 5/index.html",
    "title": "Blog 5 - Implementing Logistic Regression",
    "section": "",
    "text": "Abstract\nThis project focuses on implementing logistic regression with an emphasis on understanding the underlying optimization techniques. Logistic regression is a fundamental method for binary classification, relying on a linear decision boundary and the sigmoid activation function to model probabilities. The implementation includes the computation of the logistic loss, its gradient, and parameter updates using gradient descent, both with and without the inclusion of momentum. Through a series of experiments, we explore how different learning rates and momentum values affect the convergence behavior of the model. We also explore how the model handles overfitting as well as how applicable the model is to real world data.\n\n\nData Preparation\nSource Code: https://raw.githubusercontent.com/Nibsquatch/Nibsquatch.github.io/refs/heads/main/logistic.py\n\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport matplotlib.pyplot as plt\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n# generate random data points\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\n\nGradient Descent Experimentation\nFirst lets performa a vanilla gradient descent with pdims = 2, a small Œ±, and ùõΩ = 0. We would expect the loss to decrease monotonically.\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\n# iterate through 1000 gradient descent updates\nfor _ in range(1000):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    opt.step(X, y, alpha = 0.1, Beta = 0)\n\n# plot the loss as a function of the number of gradient descents\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nThe implementation appears correct because the loss function decreases monotonically over iterations, which is expected in properly functioning gradient descent for logistic regression. The smooth decline in binary cross-entropy loss indicates that the model is learning effectively without divergence or oscillations. Additionally, the logarithmic x-axis confirms that the loss reduction is well-behaved over multiple orders of magnitude in iterations. These characteristics suggest that the weight updates are correctly applied and that the optimization process is progressing as intended. Now on the same data, implement gradient descent with a ùõΩ = .9 and compare the difference in plots.\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\n# iterate through 1000 gradient descent updates\nfor _ in range(1000):\n\n    # not part of the update: just for tracking our progress    \n    loss = LR.loss(X, y)\n    loss_vec.append(loss)\n\n    # only this line actually changes the parameter value\n    opt.step(X, y, alpha = 0.1, Beta = .9)\n\n# plot the loss as a function of the # of gradient descent updateds\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.semilogx()\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nComparing the gradient descent with momentum (ùõΩ&gt;0) to the previous vanilla gradient descent (ùõΩ=0), we observe that the addition of momentum leads to a smoother and faster convergence. The loss decreases more rapidly in the early iterations, demonstrating how momentum helps accelerate learning by reducing oscillations and maintaining directional consistency. In contrast, the previous descent without momentum had a more gradual decline in loss. This confirms that momentum improves optimization efficiency, reaching a lower loss in fewer iterations while maintaining stability. Now, let‚Äôs explore how the algorithim handles overfitting where pdims &gt; n_points.\n\n# generate random data points with pdims &gt; n points\ndef classification_data2(n_points = 150, noise = 0.2, p_dims = 300):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# generate a training and test set with pdims &gt; n points\nX_train, y_train = classification_data2(noise = 0.5)\nX_test, y_test = classification_data2(noise = .5)\n\n# instantiate a model and an optimizer\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\n# training loop without keeping track of loss since we care more about the predictive accuracy of the model\nfor _ in range(1000):\n\n    # only this line actually changes the parameter value\n    opt.step(X_train, y_train, alpha = 0.1, Beta = .9)\n\n# calculate training accuracy\ny_pred = LR.predict(X_train)\nacc = (1.0*(y_pred == y_train)).mean().item()\n\nprint(\"The model has a training accuracy of: \" + str(acc))\n\n# calculate the testing accuracy\ny_pred = LR.predict(X_test)\nacc = (1.0*(y_pred == y_test)).mean().item()\n\nprint(\"The model has a testing accuracy of: \" + str(acc))\n\nThe model has a training accuracy of: 1.0\nThe model has a testing accuracy of: 0.9399999976158142\n\n\nThis demonstrates that although the testing accuracy is still fairly high, working with data where the number of dimensions is greater than the number entries tends to cause overfitting of the training data as the model fit 100% to the training data\n\n\nLogistic Regression on Real World Data\nNow, lets perform logistic regression on real world data. The Wine Quality dataset from the UCI Machine Learning Repository consists of two datasets related to red and white Vinho Verde wines from Portugal. These datasets were originally compiled by researchers Paulo Cortez and colleagues at the University of Minho in collaboration with the Viticulture Commission of the Vinho Verde Region (CVRVV). Each dataset contains physicochemical measurements of wine samples‚Äî1,599 red wines and 4,898 white wines‚Äîalongside sensory quality scores rated by professional tasters. The goal is to model wine quality based on chemical properties, making the data suitable for both regression and classification tasks. The original study describing the dataset is ‚ÄúModeling wine preferences by data mining from physicochemical properties‚Äù by Cortez et al., published in Decision Support Systems (2009). We will converting the wine quality score to high or low, with high quality (1) being scores greater than 5 and low quality (0) being scores less than or equal to 5. Our logistic regression will attempt to classify the wine as high or low quality based on its physiochemical measurements.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom ucimlrepo import fetch_ucirepo \nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n  \n# fetch dataset \nwine_quality = fetch_ucirepo(id=186) \n  \n# data (as pandas dataframes) \nX = wine_quality.data.features.to_numpy() \ny = wine_quality.data.targets.to_numpy()\n\n# Convert quality scores to binary labels (1 for high, 0 for low)\ny = (y &gt; 5).astype(int)  # Vectorized operation\n\n# Split into train (60%), val (20%), test (20%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n\n# Normalize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train, dtype = torch.float32)\ny_train = torch.tensor(y_train, dtype = torch.float32).squeeze()\nX_test = torch.tensor(X_test, dtype = torch.float32)\ny_test = torch.tensor(y_test, dtype = torch. float32).squeeze()\n\n# add a 1 to the end of each row\nX_train = torch.cat((X_train, torch.ones((X_train.shape[0], 1))), 1)\nX_test = torch.cat((X_test, torch.ones((X_test.shape[0], 1))), 1)\n\n\n# perform a logistic regression on the data\n# instantiate a model and an optimizer\nLR1 = LogisticRegression()\nLR2 = LogisticRegression() \nopt1 = GradientDescentOptimizer(LR1)\nopt2 = GradientDescentOptimizer(LR2)\n\n# for keeping track of loss values\nloss1_vec = []\nloss2_vec = []\n\n# perform 1000 iterations of gradient descent\nfor _ in range(1000):\n    # Track loss values (convert tensors to scalars)\n    loss1_vec.append(LR1.loss(X_train, y_train))\n    loss2_vec.append(LR2.loss(X_train, y_train))\n\n    # Perform optimization step\n    opt1.step(X_train, y_train, alpha = 0.1, Beta = 0)\n    opt2.step(X_train, y_train, alpha = 0.1, Beta = 0.9)\n\n# Plot loss over iterations\nplt.plot(range(1, len(loss1_vec) + 1), loss1_vec, color = \"blue\", label = \"Beta = 0\")\nplt.plot(range(1, len(loss2_vec) + 1), loss2_vec, color = \"red\", label = \"Beta = 0.9\")\nplt.semilogx()\nplt.xlabel(\"Number of gradient descent iterations\")\nplt.ylabel(\"Loss (binary cross entropy)\")\nplt.legend()\nplt.show()\n\n# Compute the Loss and Accuracy of the Model Against the Test Set\nprint(\"The loss of the model with Beta = 0 is: \" + str(LR1.loss(X_test, y_test).item()))\nprint(\"The loss of the model with Beta = .9 is: \" + str(LR2.loss(X_test, y_test).item()))\n\n# calculate the models accuracies\ny_pred1 = LR1.predict(X_test)\ny_pred2 = LR2.predict(X_test)\n\nacc1 = (1.0*(y_pred1 == y_test)).mean()\nacc2 = (1.0*(y_pred2 == y_test)).mean()\n\nprint(\"The accuracy of the model with Beta = 0 is: \" + str(acc1.item()))\nprint(\"The accuracy of the model with Beta = .9 is: \" + str(acc2.item()))\n\n\n\n\n\n\n\n\nThe loss of the model with Beta = 0 is: 0.5396895408630371\nThe loss of the model with Beta = .9 is: 0.5408527851104736\nThe accuracy of the model with Beta = 0 is: 0.7169230580329895\nThe accuracy of the model with Beta = .9 is: 0.7192307710647583\n\n\nThe model with Beta = .9 as opposed to Beta = 0 (standard logistic regression) demonstrates almost identical loss to the Beta = 0 model but converges to its final loss much quicker; however, the loss, even after convergence remains extremely high which is not ideal for prediction. The accuracy of the two models are almost identical but both seem to be better than simply guessing at predicting the wine quality; however, the predictive accuracy could be better if the loss could be reduced.\n\n\nDicussion\nThis project demonstrated the implementation of logistic regression from scratch, focusing on how different optimization techniques impact model performance. By manually coding the sigmoid function, logistic loss, and gradient descent (with and without momentum), the underlying process of empirical risk minimization was explored in detail. Experiments with synthetic datasets showed that the algorithm behaves as expected, with momentum significantly improving convergence speed, particularly in the early stages of training. However, when working with high-dimensional data, it was observed that logistic regression can overfit, achieving perfect training accuracy but failing to generalize well to new data. In applying the model to real-world wine quality data, it became evident that, despite faster convergence with momentum, the models still struggled with high loss values. This highlighted the importance of further tuning and potentially exploring more complex models to improve predictive accuracy. Overall, this project emphasized the crucial balance between optimization and generalization in machine learning."
  },
  {
    "objectID": "posts/Blog 3/index.html",
    "href": "posts/Blog 3/index.html",
    "title": "Blog 3 - Auditing Bias",
    "section": "",
    "text": "Abstract\nThis blog post examines the fairness and accuracy of a machine learning model designed to predict employment status in Missouri using demographic data. By auditing for bias, the analysis assesses disparities in prediction accuracy, false positive rates, and false negative rates across racial groups. The results indicate that the model meets calibtration and statistical parity, but fails to meet error rate balance. The study also explores potential ethical and practical concerns, such as privacy risks, model reliability, and unintended consequences in commercial and governmental applications. Recommendations for mitigating these risks include implementing strict data privacy measures, incorporating human oversight, regularly auditing the model, and ensuring transparency in decision-making processes.\n\n\nData Preparation and Package Installation\nLoading the ACS data from Missouri and selecting the features we want to train on.\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MO\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n# filte the data to only use recommended features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n27\n17.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n4\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n42\n19.0\n5\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n2\n6.0\n\n\n2\n20\n19.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n2.0\n1\n1\n1.0\n\n\n3\n26\n17.0\n5\n16\n1\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n1.0\n1\n2\n6.0\n\n\n4\n37\n16.0\n5\n16\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n1\n1.0\n1\n2\n6.0\n\n\n\n\n\n\n\nTransforming the features into a basic problem to address the task of predicting employment status while excluding race as a demographic.\n\n# subset the features we want to use\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n# construct a basic problem to predict employment status without considering race\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\n# separate into a feature matrix, label vector, and a group label vector\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nSplit into a training and test split.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nAssessing Initial Variance\nLets now look at some basic descriptives.\n\nimport pandas as pd\nimport seaborn as sns\n\n# convert the data back into a dataframe to make it easier to work with\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n# how many individuals are there in the data\nprint(\"There are \" + str(len(df)) + \" individuals in the dataset \\n\")\n\n# how many individuals are currently employed\nprint(\"There are \" + str(len(df[df[\"label\"] == 1])) + \" individuals in the dataset are currently employed \\n\")\n\n# how many individuals in each group\nprint(\"The number of individuals in each group: \")\nprint(df.groupby(\"group\").size())\n\n# proportion of individuals in each group that are currently employed\nprint(\"\\nThe proportion of individuals in each group that are currently employed: \")\nprint(df.groupby(\"group\")[\"label\"].mean())\n\n# lets now add sex to the consideration of race and proportion predicted to be employed\ndf[\"group\"][df[\"group\"] &gt; 2] = 3\nax = sns.barplot(data = df, x = \"group\", y = \"label\", hue = \"SEX\", errorbar = None)\nax.set_xlabel(\"Race (White, Black, Other)\")\nax.set_ylabel(\"Proportion Employed\")\n\nThere are 49932 individuals in the dataset \n\nThere are 22306 individuals in the dataset are currently employed \n\nThe number of individuals in each group: \ngroup\n1    43713\n2     3838\n3      150\n4        5\n5       27\n6      762\n7       51\n8      316\n9     1070\ndtype: int64\n\nThe proportion of individuals in each group that are currently employed: \ngroup\n1    0.454396\n2    0.395779\n3    0.460000\n4    0.600000\n5    0.407407\n6    0.493438\n7    0.529412\n8    0.373418\n9    0.299065\nName: label, dtype: float64\n\n\nText(0, 0.5, 'Proportion Employed')\n\n\n\n\n\n\n\n\n\nWhile there is some variance across sex and race in the employment rate, for the most part the employment rate is approximately equal across all groups. Although both white females and males are more employed more than any other subgroup.\n\n\nTraining a Model\nLets train a model using a decision tree classifier with an optimal model complexity. Select model complexity based on the complexity that gets the higest accuracy and the lowest standard deviation.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n# assess 50 model complexities\n\ncomplexity = []\n\nfor i in range(1, 50, 1):\n    # fit a model with the given complexity\n    m = DecisionTreeClassifier(max_depth = i)\n    m.fit(X_train, y_train)\n\n    # assess accuracy\n    test_hat = m.predict(X_test)\n    test_acc = (test_hat == y_test).mean()\n\n    # store this result\n    complexity.append((test_acc, i))\n\ncomplexity.sort(reverse = True)\n\nprint(\"Purely by accuracy, the best five model complexities are: \" + str(complexity[:5]))\n\nPurely by accuracy, the best five model complexities are: [(0.8406760653636655, 7), (0.8403556552387056, 9), (0.8398750400512656, 8), (0.8394745273950657, 10), (0.8393143223325857, 6)]\n\n\nAssess these models to find which one is most consistent by checking with cross validation.\n\nfrom sklearn.model_selection import cross_val_score\n\n# pull out the top 5 complexities\nbest_complexity = []\nfor comp in complexity:\n    best_complexity.append(comp[1])\n\nbest_complexity = best_complexity[:5]\nbest_comp = 0\nbest_std = float(\"inf\")\n\n# assess which of the best complexities has the lowest standard deviation in cross validation\nfor comp in best_complexity:\n    # fit the model\n    m = DecisionTreeClassifier(max_depth = comp)\n    m.fit(X_train, y_train)\n\n    # assess cross validation\n    cv_scores = cross_val_score(m, X_train, y_train, cv = 5)\n    std = np.std(cv_scores)\n\n    if std &lt; best_std:\n        best_std = std\n        best_comp = comp\n\nprint(\"The model complexity that minimizes standard devition is: \" + str(best_comp) + \" with a standard deviation of: \" + str(best_std))\n\nThe model complexity that minimizes standard devition is: 6 with a standard deviation of: 0.003064476956674178\n\n\nLets now fit the model using our optimal depth into our pipeline.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\n# fit the model into our pipeline\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = best_comp))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=6))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=6))]) StandardScaler?Documentation for StandardScalerStandardScaler() DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=6) \n\n\n\n# pull predictions from the model based on the test data\ny_hat = model.predict(X_test)\n\n# assess the model's overall predicitve accuracy\nprint(\"The overall testing accuracy is: \" + str((y_hat == y_test).mean()))\n\n# asses the model's predictive accuracy for white individuals\nprint(\"The testing accuracy for white individuals is: \" + str((y_hat == y_test)[group_test == 1].mean()))\n\n# assess the model's predicitve accuracy for black individuals\nprint(\"The testing accuracy for black individuals is: \" + str((y_hat == y_test)[group_test == 2].mean()))\n\n# assess the model's predicitve accuracy for others\nprint(\"The testing accuracy for other racial groups is: \" + str((y_hat == y_test)[group_test == 3].mean()))\n\nThe overall testing accuracy is: 0.8393143223325857\nThe testing accuracy for white individuals is: 0.8399193178692582\nThe testing accuracy for black individuals is: 0.8356605800214822\nThe testing accuracy for other racial groups is: 0.8421052631578947\n\n\nThe model has an approximately equal prediction rate for employment status across racial groups. In this model white individuals are the least likely to be predicted as employed, black individuals are slightly more likely, and members of all other racial groups are more likely as a whole. Lets now look at the positive predictive value as well as the false negative and false positive rates of the model.\n\n# calculate the overall accuracy of the model\nprint(\"The overall accuracy of the model at predicting employment status is: \" + str((y_hat == y_test).mean()))\n\n# calculate the overall PPV\nTP = np.sum((y_hat == y_test) & (y_hat == 1))\nFP = np.sum((y_hat != y_test) & (y_hat == 1))\nPPV = TP / (TP + FP)\nprint(\"The overall positive predictive value of our model is: \" + str(PPV))\n\n# calculate the overall FNR\nFN = np.sum((y_hat != y_test) & (y_hat == 0))\nFNR = FN / (FN + TP)\nprint(\"The overall false negative rate of our model is: \" + str(FNR))\n\n# calculate the overall FPR\nTN = np.sum((y_hat == y_test) & (y_hat == 0))\nFPR = FP / (FP + TN)\nprint(\"The overall false positive rate of our model is: \" + str(FPR))\n\nThe overall accuracy of the model at predicting employment status is: 0.8393143223325857\nThe overall positive predictive value of our model is: 0.802518223989397\nThe overall false negative rate of our model is: 0.1438670908448215\nThe overall false positive rate of our model is: 0.17462642836214473\n\n\nNow lets look at these statistics broken down by group.\n\n# calculate the model's accuracy of predicting employment status for white, black and othe racial groups\nprint(\"The model's accuracy of predicting employment status for white individuals is: \" + str((y_hat == y_test)[group_test == 1].mean()))\nprint(\"The model's accuracy of predicting employment status for black individuals is: \" + str((y_hat == y_test)[group_test == 2].mean()))\nprint(\"The model's accuracy of predicting employment status for other individuals is: \" + str((y_hat == y_test)[group_test &gt; 2].mean()) + \"\\n\")\n\n# calculate the PPV for white, black, and other racial groups\n# white individuals\nTP_W = np.sum((y_hat == y_test)[group_test == 1] & (y_hat == 1)[group_test == 1])\nFP_W = np.sum((y_hat != y_test)[group_test == 1] & (y_hat == 1)[group_test == 1])\nPPV_W = TP_W / (TP_W + FP_W)\nprint(\"The positive predictive value for white individuals in our model is: \" + str(PPV_W))\n\n# black individuals\nTP_B = np.sum((y_hat == y_test)[group_test == 2] & (y_hat == 1)[group_test == 2])\nFP_B = np.sum((y_hat != y_test)[group_test == 2] & (y_hat == 1)[group_test == 2])\nPPV_B = TP_B / (TP_B + FP_B)\nprint(\"The positive predictive value for black individuals in our model is: \" + str(PPV_B))\n\n# other individuals\nTP_O = np.sum((y_hat == y_test)[group_test &gt; 2] & (y_hat == 1)[group_test &gt; 2])\nFP_O = np.sum((y_hat != y_test)[group_test &gt; 2] & (y_hat == 1)[group_test &gt; 2])\nPPV_O = TP_O / (TP_O + FP_O)\nprint(\"The positive predictive value for other individuals in our model is: \" + str(PPV_O) + \"\\n\")\n\n# calculate the FNR for white, black, and other racial groups\n# white individuals\nFN_W = np.sum((y_hat != y_test)[group_test == 1] & (y_hat == 0)[group_test == 1])\nFNR_W = FN_W / (FN_W + TP_W)\nprint(\"The false negative rate for white individuals in our model is: \" + str(FNR_W))\n\n# black individuals\nFN_B = np.sum((y_hat != y_test)[group_test == 2] & (y_hat == 0)[group_test == 2])\nFNR_B = FN_B / (FN_B + TP_B)\nprint(\"The false negative rate for black individuals in our model is: \" + str(FNR_B))\n\n# other individuals\nFN_O = np.sum((y_hat != y_test)[group_test &gt; 2] & (y_hat == 0)[group_test &gt; 2])\nFNR_O = FN_O / (FN_O + TP_O)\nprint(\"The false negative rate for other individuals in our model is: \" + str(FNR_O) + \"\\n\")\n\n# calculate the FPR for white, black, and other racial groups\n# white individuals\nTN_W = np.sum((y_hat == y_test)[group_test == 1] & (y_hat == 0)[group_test == 1])\nFPR_W = FP_W / (FP_W + TN_W)\nprint(\"The false positive rate for white individuals in our model is: \" + str(FPR_W))\n\n# black individuals\nTN_B = np.sum((y_hat == y_test)[group_test == 2] & (y_hat == 0)[group_test == 2])\nFPR_B = FP_B / (FP_B + TN_B)\nprint(\"The false positive rate for black individuals in our model is: \" + str(FPR_B))\n\n# other individuals\nTN_O = np.sum((y_hat == y_test)[group_test &gt; 2] & (y_hat == 0)[group_test &gt; 2])\nFPR_O = FP_O / (FP_O + TN_O)\nprint(\"The false positive rate for other individuals in our model is: \" + str(FPR_O))\n\nThe model's accuracy of predicting employment status for white individuals is: 0.8399193178692582\nThe model's accuracy of predicting employment status for black individuals is: 0.8356605800214822\nThe model's accuracy of predicting employment status for other individuals is: 0.8343653250773994\n\nThe positive predictive value for white individuals in our model is: 0.8082397003745319\nThe positive predictive value for black individuals in our model is: 0.7468671679197995\nThe positive predictive value for other individuals in our model is: 0.7744107744107744\n\nThe false negative rate for white individuals in our model is: 0.14331083763398175\nThe false negative rate for black individuals in our model is: 0.14857142857142858\nThe false negative rate for other individuals in our model is: 0.14814814814814814\n\nThe false positive rate for white individuals in our model is: 0.17447606065769297\nThe false positive rate for black individuals in our model is: 0.1738382099827883\nThe false positive rate for other individuals in our model is: 0.17819148936170212\n\n\nInterestingly, across all groups the accuracy, positive predictive value, FNR, and FPR are all extremely similar and displays very little discrepancy from the overall rates. The only thing that could be said is that white individuals have the highest PPV, other indivudals a slightly lower PPV, and black individuals having the clear lowest PPV. Based on the information above, this implies that our model is well - calibrated as the algorithim is aproximately equally likely to predict white, black, and other individuals to be employed. The false negative rate is approximately equal across racial groups and the false positive rate is also approximately equal across racial groups; however, the false positive rates are not equal to the false negative rates and our model fails to meet error rate balance. Because our data is binary, and the proportion of individuals in each group predicted to be employed is approximately equal, statistical parity is met for our model. Thus our model can be considered reasonably fair.\n\nimport matplotlib.pyplot as plt\n\n# pull out prevalence from the dataset\np_W, p_B, p_O = df.groupby(\"group\")[\"label\"].mean()\n\n# fix the PPV to the lowest of the PPV across groups\nPPV_fixed = min(PPV_W, PPV_B, PPV_O)\n\n# function to compute FPR given FNR based on eqn 2.6\ndef compute_FPR(fnr, p, ppv_fixed):\n    return (p / (1 - p)) * ((1 - ppv_fixed) / ppv_fixed) * (1 - fnr)\n\n# Generate FNR-FPR tradeoff curves for each group\nfnr_range = np.linspace(0, 1, 100)\nfpr_W_curve = compute_FPR(fnr_range, p_W, PPV_fixed)\nfpr_B_curve = compute_FPR(fnr_range, p_B, PPV_fixed)\nfpr_O_curve = compute_FPR(fnr_range, p_O, PPV_fixed)\n\n# generate an empty plot\nplt.figure(figsize = (8,6))\n\n# plot the feasible trade off curves\nplt.plot(fnr_range, fpr_W_curve, color = \"blue\", label = \"White Individuals\")\nplt.plot(fnr_range, fpr_B_curve, color = \"red\", label = \"Black Individuals\", linestyle = \"dashed\")\nplt.plot(fnr_range, fpr_O_curve, color = \"green\", label = \"Other Individuals\", linestyle = \"dashdot\")\n\n# mark the observed (FNR, FPR) values\nplt.scatter(FNR_W, FPR_W, color = \"blue\", label = \"White Individuals\", marker = \"x\")\nplt.scatter(FNR_B, FPR_B, color = \"red\", label = \"Black Individuals\", marker = \"x\")\nplt.scatter(FNR_O, FPR_O, color = \"green\", label = \"Other Individuals\", marker = \"x\")\n\n# Labels and legend\nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible (FNR, FPR) combinations\")\nplt.legend()\nplt.grid(True)\n\n\n\n\n\n\n\n\nTo equalize FPR across groups, the FNR for white individuals would need to increase, while the FNR for black and other individuals would need to decrease. The exact amount of change is dependent on the slopes of the curves. Lets calculate the neccesary changes.\n\n# Compute the target FPR (mean across groups)\ntarget_FPR = (FPR_W + FPR_B + FPR_O) / 3\n\n# Compute adjustments for each group\nadj_W = FNR_W * (target_FPR / FPR_W)\nadj_B = FNR_B * (target_FPR / FPR_B)\nadj_O = FNR_O * (target_FPR / FPR_O)\n\nprint(\"The required FNR adjustment for white individuals is: \" + str(adj_W))\nprint(\"The required FNR adjustment for black individuals is: \" + str(adj_B))\nprint(\"The required FNR adjustment for other individuals is: \" + str(adj_O))\n\nThe required FNR adjustment for white individuals is: 0.14415345616394387\nThe required FNR adjustment for black individuals is: 0.14999332410359234\nThe required FNR adjustment for other individuals is: 0.14591204404703959\n\n\n\n\nDiscussion\nBecause our model is relatively fair across all groups, and has reasonable overall predictive accuracy many companies which may want to consider employment status could be imterested. A primary example would be banks seeking to hand out loans; employment status is a great marker of financial stability. Banks would not want to given loans that would be unlikely to receive back, thus they would want to be able to do predict the employment status of the individuals seeking loans.\nDeploying this model for large-scale predictions in commercial or governmental settings could influence critical decisions, such as loan approvals, hiring practices, or government assistance distribution. Since the model demonstrates fairness across racial groups and satisfies error rate balance and statistical parity, it reduces the risk of systemic bias in predictions. However, small discrepancies, such as lower positive predictive value for Black individuals, could still lead to disparities in real-world outcomes; this could especially be true in systemically oppressed neighborhoods which have disproportionately higher unemployment rates. This could result in continuing to deny these individuals the resources they need.\nAlthough our model displays error rate balance, the difference is only ~ 3%. This is a small difference and is likely not incredibly harmful. Across all other measures the models is reasonably fair. The only other troubling statistic is the fairly large decrease in PPV for black individuals as opposed to white individuals.\nBeyond bias, several potential issues with deploying this model could raise concerns. First, the model‚Äôs reliance on demographic data may lead to privacy risks and ethical dilemmas, particularly if sensitive attributes are misused or inferred. Additionally, employment status is dynamic and influenced by economic conditions, meaning the model could become outdated over time. To address these concerns, I would propose several solutions. First, strict data privacy protocols should be in place to ensure that sensitive information is handled securely and used responsibly. Second, human oversight should complement automated decisions, especially in high-stakes scenarios, to prevent over-reliance on imperfect predictions. Third, regular model audits and recalibrations should be conducted to maintain accuracy and fairness as employment patterns and economic conditions shift. Finally, transparency in model deployment is essential‚Äîindividuals affected by the predictions should have access to explanations and recourse options if they believe they were unfairly assessed."
  },
  {
    "objectID": "posts/Blog 1/index.html",
    "href": "posts/Blog 1/index.html",
    "title": "Blog 1 - Classifying Palmer Penquins",
    "section": "",
    "text": "Abstract\nThis blog utilizes the Palmer Penguins dataset to develop predictive models for determining the species of penguins based on their morphological measurements. The dataset comprises various features, including culmen length and depth, flipper length, and body mass, across three species: Adelie, Chinstrap, and Gentoo. Qualitative features such as Island, Clutch Completion, and Sex are also included. Through visual analysis, features which differed between species were identified and selected for model training. Both Logistic Regression and Decision Trees (not shown) were implemented and evaluated. Model performance was assessed using training accuracy both absolute and through cross validation as well as assessment on separate testing data.\n\n\nData Preparation and Feature Selection\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution.\nLets look at the three quantitative predictor variables and plot the combinations we can make.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Depth vs Culmen Length\")\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth vs Flipper Length\")\n\nsns.stripplot(x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Culmen Length vs Flipper Length\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile Culmen Depth vs Flipper Length provides the clearest specification for Gentoo Penguins, Adelie and Chinstrap become too muddled to properly distinguish. Culmen Length vs Flipper Length does a good job in making three clusters, with Chinstrap isolating the most, howeever there is still overlap in the regions of Gentoo and Adelie.\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoders for categorical variables\nle = LabelEncoder()\nle.fit(df[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare data\ndf_train, y_train = prepare_data(df)\n\n# Visualize our new training data\ndf_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nWith our new training data lets prepare a new data frame only including the predictors we want\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\ncols = []\nfor pair in combinations(all_quant_cols, 2):\n    \n    # Combinations to test training accuracy of\n    cols.append(list(pair) + all_qual_cols)\n\nfor combo in cols:\n    print(combo)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n\nTesting the Model\nNow that we have our combination of predictors, lets test the training accuracy of Linear Regression models on each predictor to determine which is the best.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\ni = 0\nbest_acc = 0\nbest_combo = 1\n\nfor combo in cols:\n    \n    print(\"Testing accuracy for predictors: \" + str(combo) + \"\\n\")\n\n    LR = LogisticRegression(max_iter = 10000)\n    m = LR.fit(df_train[combo], y_train)\n    \n    acc = LR.score(df_train[combo], y_train)\n    \n    if acc &gt; best_acc:\n        best_acc = acc\n        best_combo = i\n    \n    print(\"Iteration \" + str(i + 1) + \" has a training accuracy of: \" + str(acc))\n    cv_scores_LR = cross_val_score(LR, df_train, y_train, cv = 5)\n    print(\"Iteration \" + str(i + 1) + \" also cross validation: \" + str(cv_scores_LR) + \"\\n\")\n    i += 1\n\nprint(\"The best combination of predictors is: \" + str(cols[best_combo]))\n\n# pull out the columns that we want based on the best training accuracy\npredictor_cols = cols[best_combo]\n\nX_train = df_train[predictor_cols]\n\nLR_real = LogisticRegression(max_iter = 10000)\nm_real = LR_real.fit(X_train, y_train)\n\nTesting accuracy for predictors: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 1 has a training accuracy of: 0.99609375\nIteration 1 also cross validation: [1. 1. 1. 1. 1.]\n\nTesting accuracy for predictors: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 2 has a training accuracy of: 0.9765625\nIteration 2 also cross validation: [1. 1. 1. 1. 1.]\n\nTesting accuracy for predictors: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 3 has a training accuracy of: 0.8828125\nIteration 3 also cross validation: [1. 1. 1. 1. 1.]\n\nThe best combination of predictors is: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nAfter analyzing the training accuracy for each combination of predictors, Culmen Length and Culmen Depth have the highest training accuracy, thus we will test this model against the test data. Next we will plot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\n# load the testing data to check the accuracy of our model\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# prep the testing data for later use\nX_test, y_test = prepare_data(test)\nX_test = X_test[predictor_cols]\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR_real, X_train, y_train)\n\n\n\n\n\n\n\n\nThe model seems to be making reasonable decisions for classiying Penquin species on each island. The only island in which the model does not make perfect decisions, is Dream island which is home to all species of Penquins. Next, assess the model against the testing data.\n\n# Check the model accuracy against the testing data\nprint(\"Linear Regression for the model has testing accuracy of: \" + str(LR_real.score(X_test, y_test)))\n\nLinear Regression for the model has testing accuracy of: 1.0\n\n\n100% testing accuracy has been achieved with a Linear Regression model for classifying Penquin Species. Lets view the confusion matrix for our succesfull model as well as look at the decision regions for the model evaluated on the test set.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR_real.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix For the Model:\\n\" + str(C))\n\nplot_regions(LR_real, X_test, y_test)\n\nConfusion Matrix For the Model:\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\n\n\n\n\n\n\n\n100% accuracy has been achieved! We have now developed a model with 100% testing accuracy for identifying Palmer Penquins based on physiologcal characteristics. By plotting the decision regions versus the test data and by looking at the confusion matrix, it becomes clear that the model was able to distinguish every penquin soley based on the provided information. If you ran this code yourself and did not achieve 100% accuracy on one of these models, re-run the code with new training and testing data (this will produce a model with 100% testing accuracy after 2 - 3 tries if not on the first). Since it was mentioned above, lets run the DecisionTreeClassifiers for fun.\n\n\nDiscussion\nThrough the process of analyzing the Palmer Penguins dataset, several key insights were uncovered regarding the classification of penguin species based on their physical characteristics. First, identifying the quantitative features which created the clearest three groupings was key. In fact, more time should have been spent in the beginning graphing out possible combinations of quantitative features; this process may have more quickly identified Culmen Length and Depth as the best features for clustering penquin species together. Plotting tables for the qualitative features also proved key, as it quickly identified that Island was a good indicator of species. In addition, when dealing with models such as the DecisionTreeClassifier (not shown in this post) utilizing cross validation as a check for overfitting helped identify the correct depth range for our models, as lower values procuded worse testing accuracy, but the extremely high complexity values produced overfitted results. Finally, as a general note, I would spend more time in the beginning exploring potential data combinations via graphical methods in order to more quickly identify the predicators which might work best rather than plug and chug; I did, however, enjoy the process of tinkering with the different models and datasets until I found the perfect one."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing whether or not I can edit this"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 5 - Implementing Logistic Regression\n\n\n\n\n\nImplementing gradient descent to solve emperical risk minimization for Logistic Regression\n\n\n\n\n\nApr 7, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4 - Perceptron\n\n\n\n\n\nImplementing and Testing the Perceptron Algorithim\n\n\n\n\n\nMar 24, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3 - Auditing Bias\n\n\n\n\n\nPredicting employment status in Missouri on the basis of demographics and auditing the fairness of our model.\n\n\n\n\n\nMar 12, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 2 - Maximizing Loan Profit\n\n\n\n\n\nThis blog post identifies variables in individual loaners personal information and history to identify the best variables to fit a linear score based classifier in order to maximize bank profits per buyer.\n\n\n\n\n\nMar 5, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1 - Classifying Palmer Penquins\n\n\n\n\n\nBuilding a machine learning model to classify Palmer Penquins based on physical properties\n\n\n\n\n\nFeb 26, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog 2/index.html",
    "href": "posts/Blog 2/index.html",
    "title": "Blog 2 - Maximizing Loan Profit",
    "section": "",
    "text": "Introduction\nAccess to bank loans plays a critical role in enabling individuals and businesses to finance important expenses, from home purchases to entrepreneurial ventures. This blog post explores the landscape of bank lending, focusing on the factors influencing loan approval, interest rates, and default risks. Variables such as loan intent were explored across age groups, as well as the variables affecting load grade. High grade loans (A) are associated with individuals at low risk of defaulting. Through exploratory data analysis income, loan amount, loan as a percent of income, loan interest rate, and past default hisotry were identified as variables most stronly associated with loan quality. Weigthed variables that took into account past default history as well as percent income, loan amount, and interst rate were also generated. Linear Regression was then used to determine the combination of these variables that could most accurately predict whether or not a loaner would default. The weight vector for these variables was taken to build a linear score function, and the score function threshold was optimized to maximize bank profit per buyer in the final model. In the final model, loan as a percent of income, loan interest weight, weighted interest rate, and past default history were selected as predictors with a threshold of t = 2.2, resulting in an expected profit per buyer of 1294.33.\n\n\nData Exploration\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# load the raw bank data and clean it\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n# drop rows with NA values\ndf_train = df_train.dropna()\n\n# convert the interest rate to a percent\ndf_train[\"loan_int_rate\"] = df_train[\"loan_int_rate\"] / 100\n\n# make a copy of the data to use and view\ndf_ = df_train.copy()\ndf_.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n0.1491\n1\n0.25\nN\n2\n\n\n\n\n\n\n\nLets explore how loan intent varies across age groups\n\n# first lets add a column for age groups by creating age groups with 10 year ranges (ex. 20 - 29 is group 2)\ndf_[\"age_group\"] = df_[\"person_age\"] // 10\ndf_[\"age_group\"][df_[\"age_group\"] &gt;= 10 ] = 10 # group all loaners older than 100 into a single group\n\n# Get the counts of each loan intent within each age group\nloan_intent_counts = df_.groupby(\"age_group\")[\"loan_intent\"].value_counts().unstack()\nloan_intent_counts = loan_intent_counts.fillna(0) # fill NA values with 0 instead\n\n# Convert counts to proportions relative to each age group\nloan_intent_prop = loan_intent_counts.div(loan_intent_counts.sum(axis=1), axis=0)\n\n# conert the table into long format so that each loan intent can be plotted for each age group\ntemp = pd.melt(loan_intent_prop)\n\ntemp[\"age_group\"] = 0 # rebuild the age group category exactly as calculated previosly\nfor i in range(0, len(temp) + 1):\n  temp[\"age_group\"][i] = (i % 9) + 2\n\n# plot loan intent as a proportion of each age group\nax = sns.barplot(data = temp, x = \"age_group\", y = \"value\", hue = \"loan_intent\")\nax.set_xlabel(\"Age Group\")\nax.set_ylabel(\"Proportion\")\nax.set_title(\"Loan Intent by Age\")\n\nText(0.5, 1.0, 'Loan Intent by Age')\n\n\n\n\n\n\n\n\n\nThe chart indicates that personal loans are most common among individuals aged 20-29, 40-49, and 90-99, suggesting a consistent need for general-purpose borrowing across different life stages. Medical loans peak in the 40-49 and 50-59 age groups, likely due to increasing healthcare expenses. Education loans spike in the 70-79 age group, which may indicate late-stage career shifts or funding for dependents. Debt consolidation loans are relatively steady, with moderate representation in the 30-39, 50-59, and 60-69 age groups. Home improvement loans have lower proportions overall but see slight increases among borrowers aged 20-29 and 80-89. Venture loans are sporadic, with small peaks in the 50-59 and 60-69 age groups, possibly reflecting mid-life entrepreneurial activity. These trends highlight how borrowing needs evolve across different age brackets; lets explore income relates to loan size.\n\n# make a copy of the data frame to work with and create income categories similar to our age group categories (ex. $0 - $99,999 is group 0)\ndf_2 = df_train.copy()\ndf_2[\"income_cat\"] = df_2[\"person_income\"] // 50000\ndf_2[\"income_cat\"][df_2[\"income_cat\"] &gt;= 10] = 10 # group all income creater than $1,000,000 into a single group\n\n# find the mean loan amount in each income category\nprint(df_2.groupby(\"income_cat\")[\"loan_amnt\"].mean().round())\n\nincome_cat\n0      7061.0\n1     10485.0\n2     13356.0\n3     15176.0\n4     16728.0\n5     19252.0\n6     18127.0\n7     20516.0\n8     19615.0\n9     17306.0\n10    14535.0\nName: loan_amnt, dtype: float64\n\n\nWhile salary (income category) does seem to affect the loan amount, with people having higher salaries taking out larger loans, the higher income categories actually often have lower loan amounts than the lower income categories. The spread of loan amount vs income fits a in some ways a bell curve. This may be a product of people making larger salaries not needing to take out loans as frequently as the middle salaries, which can afford to take out larger loans and pay them back. Let‚Äôs also determine what factors seem to be related to higher interest rates.\n\n# group the data by loan grade and view the mean interest rates for each loan grade\nprint(df_train.groupby([\"loan_grade\"])[\"loan_int_rate\"].mean())\n\nloan_grade\nA    0.073384\nB    0.110033\nC    0.134562\nD    0.153583\nE    0.170473\nF    0.185190\nG    0.202300\nName: loan_int_rate, dtype: float64\n\n\nHigh grade loans (loans which are expected to be paid back) are associated with the lowest interest rates. There seems to be a direct correlation between loan grade and interest rate. Lets explore factors related to loan grades by grouping.\n\n# group the data by loan grade and view the mean income, mean loan amount, and mean percent income for each load grade\ntemp = df_train.groupby(\"loan_grade\")[[\"person_income\", \"loan_amnt\", \"loan_percent_income\"]].mean()\n\nprint(temp)\n\n            person_income     loan_amnt  loan_percent_income\nloan_grade                                                  \nA            66773.007816   8555.884885             0.152629\nB            66662.091096  10031.025007             0.173846\nC            66416.633130   9322.102794             0.168928\nD            64555.473908  10821.646695             0.188833\nE            70868.349432  12929.083807             0.204190\nF            80756.546012  15395.705521             0.220982\nG            77342.477273  17384.659091             0.243409\n\n\nAs we can see from this datatable, high load grade is directly correlated with lower percent of total income. Meaning that higher quality loans, with lower interest rates, are associated with loaners whose loan takes up the smallest percent of their total income. This is likely related to the loan taking up a lower percent of their income making it less likely for them to default on the loan and be more capable of paying it back. It also seems to follow that people with higher income tend to take out larger loans than would correlate with their increase income.\n\n\nFeature Selection & Weight Assessment\nSelect the predictor and target variables related to loan grade.\n\nfrom sklearn.model_selection import train_test_split\n\n# select the predictor variables identified with loan grade. Also the include the loaners default history\npredictor_cols = [\"person_income\", \"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\", \"cb_person_default_on_file\"]\nX_model = df_train[predictor_cols].copy()  # Use .copy() to avoid modifying the original DataFrame\n\n# target variable selection\ny_model = np.array(df_train[\"loan_status\"])\n\n# Create binary indicators for past defaults\nX_model[\"past_default_yes\"] = X_model[\"cb_person_default_on_file\"] == \"Y\"\nX_model[\"past_default_no\"] = X_model[\"cb_person_default_on_file\"] == \"N\"\n\n# Generate weighted default predictor\nX_model[\"weighted_default\"] = X_model[\"loan_percent_income\"] * (1 +  X_model[\"past_default_yes\"])\n\n# Generate weigthed interest rate predictor\nX_model[\"weighted_interest\"] = (1 + X_model[\"loan_int_rate\"]) * X_model[\"loan_amnt\"] * (1 + X_model[\"past_default_yes\"])\n\n# Drop the original categorical column\nX_model.drop(\"cb_person_default_on_file\", axis=1, inplace=True)\n\n# Breaking into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.2)\n\nFit a Linear Regression model to the data and assess its accuracy of prediction. Try all combinations of predictors. Once a good model is fit, pull out the weight coefficient to use for a linear score based classifier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom itertools import combinations\nfrom sklearn.metrics import accuracy_score\n\n# create arrays to hold all the predictors we will consider\nnumerical_predictors = [\"person_income\", \"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\", \"weighted_default\", \"weighted_interest\"]\nqual_predictors = [\"past_default_yes\", \"past_default_no\"]\npredictor_cols = numerical_predictors + qual_predictors\n\n# create an empty array to store the accuracy results for predictor combinations\nresults = []\n\n# For each combination of predictor variables train a Linear Model and assess its accuracy\nfor r in range(1, len(predictor_cols) + 1): # From 1 feature to all features\n    for subset in combinations(predictor_cols, r): # For each combination of those features\n        \n        # assign the test columns\n        test_cols = list(subset)\n        \n        # Select subset of features for the training and testing data\n        X_train_subset = X_train[test_cols]\n        X_test_subset = X_test[test_cols]\n        \n        # Train a simple logistic regression model to the subset\n        model = LogisticRegression(max_iter=1000)\n        model.fit(X_train_subset, y_train)\n\n        # Predict and evaluate\n        y_pred = model.predict(X_test_subset)\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        # Store results\n        results.append({\"features\": test_cols, \"accuracy\": accuracy})\n\nFrom the list of feature combinations, pull out the top 5 and retest them against the premade training and testing data. Once the best from this list has been identified, verify its accuracy with cross validation to check for overfitting. If the features pass, pull out the weights coefficient to use for a linear score based classifier.\n\n# select the top 5 feature combinations by accuracy\nbest_features = pd.DataFrame(results).sort_values(by = \"accuracy\", ascending = False).head(5)[\"features\"].tolist()\n\n# create an empty array to store the features that will ultimately be selected for the model as well as variable to hold the features testing accuracy\ntrue_predictors = []\nbest_acc = 0\n\n# For each of the top 5 features train a Linear Model and asses its ability to predict on the testing data. Select the features with the highest predictive accuracy as the features to use for bank profit maximization\nfor features in best_features:\n    # fit a model to the feature list\n    LR_test = LogisticRegression()\n    m_test = LR_test.fit(X_train[features], y_train)\n\n    # use the model to generate predictions on the testing data\n    y_pred = LR_test.predict(X_test[features])\n\n    # assess the accuracy of the models predictions against the testing data\n    acc = accuracy_score(y_test, y_pred)\n\n    # if the current features are better than the previous best, reassign the new best\n    if acc &gt; best_acc:\n        best_acc = acc\n        true_predictors = features\n\nprint(\"The predictors with the highest testing accuracy are: \" + str(true_predictors) + \"\\nAccuracy = \" + str(best_acc))\n\n# Check with cross validation\nLR_real = LogisticRegression()\nm_real = LR_real.fit(X_train[true_predictors], y_train)\n\ncv_scores_LR = cross_val_score(LR_real, X_test[true_predictors], y_test, cv = 5)\n\nprint(\"Cross validation for the selected predictors: \" + str(cv_scores_LR))\n\n# store the weight coefficients for this model to build our linear score function\nw = m_real.coef_\n\n# store the dimenstions the of weight vector as it must be reshaped to a n x 1 instead of a 1 x n\ns = w.shape\n\n# reshape the weight vector\nw = w.reshape(s[1], s[0])\n\nThe predictors with the highest testing accuracy are: ['loan_percent_income', 'loan_int_rate', 'weighted_interest']\nAccuracy = 0.8347883020515059\nCross validation for the selected predictors: [0.82878953 0.81788441 0.8220524  0.81659389 0.82969432]\n\n\nUpon inspecting accuracy versus the overall test set income the predictors above were selected as the best variables based purely on accuracy. These predictors also had reasonable cross validation scores that were similar to the overall testing accuracy; this implies that the model is consisent and not overfit. The weight constants from this model were then pulled out and stored for later used in a linear score function.\n\n\nOptimizing a Linear Score Classification\nStart by defining our linear score function using our weight vector and our classification function with an arbitrary starting threshold. We will then optimize this threshold such that the bank makes the largest possible profit per loan.\n\nfrom matplotlib import pyplot as plt\n\n# Performs the dot product between the data vector X and the weight vector w\ndef score(X, w):\n    return np.dot(X,w)\n\n# Returns the predictions of the model. 1 if the loaner is predicted to default, and 0 if they are predicted to repay the loan\ndef predict(score, threshold, df):\n  scores = score(df, w)\n  return 1*(scores &gt; threshold)\n\n# pull out model data\nX_model = X_train.copy()\n\n# initialize the benefit column\nX_model[\"benefit\"] = 0\n\n# initialize variables to store the best benefit and threshold through iteration\nbest_benefit = 0\nbest_threshold = 0\n\n# assign scores to each individual based on the features selected previously and the weight vector associated with these features\nscores = score(X_model[true_predictors], w)\n\n# generate a plot to visualize how profit changes as threshold changes\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n# for thresholds between 0 and 10\nfor t in np.linspace(0, 10, 101):\n    \n    # assign predictions based on the threshold\n    X_model[\"y_pred\"] = scores &gt;= t\n\n    # calculate whether each indivdual is a true negative or a false negative\n    X_model[\"tn\"] = (X_model[\"y_pred\"] == 0) & (y_train == 0)\n    X_model[\"fn\"] = (X_model[\"y_pred\"] == 0) & (y_train == 1)\n    \n    # calculate the gain/loss for each individual based on whether they are considered a true negative or a false negative\n    X_model[\"benefit\"][X_model[\"tn\"]] = X_model[\"loan_amnt\"] * (1 + 0.25 * X_model[\"loan_int_rate\"]) ** 10 - X_model[\"loan_amnt\"]\n    X_model[\"benefit\"][X_model[\"fn\"]] = X_model[\"loan_amnt\"] * (1 + 0.25 * X_model[\"loan_int_rate\"]) ** 3 - 1.7 * X_model[\"loan_amnt\"]\n\n    # sum the total benefit across all invididuals and divide by the total number of individuals to find the average profit per loaner\n    average_benefit = np.sum(X_model[\"benefit\"]) / len(X_model)\n\n    # add this point to the cumulative graph\n    ax.scatter(t, average_benefit, color = \"steelblue\", s = 10)\n\n    # if the current benefit is bette than the previous best, reassign the benefit and store the current threshold\n    if average_benefit &gt; best_benefit:\n        best_benefit = average_benefit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net benefit\", title = f\"Best benefit ${best_benefit:.2f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nMaximization of the bank‚Äôs profit results in a threshold of 4.7, with a profit per buyer of 1308.88. The value of the threshold has been stored for later use. Lets now test the model with our optimal weight vector and threshold against new testing data.\n\n# load the testing data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n# drop NA columns and convert the interest rate to percent\ndf_test = df_test.dropna()\ndf_test[\"loan_int_rate\"] = df_test[\"loan_int_rate\"]  /100\n\n# Create binary indicators for past defaults\ndf_test[\"past_default_yes\"] = df_test[\"cb_person_default_on_file\"] == \"Y\"\ndf_test[\"past_default_no\"] = df_test[\"cb_person_default_on_file\"] == \"N\"\n\n# Generate weighted default predictor\ndf_test[\"weighted_default\"] = df_test[\"loan_percent_income\"] * (1 +  df_test[\"past_default_yes\"])\n\n# Generate weigthed interest rate predictor\ndf_test[\"weighted_interest\"] = (1 + df_test[\"loan_int_rate\"]) * df_test[\"loan_amnt\"] * (1 + df_test[\"past_default_yes\"])\n\n# Drop the original categorical column\ndf_test.drop(\"cb_person_default_on_file\", axis=1, inplace=True)\n\n# initialize the benefit column\ndf_test[\"benefit\"] = 0\n\n# calculate the score array\nscores = score(df_test[true_predictors],w)\n\n# calculate predictions based on the optimal threshold\ndf_test[\"y_pred\"] = scores &gt;= best_threshold\n\n# determine if each individual can be considered either a true negative or a false negative\ndf_test[\"tn\"] = (df_test[\"y_pred\"] == 0) & (df_test[\"loan_status\"] == 0)    \ndf_test[\"fn\"] = (df_test[\"y_pred\"] == 0) & (df_test[\"loan_status\"] == 1)\n\n# calculate the gain / loss for each individual based on whether they are true negative or a true positive\ndf_test[\"benefit\"][df_test[\"tn\"]] = df_test[\"loan_amnt\"] * (1 + 0.25 * df_test[\"loan_int_rate\"]) ** 10 - df_test[\"loan_amnt\"]\ndf_test[\"benefit\"][df_test[\"fn\"]] = df_test[\"loan_amnt\"] * (1 + 0.25 * df_test[\"loan_int_rate\"]) ** 3 - 1.7 * df_test[\"loan_amnt\"]\n\n\n# calculate the average profit per loaner based on the total benefit and the number of loaners\naverage_benefit = np.sum(df_test[\"benefit\"]) / len(df_test)\n\nprint(\"The expected profit per buyer is: $\" + str(round(average_benefit, 2)))\n\nThe expected profit per buyer is: $1239.44\n\n\nThe expected profit per buyer on the test set (1239.44) is only slightly lower than the expected profit per buyer on the training set (1308.88). Thus our model was able to generalize from the training set.\n\n\nEvaluating Model Fairness\nLets explore if some loaners are more or less likely to receive loans based on their age\n\n# remake age groups in 10 year gaps and calculate the difference in default prediction vs reality\n# age group 2 represents loaners aged between 20 and 29 years old\ndf_test[\"age_group\"] = df_test[\"person_age\"] // 10\n\n# calculates the difference between prediction and actual outcome of loan status (1 means predicted to default and did not, 0 represents accurate prediction, -1 represent failure to predict default)\ndf_test[\"diff\"] = df_test[\"y_pred\"] - df_test[\"loan_status\"]\n\n# Group by age and view mean prediction, actual default status, and the mean difference\ntemp = df_test.groupby(\"age_group\")[[\"loan_status\", \"y_pred\", \"diff\"]].mean().reset_index()\ntemp\n\n\n\n\n\n\n\n\nage_group\nloan_status\ny_pred\ndiff\n\n\n\n\n0\n2\n0.228689\n0.189085\n-0.039604\n\n\n1\n3\n0.204082\n0.153846\n-0.050235\n\n\n2\n4\n0.200000\n0.119231\n-0.080769\n\n\n3\n5\n0.261905\n0.095238\n-0.166667\n\n\n4\n6\n0.500000\n0.500000\n0.000000\n\n\n5\n7\n0.500000\n0.000000\n-0.500000\n\n\n\n\n\n\n\nIn almost all age groups, other than age group 6, the algorithm underpredicts the loan default rate. However, two age groups which are less likely to get a loan than other groups (excluding group 7 as it only has two individuals), are age group 20-29 and 30-39. Despite, the algorithim underpredicting the default rate in this age group these groups are still predicted to be much more likely to defualt than the other age groups. The algorithm also seems to underpredict default rates more as loaners grow older. Lets now look at the difficulty of getting loans based on loan intent.\n\n# create a copy dataset to work with\ntemp2 = df_test.copy()\n\n# convert the data to long format so that both prediction and actual default outcome can be viewed for each loan intent\ntemp2_melted = pd.melt(temp2, id_vars = \"loan_intent\", value_vars = [\"y_pred\", \"loan_status\"], var_name = \"Type\", value_name = \"Proportion of Loan Status\")\n\n# Generate a bar plot of defualt prediction rate  vs actual default rate for each loan type\nsns.barplot(data = temp2_melted, y = \"loan_intent\", x = \"Proportion of Loan Status\", hue = \"Type\", palette = \"BuPu\", saturation = 0.5, orient = \"h\", errorbar = None)\n\n\n\n\n\n\n\n\nMedical, Debt Consolidation, and Personal loans have the highest predicted default rate making them the hardes loans to get; however both Medical and Debt Consolidation loans do have the highest actual default rate. Education and Venture are the next hardest loans to get. Interestingly, both Venture and Education loans are preidcted to default more than they actually do and are the only loan type to display this behavior. Home Improvement loans are predicted to default the least, despite having the third highest actual default rate. Now, lets examine how gross income affects the ease with which credit can be obtained.\n\n# create a copy dataset to work with\ntemp3 = df_test.copy()\n\n# Group income into categories separated by $25,000 in income (ex. group 1: loaners making up to $24,999)\ntemp3[\"income_group\"] = temp3[\"person_income\"] // 25000\ntemp3[\"income_group\"][temp3[\"income_group\"] &gt;= 10 ] = 10 # assign all loaners making $250,000 or more into one group\n\n# convert the data to long format so that both prediction and actual default outcome can be viewed for income group\ntemp3_melted = pd.melt(temp3, id_vars = \"income_group\", value_vars = [\"y_pred\", \"loan_status\"], var_name = \"Type\", value_name = \"Proportion of Loan Status\")\n\n# Generate a bar plot of defualt prediction rate vs actual default rate for each income group\nsns.barplot(data = temp3_melted, x = \"income_group\", y = \"Proportion of Loan Status\", hue = \"Type\", palette = \"BuPu\", saturation = 0.5, errorbar = None)\n\n\n\n\n\n\n\n\nIn this model, lower income is strongly associated with greater likelihood to default, making it much harder get loans at low income. While lower income is generally associated with higher default rates, it is unrealistic that the model predicts loaners making more than 75,000 to pay back their loans 100% of the time. In fact, the second highest default rate is actually in the 200,000 - 224,999 income category.\n\n\nConclusion\nOur analysis revealed that income stability, credit history, and collateral availability are the most significant determinants in securing a loan. Borrowers with lower credit scores or irregular income streams often struggle to obtain loans or face significantly higher interest rates. While banks aim to mitigate risks through stringent approval processes, these measures inadvertently create barriers for individuals in need, particularly those seeking medical loans or small business financing.\nIndividuals seeking loans for medical expenses often face greater difficulties in obtaining credit due to the high default rates associated with medical debt. From a financial standpoint, banks perceive these loans as high-risk due to the uncertainty surrounding medical outcomes and the borrower‚Äôs ability to repay. However, this raises ethical concerns regarding fairness in credit access. Fairness, in this context, can be defined as the equitable opportunity for all individuals to obtain financial assistance without undue discrimination or excessive barriers.\nGiven that medical expenses are often urgent and unavoidable, it is arguably unjust that those in dire need of healthcare financing encounter significant hurdles. A fairer approach could involve government-backed loan programs, flexible repayment structures, or the integration of medical hardship considerations into lending criteria. By reassessing how medical loans are evaluated, financial institutions can contribute to a system that balances risk management with social responsibility."
  },
  {
    "objectID": "posts/Blog 4/index.html",
    "href": "posts/Blog 4/index.html",
    "title": "Blog 4 - Perceptron",
    "section": "",
    "text": "Abstract\nThe Perceptron algorithm iteratively updates a weight vector for binary classification, with each iteration running in O(p) time, where p is the number of features. While the number of data points (n) does not affect individual updates. Experiments show that the Perceptron always converges on linearly separable data, though iteration count varies. Notably, higher-dimensional data converges faster, suggesting that additional features aid optimization. The algorithm will never converge on linearly separable data and demonstrates high volatility in the loss after each update.\n\n\nMethods Overview\nPerceptron source code used in this project: https://raw.githubusercontent.com/Nibsquatch/Nibsquatch.github.io/refs/heads/main/perceptron.py\nPerceptron.grad() was implemented by taking as input a randomely selected entry in the feature matrix and the associated target vector. The target vector comes in the form {0,1} and is converted to {-1, 1} respectively. The score for the entry is computed, and if the score is negative then a perceptron update is passed, otherwise an update containing only a zero shift is passed.\n\n\nLoading Perceptron\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\n\nTesting That Our Perceptron Algorithim Works\nLets run a training loop to check that our Peceptron algorithim is running as expected and will eventually achieve a loss equal to 0 on linearly separable data. First we will need to generate linearly separable data.\n\nfrom matplotlib import pyplot as plt\nimport torch\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\n# first we need to generate data\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\n# assign the example data to variables\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nNow that we have generated linearly separable data, lets visualize before we begin implementing linear decision boundaries.\n\n# Visualize our training data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTime to initiate our training model to test whether or not our Perceptron Algorithim can achieve a loss of 0.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,)).item()\n    x_i = X[i].unsqueeze(0)\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAlthough it took a large number of iterations, our perceptron algorithim was eventually able to reach a loss of 0 and is able to identify the proper weight vector to linearly separate the data points. Lets look at our data with the final weight vector plotted.\n\n# define a function to plot a line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot the existing data and add a line to the graph\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\n\nplot_perceptron_data(X, y, ax) # plot data\ndraw_line(p.w, -.5, 1.5, ax, color = \"black\") # draw weight vector\n\n\n\n\n\n\n\n\nFrom the visualization, we can tell that the minimization of loss 0 has correctly identified a linear line which perfectly separates the data into their correct classes. Now, let‚Äôs explore the algorithim performs on data which is mathematically impossible to seprate linearly. A max iterations of 1000 will be applied to the training loop, as the algorithim will run forever without a cap.\n\n# Generate Linearly Inseparable Data\ndef perceptron_data_insep(n_points = 270, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # add 30 points that are the opposite class in a similar location as a previous point, thus making linearly inseparable data\n    for _ in range(30):  \n        rand_i = torch.randint(0, n_points, (1,))  # Pick a random point index\n        x_new = X[rand_i] + torch.normal(0.0, noise, size=(1, p_dims + 1))  # Slightly modify the chosen point\n        \n        # flip class\n        y_new = torch.tensor([1 - y[rand_i].item()])  # Convert to tensor\n\n        X = torch.cat((X, x_new), dim=0)  # Add new point\n        y = torch.cat((y, y_new), dim=0)  # Add new label\n\n    return X, y\n\nNow that we have generated linearly inseparable data, lets visualize before we attempt to implement a linear decision boundaries.\n\n# Visualize our training data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data_insep()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nTime to initiate a training model on linearly inseparable data, with a max iterations of 1000.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\niterations = 0\n\nwhile loss &gt; 0 and iterations &lt; 1000: # terminates after 1000 iterations\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,)).item()\n    x_i = X[i].unsqueeze(0)\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    # iterate\n    iterations += 1\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nCompared to the linearly separable data, the algorithim has significant trouble minimizing the loss. Instead of a generally smooth curve, the loss function jumps from low to high as there is never a solution to the problem the algorithim is trying to solve. Lets look at our data with the final weight vector plotted, despite the weight vector being imperfect.\n\n# plot the existing data and add a line to the graph\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\n\nplot_perceptron_data(X, y, ax) # plot the data\ndraw_line(p.w, -.5, 1.5, ax, color = \"black\") # plot the final weight vector\n\n\n\n\n\n\n\n\nAs expected, our model is not able to perfectly differentiate between classes, as there is no linear classifier that can accomplish the task. Now lets explore how the loss function evolves on data of more than two dimenstions.\n\n# Generate 5 dimensional data\ndef perceptron_data5(n_points = 300, noise = 0.2, p_dims = 5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    return X, y\n\nX, y = perceptron_data5()\n\n# perform the training loop\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # terminates after 1000 iterations\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,)).item()\n    x_i = X[i].unsqueeze(0)\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThe Perceptron Algorithim is able to work extremely quickly to reduce the loss to 0, even on data with dimensionality greater than 2; in fact, the Perceptron algorithm seems to run even faster for data of higher dimensionality. Lets explore if 10 dimensional data will be optimized even faster.\n\n# Generate 10 dimensional data\ndef perceptron_data10(n_points = 300, noise = 0.2, p_dims = 10):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    return X, y\n\nX, y = perceptron_data10()\n\n# perform the training loop\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # terminates after 1000 iterations\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,)).item()\n    x_i = X[i].unsqueeze(0)\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAlthough the Perceptron algorithim optimized the weight vector for the 10 dimensional data much faster than it did for 2 dimensional data, the rate of optimization does not seem to have a purely linear relationship. In general, multidimensional data does seem to be able to be optimized faster than simple 2 dimensional data.\n\n\nDiscussion\nReflecting on the time complexity of the Perceptron algorithm, the algorithim selects a random point which takes O(1) time; although the number of entries (n) may increase this does not affect the runtime of selecting one at random. However, for each Perceptron update the score for this entry must be calculated which is dependent on the number of features (p), as each score is calculated as a dot product. Thus, each iteration of the Perceptron algorithim takes O(p) time.\nFindings: When given linearly separable data the Perceptron algorithm will always find a weight vector that achieves a loss of 0, even if it takes in some cases thousands of iterations; the time it takes for the Perceptron algorithm to optimize the weight vector also seems to have some relationshop to the number of dimensions in the data, with multidimensional data being optimized in much fewer iterations than simple 2 dimensional data. The algorithm will also never converge on non linearly separable data, and the loss fucntion across each Perceptron update demonsrates high volatility, as each update is attempting to reach a solution which does not exist."
  }
]