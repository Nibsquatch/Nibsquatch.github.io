[
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that youâ€™ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Blog 2/index.html",
    "href": "posts/Blog 2/index.html",
    "title": "Blog 2 - Maximizing Loan Profit",
    "section": "",
    "text": "Abstract\n\n\nData Preparation\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing whether or not I can edit this"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 2 - Maximizing Loan Profit\n\n\n\n\n\nUsing a linear score based classifier to maximize bank profits on loans\n\n\n\n\n\nMar 5, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1 - Classifying Palmer Penquins\n\n\n\n\n\nBuilding a machine learning model to classify Palmer Penquins based on physical properties\n\n\n\n\n\nFeb 26, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Blog 1 - Classifying Palmer Penquins",
    "section": "",
    "text": "Abstract\nThis blog utilizes the Palmer Penguins dataset to develop predictive models for determining the species of penguins based on their morphological measurements. The dataset comprises various features, including culmen length and depth, flipper length, and body mass, across three species: Adelie, Chinstrap, and Gentoo. Qualitative features such as Island, Clutch Completion, and Sex are also included. Through visual analysis, features which differed between species were identified and selected for model training. Both Logistic Regression and Decision Trees (not shown) were implemented and evaluated. Model performance was assessed using training accuracy both absolute and through cross validation as well as assessment on separate testing data.\n\n\nData Preparation and Feature Selection\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution.\nLets look at the three quantitative predictor variables and plot the combinations we can make.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Depth vs Culmen Length\")\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth vs Flipper Length\")\n\nsns.stripplot(x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Culmen Length vs Flipper Length\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile Culmen Depth vs Flipper Length provides the clearest specification for Gentoo Penguins, Adelie and Chinstrap become too muddled to properly distinguish. Culmen Length vs Flipper Length does a good job in making three clusters, with Chinstrap isolating the most, howeever there is still overlap in the regions of Gentoo and Adelie. Culmen Depth vs Culmen Length provides the three most visually distinct clusters for each Penquin species while minimizes visual overlap. Thus, Culmen Depth vs Culme Length will serve as the best choice for quanitative predictors.\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoders for categorical variables\nle = LabelEncoder()\nle.fit(df[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare data\ndf_train, y_train = prepare_data(df)\n\n# Visualize our new training data\ndf_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nWith our new training data lets prepare a new data frame only including the predictors we want.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\n\n# Select the columns we want and separate into predictors\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train = df_train[predictor_cols]\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nIsland_Biscoe\nIsland_Torgersen\nIsland_Dream\n\n\n\n\n0\n40.9\n16.6\nFalse\nFalse\nTrue\n\n\n1\n49.0\n19.5\nFalse\nFalse\nTrue\n\n\n2\n50.0\n15.2\nTrue\nFalse\nFalse\n\n\n3\n45.8\n14.6\nTrue\nFalse\nFalse\n\n\n4\n51.0\n18.8\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nFitting a Model\nFitting a Linear Regression ML model.\n\nLR = LogisticRegression(max_iter = 10000)\nm = LR.fit(X_train, y_train)\n\nTime to check the training accuracy of the model and cross validate as well as pre-process the testing data for later use.\n\nfrom sklearn.model_selection import cross_val_score\n\n# load the testing data to check the accuracy of our model\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# prep the testing data for later use\nX_test, y_test = prepare_data(test)\nX_test1 = X_test[predictor_cols]\n\nprint(\"Linear Regression for dataset 1 has a training accuracy of : \" + str(LR.score(X_train, y_train)))\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR))\n\nLinear Regression for dataset 1 has a training accuracy of : 0.99609375\nAnd cross validation scores: [0.981 1.    1.    0.961 1.   ]\n\n\nThe model has high training accuracy which is confirmed via cross validation. Next we will plot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train, y_train)\n\n\n\n\n\n\n\n\nThe model seems to be making reasonable decisions for classiying Penquin species on each island. The only island in which the model does not make perfect decisions, is Dream island which is home to all species of Penquins. Next, assess the model against the testing data.\n\n# Check the model accuracy against the testing data\nprint(\"Linear Regression for dataset 1 has testing accuracy of: \" + str(LR.score(X_test1, y_test)))\n\nLinear Regression for dataset 1 has testing accuracy of: 1.0\n\n\n100% testing accuracy has been achieved with a Linear Regression model for classifying Penquin Species. Lets view the confusion matrix for our succesfull model as well as look at the decision regions for the model evaluated on the test set.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test1)\nC = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix For the Model:\\n\" + str(C))\n\nplot_regions(LR, X_test1, y_test)\n\nConfusion Matrix For the Model:\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\n\n\n\n\n\n\n\n100% accuracy has been achieved! We have now developed a model with 100% testing accuracy for identifying Palmer Penquins based on physiologcal characteristics. By plotting the decision regions versus the test data and by looking at the confusion matrix, it becomes clear that the model was able to distinguish every penquin soley based on the provided information. If you ran this code yourself and did not achieve 100% accuracy on one of these models, re-run the code with new training and testing data (this will produce a model with 100% testing accuracy after 2 - 3 tries if not on the first). Since it was mentioned above, lets run the DecisionTreeClassifiers for fun.\n\n\nDiscussion\nThrough the process of analyzing the Palmer Penguins dataset, several key insights were uncovered regarding the classification of penguin species based on their physical characteristics. First, identifying the quantitative features which created the clearest three groupings was key. In fact, more time should have been spent in the beginning graphing out possible combinations of quantitative features; this process may have more quickly identified Culmen Length and Depth as the best features for clustering penquin species together. Plotting tables for the qualitative features also proved key, as it quickly identified that Island was a good indicator of species. In addition, when dealing with models such as the DecisionTreeClassifier (not shown in this post) utilizing cross validation as a check for overfitting helped identify the correct depth range for our models, as lower values procuded worse testing accuracy, but the extremely high complexity values produced overfitted results. Finally, as a general note, I would spend more time in the beginning exploring potential data combinations via graphical methods in order to more quickly identify the predicators which might work best rather than plug and chug; I did, however, enjoy the process of tinkering with the different models and datasets until I found the perfect one."
  }
]