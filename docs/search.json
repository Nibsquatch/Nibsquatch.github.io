[
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Blog 1 - Classifying Palmer Penquins",
    "section": "",
    "text": "Abstract\nThis blog utilizes the Palmer Penguins dataset to develop predictive models for determining the species of penguins based on their morphological measurements. The dataset comprises various features, including culmen length and depth, flipper length, and body mass, across three species: Adelie, Chinstrap, and Gentoo. Qualitative features such as Island, Clutch Completion, and Sex are also included. Through visual analysis, features which differed between species were identified and selected for model training. Both Logistic Regression and Decision Trees were implemented and evaluated. Model performance was assessed using training accuracy both absolute and through cross validation as well as assessment on separate testing data.\n\n\nData Preparation and Feature Selection\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution.\nLets look at the three quantitative predictor variables and plot the combinations we can make.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Depth vs Culmen Length\")\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth vs Flipper Length\")\n\nsns.stripplot(x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Culmen Length vs Flipper Length\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoders for categorical variables\nle = LabelEncoder()\nle.fit(df[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare data\ndf_train, y_train = prepare_data(df)\n\n# Visualize our new training data\ndf_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\n\n# Select the columns we want and separate into predictors\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train = df_train[predictor_cols]\n\n\nLR = LogisticRegression(max_iter = 10000)\nm = LR.fit(X_train, y_train)\n\nTime to check the training accuracy of the model and cross validate.\n\nfrom sklearn.model_selection import cross_val_score\n\n# load the testing data to check the accuracy of our model\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n\nX_test, y_test = prepare_data(test)\nX_test1 = X_test[predictor_cols]\n\nprint(\"Linear Regression for dataset 1 has a training accuracy of : \" + str(LR.score(X_train, y_train)))\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR))\n\nLinear Regression for dataset 1 has a training accuracy of : 0.99609375\nAnd cross validation scores: [0.981 1.    1.    0.961 1.   ]\n\n\nPlot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train, y_train)\n\n# Check the model accuracy against the testing data\nprint(\"Linear Regression for dataset 1 has testing accuracy of: \" + str(LR.score(X_test1, y_test)))\n\nLinear Regression for dataset 1 has testing accuracy of: 1.0\n\n\n\n\n\n\n\n\n\nLinear regression with the current parameters did not result in a 100% testing accuracy. Lets try switching out our qualitative parameter to clutch completion.\n\n# Select the columns we want and separate into predictors\npredictor_cols2 = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nX_train2 = df_train[predictor_cols2]\n\n# Visualize our training dataset 1\nsns.scatterplot(data = df, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Clutch Completion\")\n\n\n\n\n\n\n\n\nTime to fit the new Linear Regression Model. This model may actually be less accurate than the model taking into account island, as clutch completion seems to be mixed more evenly between penquin species compared to origin island which had some degree of specification.\n\nLR2 = LogisticRegression(max_iter = 10000)\nm2 = LR2.fit(X_train2, y_train)\n\nCheck the new training accuracy and cross validate. Then look at the decision region for the new model.\n\nprint(\"Linear Regression for dataset 2 has a training accuracy of: \" + str(LR2.score(X_train2, y_train)))\n\ncv_scores_LR2 = cross_val_score(LR2, X_train2, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR2))\n\nplot_regions(LR2, X_train2, y_train)\n\n# Check the model accuracy against the testing data\nX_test2 = X_test[predictor_cols2]\n\nprint(\"Linear Regression for dataset 2 has a testing accuracy of: \" + str(LR2.score(X_test2, y_test)))\n\nLinear Regression for dataset 2 has a training accuracy of: 0.95703125\nAnd cross validation scores: [0.962 0.941 0.961 0.922 0.98 ]\nLinear Regression for dataset 2 has a testing accuracy of: 0.9558823529411765\n\n\n\n\n\n\n\n\n\nUnfortunately, changing the qualitative indicator for this model did not result in a testing accuracty of 100%, in fact it decreased the accuracy by ~ 2%. Let’s return to island as the qualitative predictors and explore other quantitative predictors.\n\n# Select the columns we want and separate into predictors\npredictor_cols3 = [\"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train3 = df_train[predictor_cols3]\n\n# Visualize our training dataset 1\nsns.scatterplot(data = df, x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Island\")\n\n\n\n\n\n\n\n\nVisually there is a large separation between Gentoo and the other species, however there is almost no separation between Chinstrap and Adelie. I’ll try a Linear Regression but a different ML model may be more appropriate. The separation of Gentoo is promising however.\n\nLR3 = LogisticRegression(max_iter = 10000)\nm3 = LR3.fit(X_train3, y_train)\n\nCheck the new training accuracy and cross validate. Then look at the decision region for the new model.\n\nprint(\"Linear Regression for dataset 3 has a training accuracy of: \" + str(LR3.score(X_train3, y_train)))\n\ncv_scores_LR3 = cross_val_score(LR3, X_train3, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR3))\n\nplot_regions(LR3, X_train3, y_train)\n\n# Check the model accuracy against the testing data\nX_test3 = X_test[predictor_cols3]\n\nprint(\"Linear Regression for dataset 3 has a testing accuracy of: \" + str(LR3.score(X_test3, y_test)))\n\nLinear Regression for dataset 3 has a training accuracy of: 0.8828125\nAnd cross validation scores: [0.827 0.902 0.902 0.863 0.922]\nLinear Regression for dataset 3 has a testing accuracy of: 0.8970588235294118\n\n\n\n\n\n\n\n\n\nLinear Regression clearly does not work for these parameters, however the clear separation between Gentoo is promising and other machine learning models should be considered. Lets try a DecisionTree Classifier with the two datasets involving Island as our qualitative indicator.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nreg1_1 = DecisionTreeClassifier(max_depth = 1)\nreg1_5 = DecisionTreeClassifier(max_depth = 5)\nreg2_1 = DecisionTreeClassifier(max_depth = 1)\nreg2_5 = DecisionTreeClassifier(max_depth = 5)\n\n# time to fit a decision tree classifier with depth 1 and 5 to both our datasets with Island as the qualitative indicator\nreg1_1.fit(X_train, y_train)\nreg1_5.fit(X_train, y_train)\nreg2_1.fit(X_train3, y_train)\nreg2_5.fit(X_train3, y_train)\n\n# Check the training accuracy and cross validate each model\nprint(\"DecisionTreeClassifier with depth = 1 has a training accuracy of: \" + str(reg1_1.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_1 = cross_val_score(reg1_1, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_1) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg1_5.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_5 = cross_val_score(reg1_5, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_5) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 1 has a training accuracy of: \" + str(reg2_1.score(X_train3, y_train)) + \" for dataset 3\")\n\ncv_reg2_1 = cross_val_score(reg2_1, X_train3, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg2_1) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg2_5.score(X_train3, y_train)) + \" for dataset 3\")\n\ncv_reg2_5 = cross_val_score(reg2_5, X_train3, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg2_5) + \"\\n\")\n\n\nDecisionTreeClassifier with depth = 1 has a training accuracy of: 0.7734375 for dataset 1\nAnd a cross validation of: [0.769 0.765 0.765 0.765 0.784]\n\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 0.9921875 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.941 1.   ]\n\nDecisionTreeClassifier with depth = 1 has a training accuracy of: 0.7734375 for dataset 3\nAnd a cross validation of: [0.769 0.765 0.765 0.765 0.784]\n\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 0.9140625 for dataset 3\nAnd a cross validation of: [0.865 0.863 0.863 0.882 0.902]\n\n\n\nBased on training accuracy and cross validation model 2 (dataset 1 with depth = 5) seems to be the most promising. Lets check the decision region and accuracy against the testing data.\n\nplot_regions(reg1_5, X_train, y_train)\n\n# Check the model accuracy against the testing data\nprint(\"Dataset 1 with depth = 5 has testing accuracy: \" + str(reg1_5.score(X_test1, y_test)))\n\nDataset 1 with depth = 5 has testing accuracy: 0.9852941176470589\n\n\n\n\n\n\n\n\n\nPerhaps this model was overfit to the data. Lets examine the model with the second highest training accuracy (model 4: dataset 3 with depth = 5).\n\nplot_regions(reg2_5, X_train3, y_train)\n\n# Check the model accuracy against the testing data\nprint(\"Dataset 3 with depth = 5 has testing accuracy: \" + str(reg2_5.score(X_test3, y_test)))\n\nDataset 3 with depth = 5 has testing accuracy: 0.8676470588235294\n\n\n\n\n\n\n\n\n\nThe accuracy of the models using dataset 3 were not as good as the models for dataset 1. We will now try additional model complexities in between 1 and 5 on dataset 1 to see if there is a more optimal model depth.\n\nreg1_2 = DecisionTreeClassifier(max_depth = 2)\nreg1_3 = DecisionTreeClassifier(max_depth = 3)\nreg1_4 = DecisionTreeClassifier(max_depth = 4)\n\nreg1_2.fit(X_train, y_train)\nreg1_3.fit(X_train, y_train)\nreg1_4.fit(X_train, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 2 has a training accuracy of: \" + str(reg1_2.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_2 = cross_val_score(reg1_2, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_2) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 3 has a training accuracy of: \" + str(reg1_3.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_3 = cross_val_score(reg1_3, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_3) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 4 has a training accuracy of: \" + str(reg1_4.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_4 = cross_val_score(reg1_4, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_4) + \"\\n\")\n\n# test the accuracy for each model of varying depth against the test data\nprint(\"Testing the model with complexity 2: \" + str(reg1_2.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 3: \" + str(reg1_3.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 4: \" + str(reg1_4.score(X_test1, y_test)) + \"\\n\")\n\nDecisionTreeClassifier with depth = 2 has a training accuracy of: 0.9609375 for dataset 1\nAnd a cross validation of: [0.981 0.902 0.961 0.902 0.98 ]\n\nDecisionTreeClassifier with depth = 3 has a training accuracy of: 0.98046875 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.98  0.941 0.98 ]\n\nDecisionTreeClassifier with depth = 4 has a training accuracy of: 0.98828125 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.941 1.   ]\n\nTesting the model with complexity 2: 0.9558823529411765\n\nTesting the model with complexity 3: 0.9705882352941176\n\nTesting the model with complexity 4: 0.9852941176470589\n\n\n\nIt seems that the higher model complexity is yielding better training accuracy and testing accuracy. Lets try the extremes of complexity on dataset 1, even though this may result in overfitting to the training data.\n\nreg1_25 = DecisionTreeClassifier(max_depth = 25)\nreg1_100 = DecisionTreeClassifier(max_depth = 100)\nreg1_500 = DecisionTreeClassifier(max_depth = 500)\n\nreg1_25.fit(X_train, y_train)\nreg1_100.fit(X_train, y_train)\nreg1_500.fit(X_train, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 25 has a training accuracy of: \" + str(reg1_25.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_25 = cross_val_score(reg1_25, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_25) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 100 has a training accuracy of: \" + str(reg1_100.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_100 = cross_val_score(reg1_100, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_100) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 500 has a training accuracy of: \" + str(reg1_500.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_500 = cross_val_score(reg1_500, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_500) + \"\\n\")\n\n# test the accuracy for each model of varying depth against the test data\nprint(\"Testing the model with complexity 25: \" + str(reg1_25.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 100: \" + str(reg1_100.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 500: \" + str(reg1_500.score(X_test1, y_test)) + \"\\n\")\n\nDecisionTreeClassifier with depth = 25 has a training accuracy of: 1.0 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.941 0.98 ]\n\nDecisionTreeClassifier with depth = 100 has a training accuracy of: 1.0 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.961 0.98 ]\n\nDecisionTreeClassifier with depth = 500 has a training accuracy of: 1.0 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.98  0.902 0.98 ]\n\nTesting the model with complexity 25: 0.9852941176470589\n\nTesting the model with complexity 100: 0.9705882352941176\n\nTesting the model with complexity 500: 0.9852941176470589\n\n\n\n100% training accuracy and extremely high accuracy in cross-validation likely means we are overfitting the model to the training data, preventing us from generalizing the model to the test data resulting in lower testing accuracy. This was expcted. Lets try making a new dataset and only looking at Culmen measurements instead of including Flipper Length as a measurment. We will try both Linear Regression as well as a Decision Tree Classifier with depths of 3,4, & 5 as those depth values produced the highest testing accuracy on dataset 1.\n\n# Select the columns we want and separate into predictors\npredictor_cols4 = [\"Culmen Depth (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train4 = df_train[predictor_cols4]\n\n# Visualize our training dataset 4\nsns.scatterplot(data = df, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\", legend = \"brief\")\n\n\n\n\n\n\n\n\nOur fourth batch of data present three fairly clear groups. Gentoo penquins cluster in the upper length, and excuslively belong to Biscoe Island. Chinstrap penquins cluster in between the other two groups, and exclusively belong to Dream Island. While, Adelie penguins can be found at every island, they have a fairly distinct cluster in the bottom right. This data clustering, while it presents some overlap, is extremely promising based on the clustering patterns especially with regard to islands for Gentoo and Chinstrap penquins.\n\nLR4 = LogisticRegression(max_iter = 10000)\nm4 = LR4.fit(X_train4, y_train)\n\nCheck the new model against the training data and cross validate. Then plot the decision region and check the model against the test data\n\nprint(\"Linear Regression for dataset 4 has a training accuracy of: \" + str(LR4.score(X_train4, y_train)))\n\ncv_scores_LR4 = cross_val_score(LR4, X_train4, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR4))\n\nplot_regions(LR4, X_train4, y_train)\n\n# Check the model accuracy against the testing data\nX_test4 = X_test[predictor_cols4]\n\nprint(\"Linear Regression for dataset 4 has a testing accuracy of: \" + str(LR4.score(X_test4, y_test)))\n\nLinear Regression for dataset 4 has a training accuracy of: 0.99609375\nAnd cross validation scores: [0.981 1.    1.    0.941 1.   ]\nLinear Regression for dataset 4 has a testing accuracy of: 1.0\n\n\n\n\n\n\n\n\n\n\n# Lets view the confusion matrix for our succesfull model\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR4.predict(X_test4)\nC = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix For the Model:\\n\" + str(C))\n\n# Lets also look at the decision regions for the model evaluated on the test set\nplot_regions(LR4, X_test4, y_test)\n\nConfusion Matrix For the Model:\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\n\n\n\n\n\n\n\n100% accuracy has been achieved! We have now developed a model with 100% testing accuracy for identifying Palmer Penquins based on physiologcal characteristics. If you ran this code yourself and did not achieve 100% accuracy on one of these models, re-run the code with new training and testing data (this will produce a model with 100% testing accuracy after 2 - 3 tries if not on the first). Since it was mentioned above, lets run the DecisionTreeClassifiers for fun.\n\nreg4_3 = DecisionTreeClassifier(max_depth = 3)\nreg4_4 = DecisionTreeClassifier(max_depth = 4)\nreg4_5 = DecisionTreeClassifier(max_depth = 5)\n\nreg4_3.fit(X_train4, y_train)\nreg4_4.fit(X_train4, y_train)\nreg4_5.fit(X_train4, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 3 has a training accuracy of: \" + str(reg4_3.score(X_train4, y_train)) + \" for dataset 4\")\n\ncv_reg4_3 = cross_val_score(reg4_3, X_train4, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg4_3) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 4 has a training accuracy of: \" + str(reg4_4.score(X_train4, y_train)) + \" for dataset 4\")\n\ncv_reg4_4 = cross_val_score(reg4_4, X_train4, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg4_4) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg4_5.score(X_train4, y_train)) + \" for dataset 4\")\n\ncv_reg4_5 = cross_val_score(reg4_5, X_train4, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg4_5) + \"\\n\")\n\n# Plot Each Decision Region For the Decision Tree Models\n\nplot_regions(reg4_3, X_test4, y_test)\n\nplot_regions(reg4_4, X_test4, y_test)\n\nplot_regions(reg4_5, X_test4, y_test)\n\n\n# test the accuracy for each model of varying depth against the test data\nprint(\"Testing the model with complexity 3: \" + str(reg4_3.score(X_test4, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 4: \" + str(reg4_4.score(X_test4, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 5: \" + str(reg4_5.score(X_test4, y_test)) + \"\\n\")\n\nDecisionTreeClassifier with depth = 3 has a training accuracy of: 0.98828125 for dataset 4\nAnd a cross validation of: [0.981 1.    0.98  0.941 0.941]\n\nDecisionTreeClassifier with depth = 4 has a training accuracy of: 0.9921875 for dataset 4\nAnd a cross validation of: [0.981 1.    0.98  0.941 0.961]\n\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 1.0 for dataset 4\nAnd a cross validation of: [0.981 0.98  0.98  0.941 0.98 ]\n\nTesting the model with complexity 3: 0.9852941176470589\n\nTesting the model with complexity 4: 0.9852941176470589\n\nTesting the model with complexity 5: 0.9852941176470589\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese models will also achieve 100% testing accuracy depending on how the model adapts to the training data and may need to be refresehd a few times to achieve the 100% testing mark on your own machine.\nThrough the process of analyzing the Palmer Penguins dataset, several key insights were uncovered regarding the classification of penguin species based on their physical characteristics. First, identify the quantitative features which created the clearest three groupings was key. In fact, more time should have been spent in the beginning graphing out possible combinations of quantitative features; this process may have more quickly identified Culmen Length and Depth as the best features for clustering penquin species together. Plotting tables for the qualitative features also proved key, as it quickly identified that Island was a good indicator of species. In addition, when dealing with models such as the DecisionTreeClassifier utilizing the cross validation as a check for overfitting helped identify the correct depth range for our models, as lower values procuded worse testing accuracy, but the extremely high complexity values produced overfitted results. Finally, as a general note, I would spend more time in the beginning exploring potential data combinations via graphical methods in order to more quickly identify the predicators which might work best rather than plug and chug; I did, however, enjoy the process of tinkering with the different models and datasets until I found the perfect one."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 1 - Classifying Palmer Penquins\n\n\n\n\n\nBuilding a machine learning model to classify Palmer Penquins based on physical properties\n\n\n\n\n\nFeb 26, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1 - Classifying Palmer Penquins\n\n\n\n\n\nBuilding a machine learning model to classify Palmer Penquins based on physical properties\n\n\n\n\n\nFeb 26, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing whether or not I can edit this"
  },
  {
    "objectID": "posts/Blog2/index.html",
    "href": "posts/Blog2/index.html",
    "title": "Blog 1 - Classifying Palmer Penquins",
    "section": "",
    "text": "Abstract\nThis blog utilizes the Palmer Penguins dataset to develop predictive models for determining the species of penguins based on their morphological measurements. The dataset comprises various features, including culmen length and depth, flipper length, and body mass, across three species: Adelie, Chinstrap, and Gentoo. Qualitative features such as Island, Clutch Completion, and Sex are also included. Through visual analysis, features which differed between species were identified and selected for model training. Both Logistic Regression and Decision Trees were implemented and evaluated. Model performance was assessed using training accuracy both absolute and through cross validation as well as assessment on separate testing data.\n\n\nData Preparation and Feature Selection\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution.\nLets look at the three quantitative predictor variables and plot the combinations we can make.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Depth vs Culmen Length\")\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth vs Flipper Length\")\n\nsns.stripplot(x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Culmen Length vs Flipper Length\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoders for categorical variables\nle = LabelEncoder()\nle.fit(df[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare data\ndf_train, y_train = prepare_data(df)\n\n# Visualize our new training data\ndf_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\n\n# Select the columns we want and separate into predictors\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train = df_train[predictor_cols]\n\n\nLR = LogisticRegression(max_iter = 10000)\nm = LR.fit(X_train, y_train)\n\nTime to check the training accuracy of the model and cross validate.\n\nfrom sklearn.model_selection import cross_val_score\n\n# load the testing data to check the accuracy of our model\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n\nX_test, y_test = prepare_data(test)\nX_test1 = X_test[predictor_cols]\n\nprint(\"Linear Regression for dataset 1 has a training accuracy of : \" + str(LR.score(X_train, y_train)))\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR))\n\nLinear Regression for dataset 1 has a training accuracy of : 0.99609375\nAnd cross validation scores: [0.981 1.    1.    0.961 1.   ]\n\n\nPlot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train, y_train)\n\n# Check the model accuracy against the testing data\nprint(\"Linear Regression for dataset 1 has testing accuracy of: \" + str(LR.score(X_test1, y_test)))\n\nLinear Regression for dataset 1 has testing accuracy of: 1.0\n\n\n\n\n\n\n\n\n\nLinear regression with the current parameters did not result in a 100% testing accuracy. Lets try switching out our qualitative parameter to clutch completion.\n\n# Select the columns we want and separate into predictors\npredictor_cols2 = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nX_train2 = df_train[predictor_cols2]\n\n# Visualize our training dataset 1\nsns.scatterplot(data = df, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Clutch Completion\")\n\n\n\n\n\n\n\n\nTime to fit the new Linear Regression Model. This model may actually be less accurate than the model taking into account island, as clutch completion seems to be mixed more evenly between penquin species compared to origin island which had some degree of specification.\n\nLR2 = LogisticRegression(max_iter = 10000)\nm2 = LR2.fit(X_train2, y_train)\n\nCheck the new training accuracy and cross validate. Then look at the decision region for the new model.\n\nprint(\"Linear Regression for dataset 2 has a training accuracy of: \" + str(LR2.score(X_train2, y_train)))\n\ncv_scores_LR2 = cross_val_score(LR2, X_train2, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR2))\n\nplot_regions(LR2, X_train2, y_train)\n\n# Check the model accuracy against the testing data\nX_test2 = X_test[predictor_cols2]\n\nprint(\"Linear Regression for dataset 2 has a testing accuracy of: \" + str(LR2.score(X_test2, y_test)))\n\nLinear Regression for dataset 2 has a training accuracy of: 0.95703125\nAnd cross validation scores: [0.962 0.941 0.961 0.922 0.98 ]\nLinear Regression for dataset 2 has a testing accuracy of: 0.9558823529411765\n\n\n\n\n\n\n\n\n\nUnfortunately, changing the qualitative indicator for this model did not result in a testing accuracty of 100%, in fact it decreased the accuracy by ~ 2%. Let’s return to island as the qualitative predictors and explore other quantitative predictors.\n\n# Select the columns we want and separate into predictors\npredictor_cols3 = [\"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train3 = df_train[predictor_cols3]\n\n# Visualize our training dataset 1\nsns.scatterplot(data = df, x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Island\")\n\n\n\n\n\n\n\n\nVisually there is a large separation between Gentoo and the other species, however there is almost no separation between Chinstrap and Adelie. I’ll try a Linear Regression but a different ML model may be more appropriate. The separation of Gentoo is promising however.\n\nLR3 = LogisticRegression(max_iter = 10000)\nm3 = LR3.fit(X_train3, y_train)\n\nCheck the new training accuracy and cross validate. Then look at the decision region for the new model.\n\nprint(\"Linear Regression for dataset 3 has a training accuracy of: \" + str(LR3.score(X_train3, y_train)))\n\ncv_scores_LR3 = cross_val_score(LR3, X_train3, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR3))\n\nplot_regions(LR3, X_train3, y_train)\n\n# Check the model accuracy against the testing data\nX_test3 = X_test[predictor_cols3]\n\nprint(\"Linear Regression for dataset 3 has a testing accuracy of: \" + str(LR3.score(X_test3, y_test)))\n\nLinear Regression for dataset 3 has a training accuracy of: 0.8828125\nAnd cross validation scores: [0.827 0.902 0.902 0.863 0.922]\nLinear Regression for dataset 3 has a testing accuracy of: 0.8970588235294118\n\n\n\n\n\n\n\n\n\nLinear Regression clearly does not work for these parameters, however the clear separation between Gentoo is promising and other machine learning models should be considered. Lets try a DecisionTree Classifier with the two datasets involving Island as our qualitative indicator.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nreg1_1 = DecisionTreeClassifier(max_depth = 1)\nreg1_5 = DecisionTreeClassifier(max_depth = 5)\nreg2_1 = DecisionTreeClassifier(max_depth = 1)\nreg2_5 = DecisionTreeClassifier(max_depth = 5)\n\n# time to fit a decision tree classifier with depth 1 and 5 to both our datasets with Island as the qualitative indicator\nreg1_1.fit(X_train, y_train)\nreg1_5.fit(X_train, y_train)\nreg2_1.fit(X_train3, y_train)\nreg2_5.fit(X_train3, y_train)\n\n# Check the training accuracy and cross validate each model\nprint(\"DecisionTreeClassifier with depth = 1 has a training accuracy of: \" + str(reg1_1.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_1 = cross_val_score(reg1_1, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_1) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg1_5.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_5 = cross_val_score(reg1_5, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_5) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 1 has a training accuracy of: \" + str(reg2_1.score(X_train3, y_train)) + \" for dataset 3\")\n\ncv_reg2_1 = cross_val_score(reg2_1, X_train3, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg2_1) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg2_5.score(X_train3, y_train)) + \" for dataset 3\")\n\ncv_reg2_5 = cross_val_score(reg2_5, X_train3, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg2_5) + \"\\n\")\n\n\nDecisionTreeClassifier with depth = 1 has a training accuracy of: 0.7734375 for dataset 1\nAnd a cross validation of: [0.769 0.765 0.765 0.765 0.784]\n\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 0.9921875 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.941 1.   ]\n\nDecisionTreeClassifier with depth = 1 has a training accuracy of: 0.7734375 for dataset 3\nAnd a cross validation of: [0.769 0.765 0.765 0.765 0.784]\n\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 0.9140625 for dataset 3\nAnd a cross validation of: [0.865 0.863 0.863 0.882 0.902]\n\n\n\nBased on training accuracy and cross validation model 2 (dataset 1 with depth = 5) seems to be the most promising. Lets check the decision region and accuracy against the testing data.\n\nplot_regions(reg1_5, X_train, y_train)\n\n# Check the model accuracy against the testing data\nprint(\"Dataset 1 with depth = 5 has testing accuracy: \" + str(reg1_5.score(X_test1, y_test)))\n\nDataset 1 with depth = 5 has testing accuracy: 0.9852941176470589\n\n\n\n\n\n\n\n\n\nPerhaps this model was overfit to the data. Lets examine the model with the second highest training accuracy (model 4: dataset 3 with depth = 5).\n\nplot_regions(reg2_5, X_train3, y_train)\n\n# Check the model accuracy against the testing data\nprint(\"Dataset 3 with depth = 5 has testing accuracy: \" + str(reg2_5.score(X_test3, y_test)))\n\nDataset 3 with depth = 5 has testing accuracy: 0.8676470588235294\n\n\n\n\n\n\n\n\n\nThe accuracy of the models using dataset 3 were not as good as the models for dataset 1. We will now try additional model complexities in between 1 and 5 on dataset 1 to see if there is a more optimal model depth.\n\nreg1_2 = DecisionTreeClassifier(max_depth = 2)\nreg1_3 = DecisionTreeClassifier(max_depth = 3)\nreg1_4 = DecisionTreeClassifier(max_depth = 4)\n\nreg1_2.fit(X_train, y_train)\nreg1_3.fit(X_train, y_train)\nreg1_4.fit(X_train, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 2 has a training accuracy of: \" + str(reg1_2.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_2 = cross_val_score(reg1_2, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_2) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 3 has a training accuracy of: \" + str(reg1_3.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_3 = cross_val_score(reg1_3, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_3) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 4 has a training accuracy of: \" + str(reg1_4.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_4 = cross_val_score(reg1_4, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_4) + \"\\n\")\n\n# test the accuracy for each model of varying depth against the test data\nprint(\"Testing the model with complexity 2: \" + str(reg1_2.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 3: \" + str(reg1_3.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 4: \" + str(reg1_4.score(X_test1, y_test)) + \"\\n\")\n\nDecisionTreeClassifier with depth = 2 has a training accuracy of: 0.9609375 for dataset 1\nAnd a cross validation of: [0.981 0.902 0.961 0.902 0.98 ]\n\nDecisionTreeClassifier with depth = 3 has a training accuracy of: 0.98046875 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.98  0.941 0.98 ]\n\nDecisionTreeClassifier with depth = 4 has a training accuracy of: 0.98828125 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.941 1.   ]\n\nTesting the model with complexity 2: 0.9558823529411765\n\nTesting the model with complexity 3: 0.9705882352941176\n\nTesting the model with complexity 4: 0.9852941176470589\n\n\n\nIt seems that the higher model complexity is yielding better training accuracy and testing accuracy. Lets try the extremes of complexity on dataset 1, even though this may result in overfitting to the training data.\n\nreg1_25 = DecisionTreeClassifier(max_depth = 25)\nreg1_100 = DecisionTreeClassifier(max_depth = 100)\nreg1_500 = DecisionTreeClassifier(max_depth = 500)\n\nreg1_25.fit(X_train, y_train)\nreg1_100.fit(X_train, y_train)\nreg1_500.fit(X_train, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 25 has a training accuracy of: \" + str(reg1_25.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_25 = cross_val_score(reg1_25, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_25) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 100 has a training accuracy of: \" + str(reg1_100.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_100 = cross_val_score(reg1_100, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_100) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 500 has a training accuracy of: \" + str(reg1_500.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_500 = cross_val_score(reg1_500, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_500) + \"\\n\")\n\n# test the accuracy for each model of varying depth against the test data\nprint(\"Testing the model with complexity 25: \" + str(reg1_25.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 100: \" + str(reg1_100.score(X_test1, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 500: \" + str(reg1_500.score(X_test1, y_test)) + \"\\n\")\n\nDecisionTreeClassifier with depth = 25 has a training accuracy of: 1.0 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.941 0.98 ]\n\nDecisionTreeClassifier with depth = 100 has a training accuracy of: 1.0 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.941 0.961 0.98 ]\n\nDecisionTreeClassifier with depth = 500 has a training accuracy of: 1.0 for dataset 1\nAnd a cross validation of: [0.981 0.98  0.98  0.902 0.98 ]\n\nTesting the model with complexity 25: 0.9852941176470589\n\nTesting the model with complexity 100: 0.9705882352941176\n\nTesting the model with complexity 500: 0.9852941176470589\n\n\n\n100% training accuracy and extremely high accuracy in cross-validation likely means we are overfitting the model to the training data, preventing us from generalizing the model to the test data resulting in lower testing accuracy. This was expcted. Lets try making a new dataset and only looking at Culmen measurements instead of including Flipper Length as a measurment. We will try both Linear Regression as well as a Decision Tree Classifier with depths of 3,4, & 5 as those depth values produced the highest testing accuracy on dataset 1.\n\n# Select the columns we want and separate into predictors\npredictor_cols4 = [\"Culmen Depth (mm)\", \"Culmen Length (mm)\", \"Island_Biscoe\", \"Island_Torgersen\", \"Island_Dream\"]\n\nX_train4 = df_train[predictor_cols4]\n\n# Visualize our training dataset 4\nsns.scatterplot(data = df, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Island\", legend = \"brief\")\n\n\n\n\n\n\n\n\nOur fourth batch of data present three fairly clear groups. Gentoo penquins cluster in the upper length, and excuslively belong to Biscoe Island. Chinstrap penquins cluster in between the other two groups, and exclusively belong to Dream Island. While, Adelie penguins can be found at every island, they have a fairly distinct cluster in the bottom right. This data clustering, while it presents some overlap, is extremely promising based on the clustering patterns especially with regard to islands for Gentoo and Chinstrap penquins.\n\nLR4 = LogisticRegression(max_iter = 10000)\nm4 = LR4.fit(X_train4, y_train)\n\nCheck the new model against the training data and cross validate. Then plot the decision region and check the model against the test data\n\nprint(\"Linear Regression for dataset 4 has a training accuracy of: \" + str(LR4.score(X_train4, y_train)))\n\ncv_scores_LR4 = cross_val_score(LR4, X_train4, y_train, cv = 5)\nprint(\"And cross validation scores: \" + str(cv_scores_LR4))\n\nplot_regions(LR4, X_train4, y_train)\n\n# Check the model accuracy against the testing data\nX_test4 = X_test[predictor_cols4]\n\nprint(\"Linear Regression for dataset 4 has a testing accuracy of: \" + str(LR4.score(X_test4, y_test)))\n\nLinear Regression for dataset 4 has a training accuracy of: 0.99609375\nAnd cross validation scores: [0.981 1.    1.    0.941 1.   ]\nLinear Regression for dataset 4 has a testing accuracy of: 1.0\n\n\n\n\n\n\n\n\n\n\n# Lets view the confusion matrix for our succesfull model\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR4.predict(X_test4)\nC = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix For the Model:\\n\" + str(C))\n\n# Lets also look at the decision regions for the model evaluated on the test set\nplot_regions(LR4, X_test4, y_test)\n\nConfusion Matrix For the Model:\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\n\n\n\n\n\n\n\n100% accuracy has been achieved! We have now developed a model with 100% testing accuracy for identifying Palmer Penquins based on physiologcal characteristics. If you ran this code yourself and did not achieve 100% accuracy on one of these models, re-run the code with new training and testing data (this will produce a model with 100% testing accuracy after 2 - 3 tries if not on the first). Since it was mentioned above, lets run the DecisionTreeClassifiers for fun.\n\nreg4_3 = DecisionTreeClassifier(max_depth = 3)\nreg4_4 = DecisionTreeClassifier(max_depth = 4)\nreg4_5 = DecisionTreeClassifier(max_depth = 5)\n\nreg4_3.fit(X_train4, y_train)\nreg4_4.fit(X_train4, y_train)\nreg4_5.fit(X_train4, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 3 has a training accuracy of: \" + str(reg4_3.score(X_train4, y_train)) + \" for dataset 4\")\n\ncv_reg4_3 = cross_val_score(reg4_3, X_train4, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg4_3) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 4 has a training accuracy of: \" + str(reg4_4.score(X_train4, y_train)) + \" for dataset 4\")\n\ncv_reg4_4 = cross_val_score(reg4_4, X_train4, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg4_4) + \"\\n\")\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg4_5.score(X_train4, y_train)) + \" for dataset 4\")\n\ncv_reg4_5 = cross_val_score(reg4_5, X_train4, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg4_5) + \"\\n\")\n\n# Plot Each Decision Region For the Decision Tree Models\n\nplot_regions(reg4_3, X_test4, y_test)\n\nplot_regions(reg4_4, X_test4, y_test)\n\nplot_regions(reg4_5, X_test4, y_test)\n\n\n# test the accuracy for each model of varying depth against the test data\nprint(\"Testing the model with complexity 3: \" + str(reg4_3.score(X_test4, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 4: \" + str(reg4_4.score(X_test4, y_test)) + \"\\n\")\nprint(\"Testing the model with complexity 5: \" + str(reg4_5.score(X_test4, y_test)) + \"\\n\")\n\nDecisionTreeClassifier with depth = 3 has a training accuracy of: 0.98828125 for dataset 4\nAnd a cross validation of: [0.981 1.    0.98  0.941 0.941]\n\nDecisionTreeClassifier with depth = 4 has a training accuracy of: 0.9921875 for dataset 4\nAnd a cross validation of: [0.981 1.    0.98  0.941 0.961]\n\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 1.0 for dataset 4\nAnd a cross validation of: [0.981 0.98  0.98  0.941 0.98 ]\n\nTesting the model with complexity 3: 0.9852941176470589\n\nTesting the model with complexity 4: 0.9852941176470589\n\nTesting the model with complexity 5: 0.9852941176470589\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese models will also achieve 100% testing accuracy depending on how the model adapts to the training data and may need to be refresehd a few times to achieve the 100% testing mark on your own machine.\nThrough the process of analyzing the Palmer Penguins dataset, several key insights were uncovered regarding the classification of penguin species based on their physical characteristics. First, identify the quantitative features which created the clearest three groupings was key. In fact, more time should have been spent in the beginning graphing out possible combinations of quantitative features; this process may have more quickly identified Culmen Length and Depth as the best features for clustering penquin species together. Plotting tables for the qualitative features also proved key, as it quickly identified that Island was a good indicator of species. In addition, when dealing with models such as the DecisionTreeClassifier utilizing the cross validation as a check for overfitting helped identify the correct depth range for our models, as lower values procuded worse testing accuracy, but the extremely high complexity values produced overfitted results. Finally, as a general note, I would spend more time in the beginning exploring potential data combinations via graphical methods in order to more quickly identify the predicators which might work best rather than plug and chug; I did, however, enjoy the process of tinkering with the different models and datasets until I found the perfect one."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  }
]