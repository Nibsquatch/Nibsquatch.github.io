[
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Blog 2/index.html",
    "href": "posts/Blog 2/index.html",
    "title": "Blog 2 - Maximizing Loan Profit",
    "section": "",
    "text": "Data Exploration\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train.dropna()\ndf_train[\"loan_int_rate\"] = df_train[\"loan_int_rate\"] / 100\n\n# make a copy to use and view that data\ndf_ = df_train.copy()\ndf_\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n0.1347\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n0.0751\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n0.1287\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n0.0963\n0\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n0.1491\n1\n0.25\nN\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n0.0729\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n0.0542\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n0.1171\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n0.1268\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n0.0729\n1\n0.36\nN\n4\n\n\n\n\n22907 rows × 12 columns\n\n\n\nLets explore how loan intent varies across age groups\n\n# first lets add a column for age groups\ndf_[\"age_group\"] = df_[\"person_age\"] // 10\ndf_[\"age_group\"][df_[\"age_group\"] &gt;= 10 ] = 10\n\n# Get the counts of each loan intent within each age group\nloan_intent_counts = df_.groupby(\"age_group\")[\"loan_intent\"].value_counts().unstack()\nloan_intent_counts = loan_intent_counts.fillna(0)\n\n# Convert counts to proportions relative to each age group\nloan_intent_prop = loan_intent_counts.div(loan_intent_counts.sum(axis=1), axis=0)\n\n# visualize the trends of loan intent and age\ntemp = pd.melt(loan_intent_prop)\n\ntemp[\"age_group\"] = 0 # rebuild the age group category\nfor i in range(0, len(temp) + 1):\n  temp[\"age_group\"][i] = (i % 9) + 2\n\nax = sns.barplot(data = temp, x = \"age_group\", y = \"value\", hue = \"loan_intent\")\nax.set_xlabel(\"Age Group\")\nax.set_ylabel(\"Proportion\")\nax.set_title(\"Loan Intent by Age\")\n\nText(0.5, 1.0, 'Loan Intent by Age')\n\n\n\n\n\n\n\n\n\nThe chart indicates that personal loans are most common among individuals aged 20-29, 40-49, and 90-99, suggesting a consistent need for general-purpose borrowing across different life stages. Medical loans peak in the 40-49 and 50-59 age groups, likely due to increasing healthcare expenses. Education loans spike in the 70-79 age group, which may indicate late-stage career shifts or funding for dependents. Debt consolidation loans are relatively steady, with moderate representation in the 30-39, 50-59, and 60-69 age groups. Home improvement loans have lower proportions overall but see slight increases among borrowers aged 20-29 and 80-89. Venture loans are sporadic, with small peaks in the 50-59 and 60-69 age groups, possibly reflecting mid-life entrepreneurial activity. These trends highlight how borrowing needs evolve across different age brackets.\n\ndf_2 = df_train.copy()\ndf_2[\"income_cat\"] = df_2[\"person_income\"] // 100000\n\nprint(df_2.groupby(\"income_cat\")[\"loan_amnt\"].mean().round())\n\nincome_cat\n0      8892.0\n1     13697.0\n2     17380.0\n3     18896.0\n4     18670.0\n5     15136.0\n6     17267.0\n7     13192.0\n9     19150.0\n12    10000.0\n13     6600.0\n14     6400.0\n20     8450.0\n60     5000.0\nName: loan_amnt, dtype: float64\n\n\nWhile salary (income category) does seem to affect the loan amount, with people having higher salaries taking out larger loans, the higher income categories actually often have lower loan amounts than the lower income categories. This may be a product of people making larger salaries not needing to take out loans as frequently as the middle salaries, which can afford to take out larger loans and pay them back. Let’s also determine what factors seem to be related to higher interest rates.\n\nprint(df_train.groupby([\"loan_int_rate\", \"loan_grade\"]).size())\n\nloan_int_rate  loan_grade\n0.0542         A             443\n0.0579         A             289\n0.0599         A             284\n0.0600         A               1\n               B               3\n                            ... \n0.2136         F               4\n0.2174         F               2\n0.2211         G               2\n0.2248         G               1\n0.2322         G               1\nLength: 371, dtype: int64\n\n\nFrom this snapshot of the larger table, it seems that the high grade loans are associated with the lowest interest rates. Lets explore factors related to loan grades by grouping.\n\ntemp = df_train.groupby(\"loan_grade\")[[\"person_income\", \"loan_percent_income\", \"loan_amnt\"]].mean()\n\nprint(temp)\n\n            person_income  loan_percent_income     loan_amnt\nloan_grade                                                  \nA            66773.007816             0.152629   8555.884885\nB            66662.091096             0.173846  10031.025007\nC            66416.633130             0.168928   9322.102794\nD            64555.473908             0.188833  10821.646695\nE            70868.349432             0.204190  12929.083807\nF            80756.546012             0.220982  15395.705521\nG            77342.477273             0.243409  17384.659091\n\n\nAs we can see from this datatable, loan grade is directly correlated with lower percent of total income. Meaning that higher quality loans, with lower interest rates are associated with loaners whose loan takes up the smallest percent of their total income. This is likely related to the loan taking up a lower percent of their income making it less likely for them to default on the loan. It also seems to follow that people with higher income tend to take out larger loans.\n\nsns.scatterplot(data = temp, x = temp[\"person_income\"], y = \"loan_amnt\", hue = \"loan_percent_income\", legend = \"brief\")\n\n\n\n\n\n\n\n\nLower income tends to result in smaller loans, which also take up less percent of loaners total income. The darker shades representing higher percent of total income are associated with lower quality loans. Higher quality loans are assesed to be more likely to repay the loan instead of defaulting.\n\n\nFeature Selection & Weight Assessment\nSelect the predictor and target variables related to loan grade.\n\nfrom sklearn.model_selection import train_test_split\n\n# select the predictor variables identified with loan grade. Also the include the loaners default history\npredictor_cols = [\"person_income\", \"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\", \"cb_person_default_on_file\"]\nX_model = df_train[predictor_cols].copy()  # Use .copy() to avoid modifying the original DataFrame\n\n# target variable selection\ny_model = np.array(df_train[\"loan_status\"])\n\n# Create binary indicators for past defaults\nX_model[\"past_default_yes\"] = X_model[\"cb_person_default_on_file\"] == \"Y\"\nX_model[\"past_default_no\"] = X_model[\"cb_person_default_on_file\"] == \"N\"\n\n# Generate weighted default predictor\nX_model[\"weighted_default\"] = X_model[\"loan_percent_income\"] * (1 +  X_model[\"past_default_yes\"])\n\n# Generate weigthed interest rate predictor\nX_model[\"weighted_interest\"] = (1 + X_model[\"loan_int_rate\"]) * X_model[\"loan_amnt\"] * (1 + X_model[\"past_default_yes\"])\n\n# Drop the original categorical column\nX_model.drop(\"cb_person_default_on_file\", axis=1, inplace=True)\n\n# Breaking into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.2)\n\nFit a Linear Regression model to the data and assess its accuracy of prediction. Try all combinations of predictors. Once a good model is fit, pull out the weight coefficient to use for a linear score based classifier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom itertools import combinations\nfrom sklearn.metrics import accuracy_score\n\nnumerical_predictors = [\"person_income\", \"loan_amnt\", \"loan_percent_income\", \"loan_int_rate\", \"weighted_default\", \"weighted_interest\"]\nqual_predictors = [\"past_default_yes\", \"past_default_no\"]\npredictor_cols = numerical_predictors + qual_predictors\n\nresults = []\n\n# compute the best combinations\nfor r in range(1, len(predictor_cols) + 1):  # From 1 feature to all features\n    for subset in combinations(predictor_cols, r):\n        test_cols = list(subset)\n        \n        # Select subset of features\n        X_train_subset = X_train[test_cols]\n        X_test_subset = X_test[test_cols]\n        \n        # Train a simple logistic regression model\n        model = LogisticRegression(max_iter=1000)\n        model.fit(X_train_subset, y_train)\n\n        # Predict and evaluate\n        y_pred = model.predict(X_test_subset)\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        # Store results\n        results.append({\"features\": test_cols, \"accuracy\": accuracy})\n\nFrom the list of feature combinations, pull out the top 5 and retest them against the premade training and testing data. Once the best from this list has been identified, verify its accuracy with cross validation to check for overfitting. If the features pass, pull out the weights coefficient to use for a linear score based classifier.\n\n# select the top results by accuracy\nbest_features = pd.DataFrame(results).sort_values(by = \"accuracy\", ascending = False).head(5)[\"features\"].tolist()\n\ntrue_predictors = []\nbest_acc = 0\n\nfor features in best_features:\n    # fit a model to the feature list\n    LR_test = LogisticRegression()\n    m_test = LR_test.fit(X_train[features], y_train)\n\n    y_pred = LR_test.predict(X_test[features])\n    acc = accuracy_score(y_test, y_pred)\n\n    if acc &gt; best_acc:\n        best_acc = acc\n        true_predictors = features\n\nprint(\"The predictors with the highest testing accuracy are: \" + str(true_predictors) + \"\\nAccuracy = \" + str(best_acc))\n\n# Check with cross validation\nLR_real = LogisticRegression()\nm_real = LR_real.fit(X_train[true_predictors], y_train)\n\ncv_scores_LR = cross_val_score(LR_real, X_test[true_predictors], y_test, cv = 5)\n\nprint(\"Cross validation for the selected predictors: \" + str(cv_scores_LR))\n\n# store the weight coefficients for this model to build our linear score function\nw = m_real.coef_\ns = w.shape\nw = w.reshape(s[1], s[0])\n\nThe predictors with the highest testing accuracy are: ['loan_amnt', 'loan_percent_income']\nAccuracy = 0.8197293758184199\nCross validation for the selected predictors: [0.81461287 0.81352236 0.81222707 0.80895197 0.81004367]\n\n\nUpon inspecting accuracy versus the overall test set income the predictors above were selected as the best variables based purely on accuracy. These predictors also had reasonable cross validation scores that were similar to the overall testing accuracy; this implies that the model is consisent and not overfit. The weight constants from this model were then pulled out and stored for later used in a linear score function.\n\n\nOptimizing a Linear Score Classification\nStart by defining our linear score function using our weight vector and our classification function with an arbitrary starting threshold. We will then optimize this threshold such that the bank makes the largest possible profit per loan.\n\nfrom matplotlib import pyplot as plt\n\n# Performs the dot product between the data vector X and the weight vector w\ndef score(X, w):\n    return np.dot(X,w)\n\n# Returns the predictions of the model. 1 if the loaner is predicted to default, and 0 if they are predicted to repay the loan\ndef predict(score, threshold, df):\n  scores = score(df, w)\n  return 1*(scores &gt; threshold)\n\n# pull out model data\nX_model = X_train.copy()\n\n# initialize the benefit column\nX_model[\"benefit\"] = 0\n\nbest_benefit = 0\nbest_threshold = 0\n\nscores = score(X_model[true_predictors], w)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nfor t in np.linspace(0, 10, 101):\n    X_model[\"y_pred\"] = scores &gt;= t\n\n    X_model[\"tn\"] = (X_model[\"y_pred\"] == 0) & (y_train == 0)\n    X_model[\"fn\"] = (X_model[\"y_pred\"] == 0) & (y_train == 1)\n    \n    X_model[\"benefit\"][X_model[\"tn\"]] = X_model[\"loan_amnt\"] * (1 + 0.25 * X_model[\"loan_int_rate\"]) ** 10 - X_model[\"loan_amnt\"]\n    X_model[\"benefit\"][X_model[\"fn\"]] = X_model[\"loan_amnt\"] * (1 + 0.25 * X_model[\"loan_int_rate\"]) ** 3 - 1.7 * X_model[\"loan_amnt\"]\n\n    average_benefit = np.sum(X_model[\"benefit\"]) / len(X_model)\n\n    ax.scatter(t, average_benefit, color = \"steelblue\", s = 10)\n    if average_benefit &gt; best_benefit:\n        best_benefit = average_benefit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Net benefit\", title = f\"Best benefit ${best_benefit:.2f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nMaximization of the bank’s profit results in a threshold of 2.1, with a profit per buyer of 1301.86. The value of the threshold has been stored for later use. Lets now test the model with our optimal weight vector and threshold against new testing data.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\ndf_test = df_test.dropna()\ndf_test[\"loan_int_rate\"] = df_test[\"loan_int_rate\"]  /100\n\n# Create binary indicators for past defaults\ndf_test[\"past_default_yes\"] = df_test[\"cb_person_default_on_file\"] == \"Y\"\ndf_test[\"past_default_no\"] = df_test[\"cb_person_default_on_file\"] == \"N\"\n\n# Generate weighted default predictor\ndf_test[\"weighted_default\"] = df_test[\"loan_percent_income\"] * (1 +  df_test[\"past_default_yes\"])\n\n# Generate weigthed interest rate predictor\ndf_test[\"weighted_interest\"] = (1 + df_test[\"loan_int_rate\"]) * df_test[\"loan_amnt\"] * (1 + df_test[\"past_default_yes\"])\n\n# Drop the original categorical column\ndf_test.drop(\"cb_person_default_on_file\", axis=1, inplace=True)\n\n# initialize the benefit column\ndf_test[\"benefit\"] = 0\n\n# calculate the score array and the expected profit per buyer\nscores = score(df_test[true_predictors],w)\n\ndf_test[\"y_pred\"] = scores &gt;= best_threshold\n\ndf_test[\"tn\"] = (df_test[\"y_pred\"] == 0) & (df_test[\"loan_status\"] == 0)    \ndf_test[\"fn\"] = (df_test[\"y_pred\"] == 0) & (df_test[\"loan_status\"] == 1)\n    \ndf_test[\"benefit\"][df_test[\"tn\"]] = df_test[\"loan_amnt\"] * (1 + 0.25 * df_test[\"loan_int_rate\"]) ** 10 - df_test[\"loan_amnt\"]\ndf_test[\"benefit\"][df_test[\"fn\"]] = df_test[\"loan_amnt\"] * (1 + 0.25 * df_test[\"loan_int_rate\"]) ** 3 - 1.7 * df_test[\"loan_amnt\"]\n\naverage_benefit = np.sum(df_test[\"benefit\"]) / len(df_test)\nprint(\"The expected profit per buyer is: $\" + str(round(average_benefit, 2)))\n\nThe expected profit per buyer is: $1227.71\n\n\nThe expected profit per buyer on the test set (1301.86) is only slightly lower than the expected profit per buyer on the training set (1227.71). Thus our model was able to generalize from the training set.\n\n\nEvaluating Model Fairness\nLets explore if some loaners are more or less likely to receive loans based on their age\n\n# remake age groups in 10 year gaps and calculate the difference in default prediction vs reality\ndf_test[\"age_group\"] = df_test[\"person_age\"] // 10\ndf_test[\"diff\"] = df_test[\"y_pred\"] - df_test[\"loan_status\"]\n\n# Display this data as a table\ntemp = df_test.groupby(\"age_group\")[[\"loan_status\", \"y_pred\", \"diff\"]].mean().reset_index()\ntemp\n\n\n\n\n\n\n\n\nage_group\nloan_status\ny_pred\ndiff\n\n\n\n\n0\n2\n0.228689\n0.163246\n-0.065443\n\n\n1\n3\n0.204082\n0.136578\n-0.067504\n\n\n2\n4\n0.200000\n0.138462\n-0.061538\n\n\n3\n5\n0.261905\n0.119048\n-0.142857\n\n\n4\n6\n0.500000\n0.333333\n-0.166667\n\n\n5\n7\n0.500000\n0.500000\n0.000000\n\n\n\n\n\n\n\nIn almost all age groups (age group 7 only has two individuals) the algorithm underpredicts the loan default rate. However, two age groups which are far less likely to get a loan than other groups (excluding group 7), are age group 20-29 and 60-69. Despite, the algorithim underpredicting the default rate in this age group, especially in the 60-69 age group, these groups are still predicted to be much more likely to defualt than the other age groups. Lets now look at the difficulty of getting loans based on loan intent.\n\ntemp2 = df_test.copy()\n\n# Melt the data so it can be plotted in a\ntemp2_melted = pd.melt(temp2, id_vars = \"loan_intent\", value_vars = [\"y_pred\", \"loan_status\"], var_name = \"Type\", value_name = \"Proportion of Loan Status\")\n\nsns.barplot(data = temp2_melted, y = \"loan_intent\", x = \"Proportion of Loan Status\", hue = \"Type\", palette = \"BuPu\", saturation = 0.5, orient = \"h\", errorbar = None)\n\n\n\n\n\n\n\n\nMedical loans have the highest predicted default rate making them the hardes loans to get, however medical loans do have the second highest default rate. Education, Personal, and Debt Consolidation are the next hardest loans to get. Venture loans are predicted to default less often than the previous, and Home Improvement loans are predicted to default the least, despite having the third highest actual default rate. In all cases, the default rate was predicted to be less than than it ended up being. Now, lets examine how gross income affects the ease with which credit can be obtained.\n\ntemp3 = df_test.copy()\n\n# Group income into categories separated by $25,000 in income\ntemp3[\"income_group\"] = temp3[\"person_income\"] // 25000\ntemp3[\"income_group\"][temp3[\"income_group\"] &gt;= 10 ] = 10\n\n# Melt the data so it can be plotted in a bar plot\ntemp3_melted = pd.melt(temp3, id_vars = \"income_group\", value_vars = [\"y_pred\", \"loan_status\"], var_name = \"Type\", value_name = \"Proportion of Loan Status\")\n\nsns.barplot(data = temp3_melted, x = \"income_group\", y = \"Proportion of Loan Status\", hue = \"Type\", palette = \"BuPu\", saturation = 0.5, errorbar = None)\n\n\n\n\n\n\n\n\nIn this model, lower income is strongly associated with greater likelihood to default, making it much harder get loans at low income. While lower income is generally associated with higher default rates, it is unrealistic that the model predicts loaners making more than 75,000 to pay back their loans 100% of the time. In fact, the second highest default rate is actually in the 200,000 - 224,999 income category."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing whether or not I can edit this"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog 2 - Maximizing Loan Profit\n\n\n\n\n\nUsing a linear score based classifier to maximize bank profits on loans\n\n\n\n\n\nMar 5, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1 - Classifying Palmer Penquins\n\n\n\n\n\nBuilding a machine learning model to classify Palmer Penquins based on physical properties\n\n\n\n\n\nFeb 26, 2024\n\n\nRyan Mauney\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Blog 1 - Classifying Palmer Penquins",
    "section": "",
    "text": "Abstract\nThis blog utilizes the Palmer Penguins dataset to develop predictive models for determining the species of penguins based on their morphological measurements. The dataset comprises various features, including culmen length and depth, flipper length, and body mass, across three species: Adelie, Chinstrap, and Gentoo. Qualitative features such as Island, Clutch Completion, and Sex are also included. Through visual analysis, features which differed between species were identified and selected for model training. Both Logistic Regression and Decision Trees (not shown) were implemented and evaluated. Model performance was assessed using training accuracy both absolute and through cross validation as well as assessment on separate testing data.\n\n\nData Preparation and Feature Selection\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution.\nLets look at the three quantitative predictor variables and plot the combinations we can make.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Depth vs Culmen Length\")\n\nsns.stripplot(x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth vs Flipper Length\")\n\nsns.stripplot(x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Culmen Length vs Flipper Length\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile Culmen Depth vs Flipper Length provides the clearest specification for Gentoo Penguins, Adelie and Chinstrap become too muddled to properly distinguish. Culmen Length vs Flipper Length does a good job in making three clusters, with Chinstrap isolating the most, howeever there is still overlap in the regions of Gentoo and Adelie.\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoders for categorical variables\nle = LabelEncoder()\nle.fit(df[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\n# Prepare data\ndf_train, y_train = prepare_data(df)\n\n# Visualize our new training data\ndf_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nWith our new training data lets prepare a new data frame only including the predictors we want\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\ncols = []\nfor pair in combinations(all_quant_cols, 2):\n    \n    # Combinations to test training accuracy of\n    cols.append(list(pair) + all_qual_cols)\n\nfor combo in cols:\n    print(combo)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n\nTesting the Model\nNow that we have our combination of predictors, lets test the training accuracy of Linear Regression models on each predictor to determine which is the best.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\ni = 0\nbest_acc = 0\nbest_combo = 1\n\nfor combo in cols:\n    \n    print(\"Testing accuracy for predictors: \" + str(combo) + \"\\n\")\n\n    LR = LogisticRegression(max_iter = 10000)\n    m = LR.fit(df_train[combo], y_train)\n    \n    acc = LR.score(df_train[combo], y_train)\n    \n    if acc &gt; best_acc:\n        best_acc = acc\n        best_combo = i\n    \n    print(\"Iteration \" + str(i + 1) + \" has a training accuracy of: \" + str(acc))\n    cv_scores_LR = cross_val_score(LR, df_train, y_train, cv = 5)\n    print(\"Iteration \" + str(i + 1) + \" also cross validation: \" + str(cv_scores_LR) + \"\\n\")\n    i += 1\n\nprint(\"The best combination of predictors is: \" + str(cols[best_combo]))\n\n# pull out the columns that we want based on the best training accuracy\npredictor_cols = cols[best_combo]\n\nX_train = df_train[predictor_cols]\n\nLR_real = LogisticRegression(max_iter = 10000)\nm_real = LR_real.fit(X_train, y_train)\n\nTesting accuracy for predictors: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 1 has a training accuracy of: 0.99609375\nIteration 1 also cross validation: [1. 1. 1. 1. 1.]\n\nTesting accuracy for predictors: ['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 2 has a training accuracy of: 0.9765625\nIteration 2 also cross validation: [1. 1. 1. 1. 1.]\n\nTesting accuracy for predictors: ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nIteration 3 has a training accuracy of: 0.8828125\nIteration 3 also cross validation: [1. 1. 1. 1. 1.]\n\nThe best combination of predictors is: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nAfter analyzing the training accuracy for each combination of predictors, Culmen Length and Culmen Depth have the highest training accuracy, thus we will test this model against the test data. Next we will plot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\n# load the testing data to check the accuracy of our model\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# prep the testing data for later use\nX_test, y_test = prepare_data(test)\nX_test = X_test[predictor_cols]\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR_real, X_train, y_train)\n\n\n\n\n\n\n\n\nThe model seems to be making reasonable decisions for classiying Penquin species on each island. The only island in which the model does not make perfect decisions, is Dream island which is home to all species of Penquins. Next, assess the model against the testing data.\n\n# Check the model accuracy against the testing data\nprint(\"Linear Regression for the model has testing accuracy of: \" + str(LR_real.score(X_test, y_test)))\n\nLinear Regression for the model has testing accuracy of: 1.0\n\n\n100% testing accuracy has been achieved with a Linear Regression model for classifying Penquin Species. Lets view the confusion matrix for our succesfull model as well as look at the decision regions for the model evaluated on the test set.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR_real.predict(X_test)\nC = confusion_matrix(y_test, y_test_pred)\nprint(\"Confusion Matrix For the Model:\\n\" + str(C))\n\nplot_regions(LR_real, X_test, y_test)\n\nConfusion Matrix For the Model:\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\n\n\n\n\n\n\n\n100% accuracy has been achieved! We have now developed a model with 100% testing accuracy for identifying Palmer Penquins based on physiologcal characteristics. By plotting the decision regions versus the test data and by looking at the confusion matrix, it becomes clear that the model was able to distinguish every penquin soley based on the provided information. If you ran this code yourself and did not achieve 100% accuracy on one of these models, re-run the code with new training and testing data (this will produce a model with 100% testing accuracy after 2 - 3 tries if not on the first). Since it was mentioned above, lets run the DecisionTreeClassifiers for fun.\n\n\nDiscussion\nThrough the process of analyzing the Palmer Penguins dataset, several key insights were uncovered regarding the classification of penguin species based on their physical characteristics. First, identifying the quantitative features which created the clearest three groupings was key. In fact, more time should have been spent in the beginning graphing out possible combinations of quantitative features; this process may have more quickly identified Culmen Length and Depth as the best features for clustering penquin species together. Plotting tables for the qualitative features also proved key, as it quickly identified that Island was a good indicator of species. In addition, when dealing with models such as the DecisionTreeClassifier (not shown in this post) utilizing cross validation as a check for overfitting helped identify the correct depth range for our models, as lower values procuded worse testing accuracy, but the extremely high complexity values produced overfitted results. Finally, as a general note, I would spend more time in the beginning exploring potential data combinations via graphical methods in order to more quickly identify the predicators which might work best rather than plug and chug; I did, however, enjoy the process of tinkering with the different models and datasets until I found the perfect one."
  }
]