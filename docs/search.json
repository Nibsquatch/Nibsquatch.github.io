[
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "title: Blog 1: Classifying Palmer Penquins author: Ryan Mauney date: ‘2024-02-18’ image: “penquins.jpg” description: “Building a Machine Learning model to identify Palmer Penquins with 100% accuracy based on the 3 indicator variables.” format: html\nLoading neccesary packages and prepping the Palmer Penquins data.\n\nimport warnings\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nnp.set_printoptions(precision = 3)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ndf = pd.read_csv(url)\n\n# Shorten the species name\ndf[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n\n# filter our data so it only contains the variables we will look at first\n# look at the first 5 entries to determine variables that seem as if they could have a correlation\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\ndf.groupby([\"Island\", \"Species\"]).size()\n\nIsland     Species  \nBiscoe     Adelie       33\n           Gentoo       98\nDream      Adelie       45\n           Chinstrap    57\nTorgersen  Adelie       42\ndtype: int64\n\n\nTorgersen Island is home exclusively to Adelie penguins, while Dream Island is the only habitat for Chinstrap penguins, despite an almost equal distribution of Adelie and Chinstrap there. Biscoe Island hosts primarily Gentoo penguins, making up 74.8% of its population. While Adelie penguins are found on all islands, each island has a degree of exclusivity in species distribution. This is a semi promising indicator.\n\n# explore the species groups by flipper length\nplt.figure(figsize=(8, 5))\nsns.stripplot(x = \"Species\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", legend = \"brief\")\nplt.title(\"Flipper Length by Species\")\nplt.show()\n\n\n\n\n\n\n\n\nGrouping by species and analyzing flipper length clearly distinguishes Gentoo penguins, as they generally have longer flippers than Chinstrap and Adelie penguins. However, flipper length is not a reliable identifier between Chinstrap and Adelie penguins, as their ranges overlap significantly.\n\n# explore the species groups by culmen length\nplt.figure(figsize=(8, 5))\nsns.stripplot(x = \"Species\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", legend = \"brief\")\nplt.title(\"Culmen Length by Species\")\nplt.show()\n\n\n\n\n\n\n\n\nWhile there is not a clear distinction between Culmen Length for Chinstrap and Gentoo penquins, there is a clear separation the two species from Adelie. This means that Culmen Length could be used as an indicator for Chinstrap penquins, in combination with island identity.\nNext we will define a method to properly give integer values to species as well as other categorical variables and apply this method to our data.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Selecting relevant columns\ndf1 = df[[\"Species\", \"Culmen Length (mm)\", \"Island\", \"Flipper Length (mm)\"]]\n\n# Initialize LabelEncoders for categorical variables\nspecies_encoder = LabelEncoder()\nisland_encoder = LabelEncoder()\n\ndef prepare_data(df):\n    df = df.dropna()\n    \n    # Encode categorical variables\n    df[\"species_label\"] = species_encoder.fit_transform(df[\"Species\"])\n    df[\"island_label\"] = island_encoder.fit_transform(df[\"Island\"])\n    \n    # Define target variable\n    y = df[\"species_label\"]\n    \n    return df, y\n\n# Prepare data\nfull_train, y_train = prepare_data(df1)\n\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_10200\\1322769177.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"species_label\"] = species_encoder.fit_transform(df[\"Species\"])\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_10200\\1322769177.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"island_label\"] = island_encoder.fit_transform(df[\"Island\"])\n\n\nNow to split our data into training and testing data\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(full_train, test_size = .2)\n\n# Visualize our Training Data\nsns.scatterplot(data = df_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Island\")\n\n\n\n\n\n\n\n\nGentoo penquins cluster very distinguishably in the upper right, whereas Chainstrap and Adelie penquins become harder to distinguish, especially as their island regions mix more. While a linear model may well, other models such as SVM may work better. A linear regression will be fitted first.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\n\npredictor_cols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"island_label\"]\ntarget_col = \"species_label\"\n\nX_train = df_train[predictor_cols]\ny_train = df_train[target_col]\n\nX_test = df_test[predictor_cols]\ny_test = df_test[target_col]\n\nLR = LogisticRegression(max_iter = 1000)\nm = LR.fit(X_train, y_train)\n\nTime to check the training accuracy of the model and cross validate.\n\nfrom sklearn.model_selection import cross_val_score\n\nprint(LR.score(X_train, y_train))\n\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv = 5)\nprint(cv_scores_LR)\n\n0.963302752293578\n[0.955 0.932 0.977 0.93  0.977]\n\n\nPlot the decision regions for the model against the training data.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Ensure axarr is always a list-like object\n    if len(qual_features) == 1:\n      axarr = [axarr]\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      plt.show()\n\nplot_regions(LR, X_train[predictor_cols], y_train)\n\n# Check the model accuracy against the testing data\nprint(LR.score(X_test, y_test))\n\n\n\n\n\n\n\n\n0.9636363636363636\n\n\nLinear regression with the current parameters did not result in a 100% testing accuracy. Lets try switching out our qualitative parameter to clutch completion.\n\ndf2 = df[[\"Species\", \"Culmen Length (mm)\", \"Clutch Completion\", \"Flipper Length (mm)\"]]\n\n# Initialize LabelEncoders for categorical variables\nspecies_encoder = LabelEncoder()\nclutch_encoder = LabelEncoder()\n\ndef prepare_data(df):\n    df = df.dropna()\n    \n    # Encode categorical variables\n    df[\"species_label\"] = species_encoder.fit_transform(df[\"Species\"])\n    df[\"clutch_label\"] = clutch_encoder.fit_transform(df[\"Clutch Completion\"])\n    \n    # Define target variable\n    y = df[\"species_label\"]\n    \n    return df, y\n\nfull_train2, y_train2 = prepare_data(df2)\n\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_10200\\1614244165.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"species_label\"] = species_encoder.fit_transform(df[\"Species\"])\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_10200\\1614244165.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"clutch_label\"] = clutch_encoder.fit_transform(df[\"Clutch Completion\"])\n\n\nLet’s visualize our new training data.\n\ndf_train2, df_test2 = train_test_split(full_train2, test_size = .2)\n\n# Visualize our Training Data\nsns.scatterplot(data = df_train2, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Clutch Completion\")\n\n\n\n\n\n\n\n\nTime to fit the new Linear Regression Model. My guess is that this model may actually be less accurate than the model taking into account island, as clutch completion seems to be mixed more evenly between penquin species compared to origin island which had some degree of specification.\n\npredictor_cols2 = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"clutch_label\"]\ntarget_col2 = \"species_label\"\n\nX_train2 = df_train2[predictor_cols2]\ny_train2 = df_train2[target_col2]\n\nX_test2 = df_test2[predictor_cols2]\ny_test2 = df_test2[target_col2]\n\nLR2 = LogisticRegression(max_iter = 1000)\nm2 = LR2.fit(X_train2, y_train2)\n\nCheck the new training accuracy and cross validate. Then look at the decision region for the new model.\n\nprint(LR2.score(X_train2, y_train2))\n\ncv_scores_LR2 = cross_val_score(LR2, X_train2, y_train2, cv = 5)\nprint(cv_scores_LR2)\n\nplot_regions(LR2, X_train2[predictor_cols2], y_train2)\n\n# Check the model accuracy against the testing data\nprint(LR2.score(X_test2, y_test2))\n\n0.963302752293578\n[0.955 0.909 0.977 1.    0.953]\n\n\n\n\n\n\n\n\n\n0.9454545454545454\n\n\nUnfortunately, changing the qualitative indicator for this model actually decreased the testing accuracty of the model. Perhaps we should go back to the original qualitative indicator and change the quanitative indicators. Lets look at each quantitative indicator next to each other.\n\n# explore the species groups by culmen length\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\nsns.stripplot(x = \"Species\", y = \"Culmen Length (mm)\", data = df, hue = \"Species\", ax = axes[0], dodge = True)\naxes[0].set_title(\"Culmen Length by Species\")\n\nsns.stripplot(x = \"Species\", y = \"Culmen Depth (mm)\", data = df, hue = \"Species\", ax = axes[1], dodge = True, legend = False)\naxes[1].set_title(\"Culmen Depth by Species\")\n\nsns.stripplot(x = \"Species\", y = \"Flipper Length (mm)\", data = df, hue = \"Species\", ax = axes[2], dodge = True, legend = False)\naxes[2].set_title(\"Flipper Length by Species\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLooking at these plots side by side Culmen Depth and Flipper Length have almost inverse plots, perhaps these two in combination with Island would serve as a better model than with Culmen Length and Flipper Length.\n\ndf3 = df[[\"Species\", \"Culmen Depth (mm)\", \"Island\", \"Flipper Length (mm)\"]]\n\n# Initialize LabelEncoders for categorical variables\nspecies_encoder = LabelEncoder()\nisland_encoder = LabelEncoder()\n\ndef prepare_data(df):\n    df = df.dropna()\n    \n    # Encode categorical variables\n    df[\"species_label\"] = species_encoder.fit_transform(df[\"Species\"])\n    df[\"island_label\"] = island_encoder.fit_transform(df[\"Island\"])\n    \n    # Define target variable\n    y = df[\"species_label\"]\n    \n    return df, y\n\nfull_train3, y_train3 = prepare_data(df3)\n\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_10200\\464828516.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"species_label\"] = species_encoder.fit_transform(df[\"Species\"])\nC:\\Users\\miceo\\AppData\\Local\\Temp\\ipykernel_10200\\464828516.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"island_label\"] = island_encoder.fit_transform(df[\"Island\"])\n\n\nVisualize our third batch of training data\n\ndf_train3, df_test3 = train_test_split(full_train3, test_size = .2)\n\n# Visualize our Training Data\nsns.scatterplot(data = df_train3, x = \"Culmen Depth (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", style = \"Island\")\n\n\n\n\n\n\n\n\nVisually there is a large separation between Gentoo and the other species, however there is almost no separation between Chinstrap and Adelie. I’ll try a Linear Regression but a different ML model may be more appropriate. The separation of Gentoo is promising however.\n\npredictor_cols3 = [\"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"island_label\"]\ntarget_col3 = \"species_label\"\n\nX_train3 = df_train3[predictor_cols3]\ny_train3 = df_train3[target_col3]\n\nX_test3 = df_test3[predictor_cols3]\ny_test3 = df_test3[target_col3]\n\nLR3 = LogisticRegression(max_iter = 1000)\nm3 = LR3.fit(X_train3, y_train3)\n\nCheck the new training accuracy and cross validate. Then look at the decision region for the new model.\n\nprint(LR3.score(X_train3, y_train3))\n\ncv_scores_LR3 = cross_val_score(LR3, X_train3, y_train3, cv = 5)\nprint(cv_scores_LR3)\n\nplot_regions(LR3, X_train3[predictor_cols3], y_train3)\n\n# Check the model accuracy against the testing data\nprint(LR3.score(X_test3, y_test3))\n\n0.8348623853211009\n[0.886 0.795 0.795 0.86  0.837]\n\n\n\n\n\n\n\n\n\n0.7818181818181819\n\n\nLinear Regression clearly does not work for these parameters, however the clear separation between Gentoo is promising and other ML should be considered\n\nfrom sklearn.svm import SVC # support vector classifier\n\nSVM = SVC(gamma = 1)\nSVM.fit(X_train3, y_train3)\n\nprint(SVM.score(X_train3, y_train3))\n\ncv_scores_SVM = cross_val_score(SVM, X_train3, y_train3, cv = 5)\nprint(cv_scores_LR3)\n\nplot_regions(SVM, X_train3[predictor_cols3], y_train3)\n\n# Check the model accuracy against the testing data\nprint(SVM.score(X_test3, y_test3))\n\n0.9128440366972477\n[0.886 0.795 0.795 0.86  0.837]\n\n\n\n\n\n\n\n\n\n0.9272727272727272\n\n\nLets try a DecisionTree Classifier with out two datasets involving Island as our qualitative indicator as the SVM had a far worse accuracy for this dataset.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nreg1_1 = DecisionTreeClassifier(max_depth = 1)\nreg1_5 = DecisionTreeClassifier(max_depth = 5)\nreg2_1 = DecisionTreeClassifier(max_depth = 1)\nreg2_5 = DecisionTreeClassifier(max_depth = 5)\n\n# time to fit a decision tree classifier with depth 1 and 5 to both our datasets with Island as the qualitative indicator\nreg1_1.fit(X_train, y_train)\nreg1_5.fit(X_train, y_train)\nreg2_1.fit(X_train3, y_train3)\nreg2_5.fit(X_train3, y_train3)\n\n# Check the training accuracy and cross validate each model\nprint(\"DecisionTreeClassifier with depth = 1 has a training accuracy of: \" + str(reg1_1.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_1 = cross_val_score(reg1_1, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_1))\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg1_5.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_5 = cross_val_score(reg1_5, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_5))\n\nprint(\"DecisionTreeClassifier with depth = 1 has a training accuracy of: \" + str(reg2_1.score(X_train3, y_train3)) + \" for dataset 3\")\n\ncv_reg2_1 = cross_val_score(reg2_1, X_train3, y_train3, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg2_1))\n\nprint(\"DecisionTreeClassifier with depth = 5 has a training accuracy of: \" + str(reg2_5.score(X_train3, y_train3)) + \" for dataset 3\")\n\ncv_reg2_5 = cross_val_score(reg2_5, X_train3, y_train3, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg2_5))\n\n\nDecisionTreeClassifier with depth = 1 has a training accuracy of: 0.7889908256880734 for dataset 1\nAnd a cross validation of: [0.795 0.795 0.795 0.791 0.767]\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 0.9908256880733946 for dataset 1\nAnd a cross validation of: [0.955 0.977 0.977 0.977 0.977]\nDecisionTreeClassifier with depth = 1 has a training accuracy of: 0.7752293577981652 for dataset 3\nAnd a cross validation of: [0.773 0.705 0.773 0.767 0.767]\nDecisionTreeClassifier with depth = 5 has a training accuracy of: 0.9036697247706422 for dataset 3\nAnd a cross validation of: [0.795 0.75  0.818 0.907 0.884]\n\n\nBased on training accuracy and cross validation model 2 (dataset 1 with depth = 5) seems to be the most promising. Lets check the decision region and accuracy against the testing data.\n\nplot_regions(reg1_5, X_train[predictor_cols], y_train)\n\n# Check the model accuracy against the testing data\nprint(reg1_5.score(X_test, y_test))\n\n\n\n\n\n\n\n\n0.9818181818181818\n\n\nPerhaps this model was overfit to the data. Lets examine the model with the second highest training accuracy (model 4: dataset 3 with depth = 5).\n\nplot_regions(reg2_5, X_train3[predictor_cols3], y_train3)\n\n# Check the model accuracy against the testing data\nprint(reg2_5.score(X_test3, y_test3))\n\n\n\n\n\n\n\n\n0.9454545454545454\n\n\nThe accuracy of this model is not as good. We will now try a model complexity in between 1 and 5 for dataset 1.\n\nreg1_2 = DecisionTreeClassifier(max_depth = 2)\nreg1_3 = DecisionTreeClassifier(max_depth = 3)\nreg1_4 = DecisionTreeClassifier(max_depth = 4)\n\nreg1_2.fit(X_train, y_train)\nreg1_3.fit(X_train, y_train)\nreg1_4.fit(X_train, y_train)\n\n\nprint(\"DecisionTreeClassifier with depth = 2 has a training accuracy of: \" + str(reg1_2.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_2 = cross_val_score(reg1_2, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_2))\n\nprint(\"DecisionTreeClassifier with depth = 3 has a training accuracy of: \" + str(reg1_3.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_3 = cross_val_score(reg1_3, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_3))\n\nprint(\"DecisionTreeClassifier with depth = 4 has a training accuracy of: \" + str(reg1_4.score(X_train, y_train)) + \" for dataset 1\")\n\ncv_reg1_4 = cross_val_score(reg1_4, X_train, y_train, cv = 5)\nprint(\"And a cross validation of: \" + str(cv_reg1_4))\n\n\nDecisionTreeClassifier with depth = 2 has a training accuracy of: 0.963302752293578 for dataset 1\nAnd a cross validation of: [0.932 0.909 0.977 0.884 0.953]\nDecisionTreeClassifier with depth = 3 has a training accuracy of: 0.9770642201834863 for dataset 1\nAnd a cross validation of: [0.932 0.977 1.    0.93  0.977]\nDecisionTreeClassifier with depth = 4 has a training accuracy of: 0.9908256880733946 for dataset 1\nAnd a cross validation of: [0.955 0.977 1.    0.93  0.977]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Testing whether or not I can edit this"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Timnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  }
]